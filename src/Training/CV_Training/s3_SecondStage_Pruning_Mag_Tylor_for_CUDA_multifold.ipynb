{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b923ec-5857-4d0b-9909-1e0236ba4583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "import os;\n",
    "import torch;\n",
    "import numpy as np;\n",
    "import torch.optim as optim;\n",
    "import torch.nn as nn;\n",
    "from operator import itemgetter;\n",
    "from heapq import nsmallest;\n",
    "import time;\n",
    "import glob;\n",
    "import math;\n",
    "import random;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a88dd74-1ca3-4b31-b50b-84b82739d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../../src/\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f540e4f9-0b63-4f73-bca1-51557fb87033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.utils as U;\n",
    "import common.opts as opt;\n",
    "import th.resources.models as models;\n",
    "import th.resources.calculator as calc;\n",
    "# import th.resources.train_generator as train_generator;\n",
    "from th.resources.pruning_tools import filter_pruning, filter_pruner;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7215cbb7-d9ce-4b64-9f81-8abc3fada11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import common.tlopts as tlopts\n",
    "from SharedLibs.datestring import genDataTimeStr, getDateStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f74b3364-cd68-464d-9426-3a4f85ad7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reproducibility\n",
    "seed = 42;\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed);\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True;\n",
    "torch.backends.cudnn.benchmark = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69211f9d-1d78-4eae-8540-2d1b04cd13c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c00a2415-5a75-4b21-aca7-aa4ad28798ac",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d34d33d-7683-4dd8-ab36-9b9b9d95be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLGenerator():\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples, labels, options):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        print(f\"length of samples:{len(samples)}\")\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = dict([('52',1),('56',2),('71',3),('99',4)])\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "        #return len(self.samples);\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        # batchX, batchY = self.generate_batch(batchIndex);\n",
    "        batchX, batchY = self.generate_batch_select_fixed_class(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            # print(f\"sound length after U.mix is {len(sound)}\")\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[str(label1)]- 1\n",
    "            idx2 = self.mapdict[str(label2)] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        # print(f\"labels in generate_batch is:\\n{labels}\")\n",
    "\n",
    "        return sounds, labels;\n",
    "\n",
    "    def generate_batch_select_fixed_class(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        #two variables recording alarm and moaning sounds count\n",
    "        alarm_selected = 0;\n",
    "        moaning_selected = 0;\n",
    "        help_eng_selected = 0;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                # print(\"enter while true\")\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                lbl1_int = np.int16(label1);\n",
    "                lbl2_int = np.int16(label2);\n",
    "                # print(f\"label1:{label1} and label2:{label2}\")\n",
    "                # print(f\"label1:{type(label1)} and label2:{type(label2)}\")\n",
    "                if (lbl1_int == 52 and lbl2_int == 99) or (lbl1_int == 99 and lbl2_int ==52):\n",
    "                    # if (alarm_selected < moaning_selected) or (alarm_selected == moaning_selected):\n",
    "                    if (alarm_selected == moaning_selected) and (alarm_selected == help_eng_selected):\n",
    "                        alarm_selected += 1;\n",
    "                        break;\n",
    "                if (lbl1_int == 56 and lbl2_int == 99) or (lbl1_int == 99 and lbl2_int == 56):\n",
    "                    if (moaning_selected < alarm_selected) and (moaning_selected == help_eng_selected):\n",
    "                        moaning_selected += 1;\n",
    "                        break;\n",
    "                if (lbl1_int == 71 and lbl2_int == 99) or (lbl1_int == 99 and lbl2_int == 71):\n",
    "                    if (help_eng_selected < alarm_selected) and (help_eng_selected < moaning_selected):\n",
    "                        help_eng_selected += 1;\n",
    "                        break;\n",
    "            # print(f\"escape for loop\");\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random());\n",
    "            #######make wanted class mix ration above 0.5##########\n",
    "            # iLbl1 = np.int16(label1);\n",
    "            # iLbl2 = np.int16(label2);\n",
    "            # r = 1.0;\n",
    "            # p_ratio1 = 0.4\n",
    "            # p_ratio2 = 0.6\n",
    "            # while True:\n",
    "            #     r = np.array(random.random());\n",
    "            #     if r > p_ratio1 and iLbl1 != 99 :\n",
    "            #         break;\n",
    "            #     if r < p_ratio2 and iLbl2 != 99 :\n",
    "            #         break;\n",
    "            #######################End#######################\n",
    "            # print(f\"r:{r}, lbl1:{label1}, lbl2:{label2}\")  \n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[str(label1)]- 1\n",
    "            idx2 = self.mapdict[str(label2)] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            \n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        # print(f\"batchIndex is {batchIndex}, total sounds is {len(sounds)}\")\n",
    "        # print(f\"labels in generate_batch is:\\n{labels}\")\n",
    "        # print(f\"alarm_selected:{alarm_selected}, moaning_selected:{moaning_selected}\");\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess_setup_withoutt_normalize(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength)\n",
    "                 ]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b648ed6-051a-49d7-ad2a-b051c05e88eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainGen(opt=None, split=None):\n",
    "    dataset = np.load(opt.trainSet, allow_pickle=True);\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    # train_sounds = [dataset['x'][i][0] for i in range(len(dataset['x']))]\n",
    "    # train_labels = [dataset['y'][i][0] for i in range(len(dataset['y']))]\n",
    "    train_sounds = dataset['{}'.format(opt.current_fold)].item()['sounds']\n",
    "    train_labels = dataset['{}'.format(opt.current_fold)].item()['labels']\n",
    "    # print(train_sounds)\n",
    "\n",
    "    trainGen = TLGenerator(train_sounds, train_labels, opt);\n",
    "    trainGen.preprocess_setup();\n",
    "    return trainGen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8392ff48-c5c7-4e0f-a7f5-4eb243d1ab56",
   "metadata": {},
   "source": [
    "### option object creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6266cad-9de1-47c0-87dc-c8ade965a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='TLACDNet',  required=False);\n",
    "    parser.add_argument('--data', default='None',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args()\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f9873d-61b4-47b1-a0e5-95d8f156a7f2",
   "metadata": {},
   "source": [
    "### ACDNet Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "086db016-0acf-4029-9634-e79d30842ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Customed_ACDNetV2(nn.Module):\n",
    "    def __init__(self, input_length, n_class, sr, ch_conf=None):\n",
    "        super(Customed_ACDNetV2, self).__init__();\n",
    "        self.input_length = input_length;\n",
    "        self.ch_config = ch_conf;\n",
    "\n",
    "        stride1 = 2;\n",
    "        stride2 = 2;\n",
    "        channels = 8;\n",
    "        k_size = (3, 3);\n",
    "        n_frames = (sr/1000)*10; #No of frames per 10ms\n",
    "\n",
    "        sfeb_pool_size = int(n_frames/(stride1*stride2));\n",
    "        # tfeb_pool_size = (2,2);\n",
    "        if self.ch_config is None:\n",
    "            self.ch_config = [channels, channels*8, channels*4, channels*8, channels*8, channels*16, channels*16, channels*32, channels*32, channels*64, channels*64, n_class];\n",
    "        # avg_pool_kernel_size = (1,4) if self.ch_config[1] < 64 else (2,4);\n",
    "        fcn_no_of_inputs =  self.ch_config[-1];#n_class #self.ch_config[-1];\n",
    "        # ch_confing_10 = 512 #8 * 64\n",
    "        # ch_n_class = n_class\n",
    "        conv1, bn1 = self.make_layers(1, self.ch_config[0], (1, 9), (1, stride1));\n",
    "        conv2, bn2 = self.make_layers(self.ch_config[0], self.ch_config[1], (1, 5), (1, stride2));\n",
    "        conv3, bn3 = self.make_layers(1, self.ch_config[2], k_size, padding=1);\n",
    "        conv4, bn4 = self.make_layers(self.ch_config[2], self.ch_config[3], k_size, padding=1);\n",
    "        conv5, bn5 = self.make_layers(self.ch_config[3], self.ch_config[4], k_size, padding=1);\n",
    "        conv6, bn6 = self.make_layers(self.ch_config[4], self.ch_config[5], k_size, padding=1);\n",
    "        conv7, bn7 = self.make_layers(self.ch_config[5], self.ch_config[6], k_size, padding=1);\n",
    "        conv8, bn8 = self.make_layers(self.ch_config[6], self.ch_config[7], k_size, padding=1);\n",
    "        conv9, bn9 = self.make_layers(self.ch_config[7], self.ch_config[8], k_size, padding=1);\n",
    "        conv10, bn10 = self.make_layers(self.ch_config[8], self.ch_config[9], k_size, padding=1);\n",
    "        conv11, bn11 = self.make_layers(self.ch_config[9], self.ch_config[10], k_size, padding=1);\n",
    "        conv12, bn12 = self.make_layers(self.ch_config[10], self.ch_config[11], (1, 1));\n",
    "        fcn = nn.Linear(fcn_no_of_inputs, n_class);\n",
    "        nn.init.kaiming_normal_(fcn.weight, nonlinearity='sigmoid') # kaiming with sigoid is equivalent to lecun_normal in keras\n",
    "\n",
    "        self.sfeb = nn.Sequential(\n",
    "            #Start: Filter bank\n",
    "            conv1, bn1, nn.ReLU(),\\\n",
    "            conv2, bn2, nn.ReLU(),\\\n",
    "            nn.MaxPool2d(kernel_size=(1, sfeb_pool_size))\n",
    "        );\n",
    "\n",
    "        tfeb_modules = [];\n",
    "        self.tfeb_width = int(((self.input_length / sr)*1000)/10); # 10ms frames of audio length in seconds\n",
    "        tfeb_pool_sizes = self.get_tfeb_pool_sizes(self.ch_config[1], self.tfeb_width);\n",
    "        p_index = 0;\n",
    "        for i in [3,4,6,8,10]:\n",
    "            tfeb_modules.extend([eval('conv{}'.format(i)), eval('bn{}'.format(i)), nn.ReLU()]);\n",
    "\n",
    "            if i != 3:\n",
    "                tfeb_modules.extend([eval('conv{}'.format(i+1)), eval('bn{}'.format(i+1)), nn.ReLU()]);\n",
    "\n",
    "            h, w = tfeb_pool_sizes[p_index];\n",
    "            if h>1 or w>1:\n",
    "                tfeb_modules.append(nn.MaxPool2d(kernel_size = (h,w)));\n",
    "            p_index += 1;\n",
    "\n",
    "        tfeb_modules.append(nn.Dropout(0.2));\n",
    "        tfeb_modules.extend([conv12, bn12, nn.ReLU()]);\n",
    "        h, w = tfeb_pool_sizes[-1];\n",
    "        if h>1 or w>1:\n",
    "            tfeb_modules.append(nn.AvgPool2d(kernel_size = (h,w)));\n",
    "        tfeb_modules.extend([nn.Flatten(), fcn]);\n",
    "\n",
    "        self.tfeb = nn.Sequential(*tfeb_modules);\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Softmax(dim=1)\n",
    "        );\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sfeb(x);\n",
    "        #swapaxes\n",
    "        x = x.permute((0, 2, 1, 3));\n",
    "        x = self.tfeb(x);\n",
    "        y = self.output[0](x);\n",
    "        return y;\n",
    "\n",
    "    def make_layers(self, in_channels, out_channels, kernel_size, stride=(1,1), padding=0, bias=False):\n",
    "        conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias);\n",
    "        nn.init.kaiming_normal_(conv.weight, nonlinearity='relu'); # kaiming with relu is equivalent to he_normal in keras\n",
    "        bn = nn.BatchNorm2d(out_channels);\n",
    "        return conv, bn;\n",
    "\n",
    "    def get_tfeb_pool_sizes(self, con2_ch, width):\n",
    "        h = self.get_tfeb_pool_size_component(con2_ch);\n",
    "        w = self.get_tfeb_pool_size_component(width);\n",
    "        # print(w);\n",
    "        pool_size = [];\n",
    "        for  (h1, w1) in zip(h, w):\n",
    "            pool_size.append((h1, w1));\n",
    "        return pool_size;\n",
    "\n",
    "    def get_tfeb_pool_size_component(self, length):\n",
    "        # print(length);\n",
    "        c = [];\n",
    "        index = 1;\n",
    "        while index <= 6:\n",
    "            if length >= 2:\n",
    "                if index == 6:\n",
    "                    c.append(length);\n",
    "                else:\n",
    "                    c.append(2);\n",
    "                    length = length // 2;\n",
    "            else:\n",
    "               c.append(1);\n",
    "\n",
    "            index += 1;\n",
    "\n",
    "        return c;\n",
    "\n",
    "def GetCustomedACDNetModel(input_len=20150, nclass=4, sr=20000, channel_config=None):\n",
    "    net = Customed_ACDNetV2(input_len, nclass, sr, ch_conf=channel_config);\n",
    "    return net;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56cae6f7-e8e2-438f-a003-f4b111ccae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PruningTrainer:\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt;\n",
    "        self.opt.channels_to_prune_per_iteration = 1;\n",
    "        self.opt.finetune_epoch_per_iteration = 2;\n",
    "        self.opt.lr=0.001;\n",
    "        self.opt.schedule = [0.5, 0.8];\n",
    "        self.opt.prune_type = opt.prun_type; #determine the prunning algo, 1: Magnitude Pruning ;2: tylor-pruning\n",
    "        # torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"); #in office use \n",
    "        # self.opt.device = 'cuda:0'#office\n",
    "        self.opt.device = opt.device;#home\n",
    "        self.pruner = None;\n",
    "        self.iterations = 0;\n",
    "        self.cur_acc = 0.0;\n",
    "        self.cur_iter = 1;\n",
    "        self.cur_lr = self.opt.lr;\n",
    "        self.net = None;\n",
    "        self.criterion = torch.nn.KLDivLoss(reduction='batchmean');\n",
    "        self.trainGen = getTrainGen(opt)#train_generator.setup(self.opt, self.opt.split);\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "        self.load_test_data();\n",
    "\n",
    "    def PruneAndTrain(self):\n",
    "        print(f\"Start to Prune and Train Using device:{self.opt.device}\");\n",
    "        dir = os.getcwd();\n",
    "        \n",
    "        trained_model = \"../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold5_2024072310_prunratio85.0/selected/uec_4C_weight_prun_fold5_haacc_95.62841033935547_valacc94.53551483154297_tracc91.19318181818183_epoch_1078_20240723111906.pt\"\n",
    "        state= torch.load(trained_model, map_location=self.opt.device);\n",
    "        self.net = GetCustomedACDNetModel(channel_config=state[\"config\"]).to(self.opt.device);\n",
    "        self.net.load_state_dict(state['weight']);#home\n",
    "        self.net = self.net.to(self.opt.device);\n",
    "        \n",
    "        self.pruner = filter_pruning.Magnitude(self.net, self.opt) if self.opt.prune_type == 1 else filter_pruning.Taylor(self.net, self.opt);\n",
    "        self.validate();\n",
    "        calc.summary(self.net, (1, 1, self.opt.inputLength), brief=False); # shape of one sample for inferenceing\n",
    "        # exit();\n",
    "        #Make sure all the layers are trainable\n",
    "        for param in self.net.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.iterations = self.estimate_pruning_iterations();\n",
    "        # exit();\n",
    "        for i in range(1, self.iterations):\n",
    "            self.cur_iter = i;\n",
    "            iter_start = time.time();\n",
    "            print(\"\\nIteration {} of {} starts..\".format(i, self.iterations-1), flush=True);\n",
    "            print(\"Ranking channels.. \", flush=True);\n",
    "            prune_targets = self.get_candidates_to_prune(self.opt.channels_to_prune_per_iteration);\n",
    "            # prune_targets = [(40,3)];\n",
    "            print(\"Pruning channels: {}\".format(prune_targets), flush=True);\n",
    "            self.net = filter_pruner.prune_layers(self.net, prune_targets, self.opt.prune_all, self.opt.device);\n",
    "            calc.summary(self.net, (1, 1, self.opt.inputLength), brief=True); # shape of one sample for inferenceing\n",
    "            self.validate();\n",
    "            print(\"Fine tuning {} epochs to recover from prunning iteration.\".format(self.opt.finetune_epoch_per_iteration), flush=True);\n",
    "\n",
    "            if self.cur_iter in list(map(int, np.array(self.iterations)*self.opt.schedule)):\n",
    "                self.cur_lr *= 0.1;\n",
    "            optimizer = optim.SGD(self.net.parameters(), lr=self.cur_lr, momentum=0.9);\n",
    "            self.train(optimizer, epoches = self.opt.finetune_epoch_per_iteration);\n",
    "            print(\"Iteration {}/{} finished in {}\".format(self.cur_iter, self.iterations+1, U.to_hms(time.time()-iter_start)), flush=True);\n",
    "            print(\"Total channels prunned so far: {}\".format(i*self.opt.channels_to_prune_per_iteration), flush=True);\n",
    "            self.__save_model(self.net);\n",
    "\n",
    "        calc.summary(self.net, (1, 1, self.opt.inputLength)); # shape of one sample for inferenceing\n",
    "        self.__save_model(self.net);\n",
    "\n",
    "    def get_candidates_to_prune(self, num_filters_to_prune):\n",
    "        self.pruner.reset();\n",
    "        if self.opt.prune_type == 1:\n",
    "            self.pruner.compute_filter_magnitude();\n",
    "        else:\n",
    "            self.train_epoch(rank_filters = True);\n",
    "            self.pruner.normalize_ranks_per_layer();\n",
    "\n",
    "        return self.pruner.get_prunning_plan(num_filters_to_prune);\n",
    "\n",
    "    def estimate_pruning_iterations(self):\n",
    "        # get total number of variables from all conv2d featuremaps\n",
    "        prunable_count = sum(self.get_channel_list(self.opt.prune_all));\n",
    "        total_count= sum(self.get_channel_list());\n",
    "        #iterations_reqired = int((prunable_count * self.opt.prune_ratio) / self.opt.channels_to_prune_per_iteration);\n",
    "        #prune_ratio works with the total number of channels, not only with the prunable channels. i.e. 80% or total will be pruned from total or from only features\n",
    "        iterations_reqired = int((total_count * self.opt.prune_ratio) / self.opt.channels_to_prune_per_iteration);\n",
    "        print('Total Channels: {}, Prunable: {}, Non-Prunable: {}'.format(total_count, prunable_count, total_count - prunable_count), flush=True);\n",
    "        print('No. of Channels to prune per iteration: {}'.format(self.opt.channels_to_prune_per_iteration), flush=True);\n",
    "        print('Total Channels to prune ({}%): {}'.format(int(self.opt.prune_ratio*100), int(total_count * self.opt.prune_ratio)-1), flush=True);\n",
    "        print('Total iterations required: {}'.format(iterations_reqired-1), flush=True);\n",
    "        return iterations_reqired;\n",
    "\n",
    "    def get_channel_list(self, prune_all=True):\n",
    "        ch_conf = [];\n",
    "        if prune_all:\n",
    "            for name, module in enumerate(self.net.sfeb):\n",
    "                if issubclass(type(module), torch.nn.Conv2d):\n",
    "                    ch_conf.append(module.out_channels);\n",
    "\n",
    "        for name, module in enumerate(self.net.tfeb):\n",
    "            if issubclass(type(module), torch.nn.Conv2d):\n",
    "                ch_conf.append(module.out_channels);\n",
    "\n",
    "        return ch_conf;\n",
    "\n",
    "    def load_test_data(self):\n",
    "        if(self.testX is None):\n",
    "            data = np.load(self.opt.valSet, allow_pickle=True);\n",
    "            dataX = np.moveaxis(data['x'], 3, 1).astype(np.float32);\n",
    "            # self.testX = torch.tensor(dataX).cuda();\n",
    "            # self.testY = torch.tensor(data['y']).cuda();\n",
    "            self.testX = torch.tensor(dataX).to(self.opt.device);\n",
    "            # self.testY = torch.tensor(data['y']).to(self.opt.device);#in office, use cuda(better) or cpu\n",
    "            self.testY = torch.FloatTensor(data['y']).to(self.opt.device);#at home use apple m2\n",
    "\n",
    "    #Calculating average prediction (10 crops) and final accuracy\n",
    "    def compute_accuracy(self, y_pred, y_target):\n",
    "        with torch.no_grad():\n",
    "            #Reshape to shape theme like each sample comtains 10 samples, calculate mean and find the indices that has highest average value for each sample\n",
    "            y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "            y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "            y_target = y_target.cuda();\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "            # valLossFunc = torch.nn.KLDivLoss();\n",
    "            loss = self.criterion(y_pred.float().log(), y_target.float()).item();\n",
    "        return acc, loss;\n",
    "\n",
    "    def train(self, optimizer = None, epoches=10):\n",
    "        for i in range(epoches):\n",
    "            # print(\"Epoch: \", i);\n",
    "            self.train_epoch(optimizer);\n",
    "            self.validate();\n",
    "        print(\"Finished fine tuning.\", flush=True);\n",
    "\n",
    "    def train_batch(self, optimizer, batch, label, rank_filters):\n",
    "        self.net.zero_grad()\n",
    "        if rank_filters:\n",
    "            output = self.pruner.forward(batch);\n",
    "            if self.opt.device == \"mps\":\n",
    "                label = label.cpu() #use apple m2, in office use cuda\n",
    "                output = output.cpu() #use apple m2, in office use cuda\n",
    "            self.criterion(output.log(), label).backward();\n",
    "        else:\n",
    "            self.criterion(self.net(batch), label).backward();\n",
    "            optimizer.step();\n",
    "\n",
    "    def train_epoch(self, optimizer = None, rank_filters = False):\n",
    "        if rank_filters is False and optimizer is None:\n",
    "            print('Please provide optimizer to train_epoch', flush=True);\n",
    "            exit();\n",
    "        n_batches = math.ceil(len(self.trainGen.data)/self.opt.batchSize);\n",
    "        for b_idx in range(n_batches):\n",
    "            x,y = self.trainGen.__getitem__(b_idx)\n",
    "            x = torch.tensor(np.moveaxis(x, 3, 1)).to(self.opt.device);\n",
    "            y = torch.tensor(y).to(self.opt.device);\n",
    "            self.train_batch(optimizer, x, y, rank_filters);\n",
    "\n",
    "    def validate(self):\n",
    "        self.net.eval();\n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = (self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops;\n",
    "            for idx in range(math.ceil(len(self.testX)/batch_size)):\n",
    "                x = self.testX[idx*batch_size : (idx+1)*batch_size];\n",
    "                x = x.type(torch.cuda.FloatTensor);\n",
    "                scores = self.net(x);\n",
    "                y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "\n",
    "            acc, loss = self.compute_accuracy(y_pred, self.testY);\n",
    "        print('Current Testing Performance - Val: Loss {:.3f}  Acc(top1) {:.3f}%'.format(loss, acc), flush=True);\n",
    "        self.cur_acc = acc;\n",
    "        self.net.train();\n",
    "        return acc, loss;\n",
    "\n",
    "    def __save_model(self, net):\n",
    "        net.ch_config = self.get_channel_list();\n",
    "        dir = os.getcwd();\n",
    "        fname = self.opt.model_name;\n",
    "        if os.path.isfile(fname):\n",
    "            os.remove(fname);\n",
    "        torch.save({'weight':net.state_dict(), 'config':net.ch_config}, fname);\n",
    "\n",
    "        \n",
    "    # def __save_model(self, acc, train_acc, epochIdx, net):\n",
    "    #     if acc > self.bestAcc:\n",
    "    #         self.bestAcc = acc;\n",
    "    #         self.bestAccEpoch = epochIdx +1;\n",
    "    #         __do_save_model(self, acc, train_acc, self.bestAccEpoch, net);\n",
    "    #     else:\n",
    "    #         if acc > 94.0 or train_acc > 85.0: \n",
    "    #             __do_save_model(self, acc, train_acc, epochIdx, net);\n",
    "    #         else:\n",
    "    #             pass\n",
    "\n",
    "    # def __do_save_model(self, acc, tr_acc, bestAccIdx, net):\n",
    "    #     save_model_name = self.opt.model_name.format(self.bestAcc, acc, train_acc, epochIdx, genDataTimeStr());\n",
    "    #     save_model_fullpath = self.opt.save_dir + save_model_name;\n",
    "    #     print(f\"save model to {save_model_fullpath}\")\n",
    "    #     torch.save({'weight':net.state_dict(), 'config':net.ch_config}, save_model_fullpath);\n",
    "    #     logObj.write(f\"save model:{model_name}, bestAcc:{self.bestAcc}@{self.}, currentAcc:{acc}@{epochIdx}\");\n",
    "    #     logObj.write(\"\\n\");\n",
    "    #     logObj.flush();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d935fefb-1073-4894-aae9-9317b88dfd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"save and record the training hyperparameters and results\\npruning algo: tylor-pruning\\npruning ration : 0.85\\nfinal accuracy : \\nepoch: \\nself.opt.LR = 0.01;\\nopt.momentum = 0.009;\\nself.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\\nself.opt.warmup = 0;\\nself.opt.prune_algo = 'tylor-pruning';\\nself.opt.prune_interval = 1;\\nself.opt.nEpochs = 1000;\\n===============================================\\nprune_type = Magnitude Pruning\\npruning ration : 0.85\\nfinal accuracy : \\nepoch: \\nself.opt.LR = 0.01;\\nopt.momentum = 0.009;\\nself.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\\nself.opt.warmup = 0;\\nself.opt.prune_algo = 'tylor-pruning';\\nself.opt.prune_interval = 1;\\nself.opt.nEpochs = 1000;\\n=============================================\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"save and record the training hyperparameters and results\n",
    "pruning algo: tylor-pruning\n",
    "pruning ration : 0.85\n",
    "final accuracy : \n",
    "epoch: \n",
    "self.opt.LR = 0.01;\n",
    "opt.momentum = 0.009;\n",
    "self.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "self.opt.warmup = 0;\n",
    "self.opt.prune_algo = 'tylor-pruning';\n",
    "self.opt.prune_interval = 1;\n",
    "self.opt.nEpochs = 1000;\n",
    "===============================================\n",
    "prune_type = Magnitude Pruning\n",
    "pruning ration : 0.85\n",
    "final accuracy : \n",
    "epoch: \n",
    "self.opt.LR = 0.01;\n",
    "opt.momentum = 0.009;\n",
    "self.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "self.opt.warmup = 0;\n",
    "self.opt.prune_algo = 'tylor-pruning';\n",
    "self.opt.prune_interval = 1;\n",
    "self.opt.nEpochs = 1000;\n",
    "=============================================\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3888935-7f32-4d8e-bdc3-48761aa4e13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b05ced26-21b7-4292-b531-276490ea99e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    opt = getOpts()\n",
    "    #Learning settings\n",
    "    opt.batchSize = 64;\n",
    "    #set train and validation sets\n",
    "    # opt.trainSet = \"../../../../uec_iot_models_datasets/version11/single_fold_train_20240603063535.npz\" #office\n",
    "    # opt.valSet = \"../../../../uec_iot_models_datasets/version11/final_single_val_20240603063755.npz\" #office\n",
    "    opt.trainSet = \"../../../../uec_iot_models_datasets/multifold/train/version15_multifold_home_fold5/fold5_train_20240721013721.npz\";#home\n",
    "    opt.valSet = \"../../../../uec_iot_models_datasets/multifold/val/version15_multifold_home_fold5/final_fold5_val_version15_multifold_home_20240721024402.npz\";#home\n",
    "    #Basic Net Settings\n",
    "    opt.prune_ratio = 0.85\n",
    "    opt.prune_all = True;\n",
    "    opt.prun_type = 2; #determine the prunning algo, 1: Magnitude Pruning ;2: tylor-pruning\n",
    "    opt.nClasses = 4\n",
    "    opt.nFolds = 1;\n",
    "    opt.split = [i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.inputLength = 20150;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    opt.sr = 20000;\n",
    "    opt.trainer = None\n",
    "    if torch.backends.mps.is_available():\n",
    "        opt.device=\"mps\"; #for apple m2 gpu\n",
    "    elif torch.cuda.is_available():\n",
    "        opt.device=\"cuda:0\"; #for nVidia gpu\n",
    "    else:\n",
    "        opt.device=\"cpu\"\n",
    "    # opt.device = 'mps';#home\n",
    "    # tlopts.display_info(opt)\n",
    "    opt.current_fold='fold5';\n",
    "    save_dir = \"../../../trained_models/step_4_second_stage_pruning/multifold/s3_prun_{}_4C_{}_prunratio{}/\".format(opt.current_fold,getDateStr(),opt.prune_ratio*100)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    # \"uec_4C_weight_prun_{}_\".format(opt.current_fold)+\"haacc_{}_valacc{}_tracc{}_epoch_{}_{}.pt\"\n",
    "    model_name = \"uec_4C_IterPrun_{}_ratio{}_{}.pt\".format(opt.current_fold, (opt.prune_ratio*100), genDataTimeStr());\n",
    "    opt.model_name = save_dir + model_name;\n",
    "\n",
    "    print(f\"save model full path:{opt.model_name}\")\n",
    "    # valid_path = False;\n",
    "    print(\"Initializing PruneAndTrain Object.....\")\n",
    "    trainer = PruningTrainer(opt=opt)#TLTrainer(opt)\n",
    "    print(\"Start to pruning.....\")\n",
    "    trainer.PruneAndTrain();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c17937-4b7a-49f2-bb4c-cf323d891beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save model full path:../../../trained_models/step_4_second_stage_pruning/multifold/s3_prun_fold5_4C_2024072314_prunratio85.0/uec_4C_IterPrun_fold5_ratio85.0_20240723140833.pt\n",
      "Initializing PruneAndTrain Object.....\n",
      "length of samples:704\n",
      "Start to pruning.....\n",
      "Start to Prune and Train Using device:cuda:0\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.536%\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 20150)     (8, 1, 10071)         72      725,112\n",
      "  BatchNorm2d-2     (8, 1, 10071)     (8, 1, 10071)         16            0\n",
      "         ReLu-3     (8, 1, 10071)     (8, 1, 10071)          0       80,568\n",
      "       Conv2d-4     (8, 1, 10071)     (64, 1, 5034)      2,560   12,887,040\n",
      "  BatchNorm2d-5     (64, 1, 5034)     (64, 1, 5034)        128            0\n",
      "         ReLu-6     (64, 1, 5034)     (64, 1, 5034)          0      322,176\n",
      "    MaxPool2d-7     (64, 1, 5034)      (64, 1, 100)          0      320,000\n",
      "      Permute-8      (64, 1, 100)      (1, 64, 100)          0            0\n",
      "       Conv2d-9      (1, 64, 100)     (32, 64, 100)        288    1,843,200\n",
      " BatchNorm2d-10     (32, 64, 100)     (32, 64, 100)         64            0\n",
      "        ReLu-11     (32, 64, 100)     (32, 64, 100)          0      204,800\n",
      "   MaxPool2d-12     (32, 64, 100)      (32, 32, 50)          0      204,800\n",
      "      Conv2d-13      (32, 32, 50)      (64, 32, 50)     18,432   29,491,200\n",
      " BatchNorm2d-14      (64, 32, 50)      (64, 32, 50)        128            0\n",
      "        ReLu-15      (64, 32, 50)      (64, 32, 50)          0      102,400\n",
      "      Conv2d-16      (64, 32, 50)      (64, 32, 50)     36,864   58,982,400\n",
      " BatchNorm2d-17      (64, 32, 50)      (64, 32, 50)        128            0\n",
      "        ReLu-18      (64, 32, 50)      (64, 32, 50)          0      102,400\n",
      "   MaxPool2d-19      (64, 32, 50)      (64, 16, 25)          0      102,400\n",
      "      Conv2d-20      (64, 16, 25)     (128, 16, 25)     73,728   29,491,200\n",
      " BatchNorm2d-21     (128, 16, 25)     (128, 16, 25)        256            0\n",
      "        ReLu-22     (128, 16, 25)     (128, 16, 25)          0       51,200\n",
      "      Conv2d-23     (128, 16, 25)     (128, 16, 25)    147,456   58,982,400\n",
      " BatchNorm2d-24     (128, 16, 25)     (128, 16, 25)        256            0\n",
      "        ReLu-25     (128, 16, 25)     (128, 16, 25)          0       51,200\n",
      "   MaxPool2d-26     (128, 16, 25)      (128, 8, 12)          0       49,152\n",
      "      Conv2d-27      (128, 8, 12)      (256, 8, 12)    294,912   28,311,552\n",
      " BatchNorm2d-28      (256, 8, 12)      (256, 8, 12)        512            0\n",
      "        ReLu-29      (256, 8, 12)      (256, 8, 12)          0       24,576\n",
      "      Conv2d-30      (256, 8, 12)      (256, 8, 12)    589,824   56,623,104\n",
      " BatchNorm2d-31      (256, 8, 12)      (256, 8, 12)        512            0\n",
      "        ReLu-32      (256, 8, 12)      (256, 8, 12)          0       24,576\n",
      "   MaxPool2d-33      (256, 8, 12)       (256, 4, 6)          0       24,576\n",
      "      Conv2d-34       (256, 4, 6)       (512, 4, 6)  1,179,648   28,311,552\n",
      " BatchNorm2d-35       (512, 4, 6)       (512, 4, 6)      1,024            0\n",
      "        ReLu-36       (512, 4, 6)       (512, 4, 6)          0       12,288\n",
      "      Conv2d-37       (512, 4, 6)       (512, 4, 6)  2,359,296   56,623,104\n",
      " BatchNorm2d-38       (512, 4, 6)       (512, 4, 6)      1,024            0\n",
      "        ReLu-39       (512, 4, 6)       (512, 4, 6)          0       12,288\n",
      "   MaxPool2d-40       (512, 4, 6)       (512, 2, 3)          0       12,288\n",
      "      Conv2d-41       (512, 2, 3)         (4, 2, 3)      2,048       12,288\n",
      " BatchNorm2d-42         (4, 2, 3)         (4, 2, 3)          8            0\n",
      "        ReLu-43         (4, 2, 3)         (4, 2, 3)          0           24\n",
      "   AvgPool2d-44         (4, 2, 3)         (4, 1, 1)          0           24\n",
      "     Flatten-45         (4, 1, 1)            (1, 4)          0            0\n",
      "      Linear-46            (1, 4)            (1, 4)         20           20\n",
      "     Softmax-47            (1, 4)            (1, 4)          0            4\n",
      "==============================================================================\n",
      "Total Params: 4,709,204\n",
      "Total FLOPs : 363,985,912\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.08\n",
      "Params size (MB): 17.96\n",
      "Total size (MB) : 18.04\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Total Channels: 2028, Prunable: 2028, Non-Prunable: 0\n",
      "No. of Channels to prune per iteration: 1\n",
      "Total Channels to prune (85%): 1722\n",
      "Total iterations required: 1722\n",
      "\n",
      "Iteration 1 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 5)]\n",
      "Input: 0.077 MB, Params: 4,709,162 (17.964 MB), Total: 18.04 MB, FLOPs: 323,535,134\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 28.415%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Testing Performance - Val: Loss nan  Acc(top1) 43.169%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 50.820%\n",
      "Finished fine tuning.\n",
      "Iteration 1/1724 finished in 0m10s\n",
      "Total channels prunned so far: 1\n",
      "\n",
      "Iteration 2 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 6)]\n",
      "Input: 0.077 MB, Params: 4,709,120 (17.964 MB), Total: 18.04 MB, FLOPs: 323,291,740\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.896%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 51.913%\n",
      "Finished fine tuning.\n",
      "Iteration 2/1724 finished in 0m09s\n",
      "Total channels prunned so far: 2\n",
      "\n",
      "Iteration 3 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 24)]\n",
      "Input: 0.077 MB, Params: 4,709,078 (17.964 MB), Total: 18.04 MB, FLOPs: 320,270,746\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 48.087%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 43.169%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 44.262%\n",
      "Finished fine tuning.\n",
      "Iteration 3/1724 finished in 0m09s\n",
      "Total channels prunned so far: 3\n",
      "\n",
      "Iteration 4 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 30)]\n",
      "Input: 0.077 MB, Params: 4,709,036 (17.964 MB), Total: 18.04 MB, FLOPs: 320,027,352\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 49.180%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 45.902%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 45.355%\n",
      "Finished fine tuning.\n",
      "Iteration 4/1724 finished in 0m09s\n",
      "Total channels prunned so far: 4\n",
      "\n",
      "Iteration 5 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 31)]\n",
      "Input: 0.077 MB, Params: 4,708,994 (17.963 MB), Total: 18.04 MB, FLOPs: 311,463,958\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 46.448%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 51.366%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 51.366%\n",
      "Finished fine tuning.\n",
      "Iteration 5/1724 finished in 0m09s\n",
      "Total channels prunned so far: 5\n",
      "\n",
      "Iteration 6 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 38)]\n",
      "Input: 0.077 MB, Params: 4,708,952 (17.963 MB), Total: 18.04 MB, FLOPs: 311,220,564\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 45.902%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 55.191%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 59.016%\n",
      "Finished fine tuning.\n",
      "Iteration 6/1724 finished in 0m09s\n",
      "Total channels prunned so far: 6\n",
      "\n",
      "Iteration 7 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(7, 2)]\n",
      "Input: 0.077 MB, Params: 4,708,365 (17.961 MB), Total: 18.04 MB, FLOPs: 310,321,564\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 49.727%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 52.459%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 58.470%\n",
      "Finished fine tuning.\n",
      "Iteration 7/1724 finished in 0m09s\n",
      "Total channels prunned so far: 7\n",
      "\n",
      "Iteration 8 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(7, 7)]\n",
      "Input: 0.077 MB, Params: 4,707,778 (17.959 MB), Total: 18.04 MB, FLOPs: 309,422,564\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 54.645%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 53.552%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 56.831%\n",
      "Finished fine tuning.\n",
      "Iteration 8/1724 finished in 0m09s\n",
      "Total channels prunned so far: 8\n",
      "\n",
      "Iteration 9 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(7, 14)]\n",
      "Input: 0.077 MB, Params: 4,707,191 (17.957 MB), Total: 18.03 MB, FLOPs: 308,523,564\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 51.913%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.060%\n",
      "Finished fine tuning.\n",
      "Iteration 9/1724 finished in 0m09s\n",
      "Total channels prunned so far: 9\n",
      "\n",
      "Iteration 10 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 48)]\n",
      "Input: 0.077 MB, Params: 4,703,733 (17.943 MB), Total: 18.02 MB, FLOPs: 308,233,176\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 66.120%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 55.738%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 63.934%\n",
      "Finished fine tuning.\n",
      "Iteration 10/1724 finished in 0m09s\n",
      "Total channels prunned so far: 10\n",
      "\n",
      "Iteration 11 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 35)]\n",
      "Input: 0.077 MB, Params: 4,699,119 (17.926 MB), Total: 18.00 MB, FLOPs: 308,150,190\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 75.410%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 51.366%\n",
      "Finished fine tuning.\n",
      "Iteration 11/1724 finished in 0m09s\n",
      "Total channels prunned so far: 11\n",
      "\n",
      "Iteration 12 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 305)]\n",
      "Input: 0.077 MB, Params: 4,694,505 (17.908 MB), Total: 17.98 MB, FLOPs: 308,067,204\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 68.852%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 12/1724 finished in 0m09s\n",
      "Total channels prunned so far: 12\n",
      "\n",
      "Iteration 13 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 321)]\n",
      "Input: 0.077 MB, Params: 4,689,891 (17.891 MB), Total: 17.97 MB, FLOPs: 307,984,218\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 57.923%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 68.852%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 54.098%\n",
      "Finished fine tuning.\n",
      "Iteration 13/1724 finished in 0m09s\n",
      "Total channels prunned so far: 13\n",
      "\n",
      "Iteration 14 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 399)]\n",
      "Input: 0.077 MB, Params: 4,685,277 (17.873 MB), Total: 17.95 MB, FLOPs: 307,901,232\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 65.027%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 67.213%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 57.377%\n",
      "Finished fine tuning.\n",
      "Iteration 14/1724 finished in 0m09s\n",
      "Total channels prunned so far: 14\n",
      "\n",
      "Iteration 15 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 444)]\n",
      "Input: 0.077 MB, Params: 4,680,663 (17.855 MB), Total: 17.93 MB, FLOPs: 307,818,246\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 54.645%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 72.678%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 68.852%\n",
      "Finished fine tuning.\n",
      "Iteration 15/1724 finished in 0m09s\n",
      "Total channels prunned so far: 15\n",
      "\n",
      "Iteration 16 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(14, 12)]\n",
      "Input: 0.077 MB, Params: 4,678,933 (17.849 MB), Total: 17.93 MB, FLOPs: 306,576,996\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 82.514%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 56.831%\n",
      "Finished fine tuning.\n",
      "Iteration 16/1724 finished in 0m09s\n",
      "Total channels prunned so far: 16\n",
      "\n",
      "Iteration 17 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 366)]\n",
      "Input: 0.077 MB, Params: 4,674,319 (17.831 MB), Total: 17.91 MB, FLOPs: 306,494,010\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 71.585%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 74.317%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 50.273%\n",
      "Finished fine tuning.\n",
      "Iteration 17/1724 finished in 0m09s\n",
      "Total channels prunned so far: 17\n",
      "\n",
      "Iteration 18 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 210)]\n",
      "Input: 0.077 MB, Params: 4,667,459 (17.805 MB), Total: 17.88 MB, FLOPs: 306,370,548\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 56.831%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 55.191%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.607%\n",
      "Finished fine tuning.\n",
      "Iteration 18/1724 finished in 0m09s\n",
      "Total channels prunned so far: 18\n",
      "\n",
      "Iteration 19 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 182)]\n",
      "Input: 0.077 MB, Params: 4,662,854 (17.787 MB), Total: 17.86 MB, FLOPs: 306,287,724\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 77.049%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 59.016%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 74.317%\n",
      "Finished fine tuning.\n",
      "Iteration 19/1724 finished in 0m09s\n",
      "Total channels prunned so far: 19\n",
      "\n",
      "Iteration 20 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 130)]\n",
      "Input: 0.077 MB, Params: 4,656,003 (17.761 MB), Total: 17.84 MB, FLOPs: 306,164,424\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 82.514%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 55.738%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 53.552%\n",
      "Finished fine tuning.\n",
      "Iteration 20/1724 finished in 0m09s\n",
      "Total channels prunned so far: 20\n",
      "\n",
      "Iteration 21 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 5)]\n",
      "Input: 0.077 MB, Params: 4,651,407 (17.744 MB), Total: 17.82 MB, FLOPs: 306,081,762\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 57.377%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 52.459%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.328%\n",
      "Finished fine tuning.\n",
      "Iteration 21/1724 finished in 0m09s\n",
      "Total channels prunned so far: 21\n",
      "\n",
      "Iteration 22 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 147)]\n",
      "Input: 0.077 MB, Params: 4,644,520 (17.717 MB), Total: 17.79 MB, FLOPs: 305,806,206\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 73.224%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.060%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 53.552%\n",
      "Finished fine tuning.\n",
      "Iteration 22/1724 finished in 0m09s\n",
      "Total channels prunned so far: 22\n",
      "\n",
      "Iteration 23 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 126)]\n",
      "Input: 0.077 MB, Params: 4,641,071 (17.704 MB), Total: 17.78 MB, FLOPs: 305,516,574\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 53.552%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 47.541%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 49.727%\n",
      "Finished fine tuning.\n",
      "Iteration 23/1724 finished in 0m09s\n",
      "Total channels prunned so far: 23\n",
      "\n",
      "Iteration 24 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 368)]\n",
      "Input: 0.077 MB, Params: 4,636,475 (17.687 MB), Total: 17.76 MB, FLOPs: 305,433,912\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 59.016%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 56.284%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 46.995%\n",
      "Finished fine tuning.\n",
      "Iteration 24/1724 finished in 0m09s\n",
      "Total channels prunned so far: 24\n",
      "\n",
      "Iteration 25 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(11, 1)]\n",
      "Input: 0.077 MB, Params: 4,635,645 (17.684 MB), Total: 17.76 MB, FLOPs: 304,231,862\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 49.180%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 59.563%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Finished fine tuning.\n",
      "Iteration 25/1724 finished in 0m09s\n",
      "Total channels prunned so far: 25\n",
      "\n",
      "Iteration 26 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 303)]\n",
      "Input: 0.077 MB, Params: 4,628,821 (17.658 MB), Total: 17.73 MB, FLOPs: 304,109,048\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 77.596%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 74.863%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 74.317%\n",
      "Finished fine tuning.\n",
      "Iteration 26/1724 finished in 0m09s\n",
      "Total channels prunned so far: 26\n",
      "\n",
      "Iteration 27 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 173)]\n",
      "Input: 0.077 MB, Params: 4,621,997 (17.632 MB), Total: 17.71 MB, FLOPs: 303,986,234\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 72.131%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 53.005%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 49.180%\n",
      "Finished fine tuning.\n",
      "Iteration 27/1724 finished in 0m09s\n",
      "Total channels prunned so far: 27\n",
      "\n",
      "Iteration 28 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 239)]\n",
      "Input: 0.077 MB, Params: 4,615,173 (17.605 MB), Total: 17.68 MB, FLOPs: 303,863,420\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 70.492%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 45.902%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 51.366%\n",
      "Finished fine tuning.\n",
      "Iteration 28/1724 finished in 0m09s\n",
      "Total channels prunned so far: 28\n",
      "\n",
      "Iteration 29 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 148)]\n",
      "Input: 0.077 MB, Params: 4,608,322 (17.579 MB), Total: 17.66 MB, FLOPs: 303,589,106\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 58.470%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 52.459%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 51.913%\n",
      "Finished fine tuning.\n",
      "Iteration 29/1724 finished in 0m09s\n",
      "Total channels prunned so far: 29\n",
      "\n",
      "Iteration 30 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 341)]\n",
      "Input: 0.077 MB, Params: 4,601,507 (17.553 MB), Total: 17.63 MB, FLOPs: 303,466,454\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 54.645%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 55.738%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 56.831%\n",
      "Finished fine tuning.\n",
      "Iteration 30/1724 finished in 0m09s\n",
      "Total channels prunned so far: 30\n",
      "\n",
      "Iteration 31 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(21, 82)]\n",
      "Input: 0.077 MB, Params: 4,598,067 (17.540 MB), Total: 17.62 MB, FLOPs: 302,870,544\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 55.191%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 54.098%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 53.552%\n",
      "Finished fine tuning.\n",
      "Iteration 31/1724 finished in 0m09s\n",
      "Total channels prunned so far: 31\n",
      "\n",
      "Iteration 32 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 406)]\n",
      "Input: 0.077 MB, Params: 4,593,507 (17.523 MB), Total: 17.60 MB, FLOPs: 302,788,530\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 53.552%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 71.038%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 69.945%\n",
      "Finished fine tuning.\n",
      "Iteration 32/1724 finished in 0m09s\n",
      "Total channels prunned so far: 32\n",
      "\n",
      "Iteration 33 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 31)]\n",
      "Input: 0.077 MB, Params: 4,591,795 (17.516 MB), Total: 17.59 MB, FLOPs: 302,189,680\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 46.995%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 50.273%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 48.087%\n",
      "Finished fine tuning.\n",
      "Iteration 33/1724 finished in 0m09s\n",
      "Total channels prunned so far: 33\n",
      "\n",
      "Iteration 34 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 138)]\n",
      "Input: 0.077 MB, Params: 4,584,989 (17.490 MB), Total: 17.57 MB, FLOPs: 302,067,190\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 49.180%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 50.273%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 57.923%\n",
      "Finished fine tuning.\n",
      "Iteration 34/1724 finished in 0m09s\n",
      "Total channels prunned so far: 34\n",
      "\n",
      "Iteration 35 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 301)]\n",
      "Input: 0.077 MB, Params: 4,578,183 (17.464 MB), Total: 17.54 MB, FLOPs: 301,944,700\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 77.049%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 73.770%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 60.109%\n",
      "Finished fine tuning.\n",
      "Iteration 35/1724 finished in 0m09s\n",
      "Total channels prunned so far: 35\n",
      "\n",
      "Iteration 36 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 428)]\n",
      "Input: 0.077 MB, Params: 4,571,377 (17.438 MB), Total: 17.52 MB, FLOPs: 301,822,210\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 57.377%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 54.645%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 48.087%\n",
      "Finished fine tuning.\n",
      "Iteration 36/1724 finished in 0m09s\n",
      "Total channels prunned so far: 36\n",
      "\n",
      "Iteration 37 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 466)]\n",
      "Input: 0.077 MB, Params: 4,566,844 (17.421 MB), Total: 17.50 MB, FLOPs: 301,740,682\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 55.191%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 54.098%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 68.306%\n",
      "Finished fine tuning.\n",
      "Iteration 37/1724 finished in 0m09s\n",
      "Total channels prunned so far: 37\n",
      "\n",
      "Iteration 38 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 154)]\n",
      "Input: 0.077 MB, Params: 4,563,413 (17.408 MB), Total: 17.48 MB, FLOPs: 301,452,562\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 60.109%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 66.120%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 58.470%\n",
      "Finished fine tuning.\n",
      "Iteration 38/1724 finished in 0m09s\n",
      "Total channels prunned so far: 38\n",
      "\n",
      "Iteration 39 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 59)]\n",
      "Input: 0.077 MB, Params: 4,561,701 (17.402 MB), Total: 17.48 MB, FLOPs: 300,853,712\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 63.388%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 75.410%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 60.109%\n",
      "Finished fine tuning.\n",
      "Iteration 39/1724 finished in 0m09s\n",
      "Total channels prunned so far: 39\n",
      "\n",
      "Iteration 40 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 465)]\n",
      "Input: 0.077 MB, Params: 4,557,168 (17.384 MB), Total: 17.46 MB, FLOPs: 300,772,184\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 57.923%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 66.120%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 74.317%\n",
      "Finished fine tuning.\n",
      "Iteration 40/1724 finished in 0m09s\n",
      "Total channels prunned so far: 40\n",
      "\n",
      "Iteration 41 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 180)]\n",
      "Input: 0.077 MB, Params: 4,550,362 (17.358 MB), Total: 17.44 MB, FLOPs: 300,499,274\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 65.027%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 77.049%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 69.399%\n",
      "Finished fine tuning.\n",
      "Iteration 41/1724 finished in 0m09s\n",
      "Total channels prunned so far: 41\n",
      "\n",
      "Iteration 42 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 155)]\n",
      "Input: 0.077 MB, Params: 4,545,829 (17.341 MB), Total: 17.42 MB, FLOPs: 300,417,746\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 79.235%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 58.470%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 79.235%\n",
      "Finished fine tuning.\n",
      "Iteration 42/1724 finished in 0m09s\n",
      "Total channels prunned so far: 42\n",
      "\n",
      "Iteration 43 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 206)]\n",
      "Input: 0.077 MB, Params: 4,539,059 (17.315 MB), Total: 17.39 MB, FLOPs: 300,295,904\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 71.585%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.328%\n",
      "Finished fine tuning.\n",
      "Iteration 43/1724 finished in 0m09s\n",
      "Total channels prunned so far: 43\n",
      "\n",
      "Iteration 44 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 174)]\n",
      "Input: 0.077 MB, Params: 4,532,289 (17.289 MB), Total: 17.37 MB, FLOPs: 300,174,062\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 60.109%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 71.585%\n",
      "Finished fine tuning.\n",
      "Iteration 44/1724 finished in 0m09s\n",
      "Total channels prunned so far: 44\n",
      "\n",
      "Iteration 45 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 225)]\n",
      "Input: 0.077 MB, Params: 4,527,774 (17.272 MB), Total: 17.35 MB, FLOPs: 300,092,858\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 59.016%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 74.317%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 61.202%\n",
      "Finished fine tuning.\n",
      "Iteration 45/1724 finished in 0m09s\n",
      "Total channels prunned so far: 45\n",
      "\n",
      "Iteration 46 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 281)]\n",
      "Input: 0.077 MB, Params: 4,523,259 (17.255 MB), Total: 17.33 MB, FLOPs: 300,011,654\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 49.180%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 54.645%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 61.202%\n",
      "Finished fine tuning.\n",
      "Iteration 46/1724 finished in 0m09s\n",
      "Total channels prunned so far: 46\n",
      "\n",
      "Iteration 47 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 134)]\n",
      "Input: 0.077 MB, Params: 4,518,744 (17.238 MB), Total: 17.31 MB, FLOPs: 299,930,450\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 81.967%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 57.923%\n",
      "Finished fine tuning.\n",
      "Iteration 47/1724 finished in 0m09s\n",
      "Total channels prunned so far: 47\n",
      "\n",
      "Iteration 48 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 267)]\n",
      "Input: 0.077 MB, Params: 4,512,001 (17.212 MB), Total: 17.29 MB, FLOPs: 299,809,094\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 64.481%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 56.284%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 48.634%\n",
      "Finished fine tuning.\n",
      "Iteration 48/1724 finished in 0m09s\n",
      "Total channels prunned so far: 48\n",
      "\n",
      "Iteration 49 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 204)]\n",
      "Input: 0.077 MB, Params: 4,507,495 (17.195 MB), Total: 17.27 MB, FLOPs: 299,728,052\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 57.923%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 68.852%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Finished fine tuning.\n",
      "Iteration 49/1724 finished in 0m09s\n",
      "Total channels prunned so far: 49\n",
      "\n",
      "Iteration 50 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 30)]\n",
      "Input: 0.077 MB, Params: 4,507,453 (17.195 MB), Total: 17.27 MB, FLOPs: 296,867,358\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 68.852%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.874%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 81.967%\n",
      "Finished fine tuning.\n",
      "Iteration 50/1724 finished in 0m09s\n",
      "Total channels prunned so far: 50\n",
      "\n",
      "Iteration 51 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(21, 0)]\n",
      "Input: 0.077 MB, Params: 4,504,040 (17.182 MB), Total: 17.26 MB, FLOPs: 296,278,504\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.874%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Finished fine tuning.\n",
      "Iteration 51/1724 finished in 0m09s\n",
      "Total channels prunned so far: 51\n",
      "\n",
      "Iteration 52 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 491)]\n",
      "Input: 0.077 MB, Params: 4,497,306 (17.156 MB), Total: 17.23 MB, FLOPs: 296,157,310\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 63.934%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 75.410%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 81.421%\n",
      "Finished fine tuning.\n",
      "Iteration 52/1724 finished in 0m09s\n",
      "Total channels prunned so far: 52\n",
      "\n",
      "Iteration 53 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 249)]\n",
      "Input: 0.077 MB, Params: 4,492,809 (17.139 MB), Total: 17.22 MB, FLOPs: 296,076,430\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 55.738%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 66.667%\n",
      "Finished fine tuning.\n",
      "Iteration 53/1724 finished in 0m09s\n",
      "Total channels prunned so far: 53\n",
      "\n",
      "Iteration 54 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 30)]\n",
      "Input: 0.077 MB, Params: 4,488,312 (17.122 MB), Total: 17.20 MB, FLOPs: 295,995,550\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 55.738%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Finished fine tuning.\n",
      "Iteration 54/1724 finished in 0m09s\n",
      "Total channels prunned so far: 54\n",
      "\n",
      "Iteration 55 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 152)]\n",
      "Input: 0.077 MB, Params: 4,484,899 (17.109 MB), Total: 17.19 MB, FLOPs: 295,708,942\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.874%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 55/1724 finished in 0m09s\n",
      "Total channels prunned so far: 55\n",
      "\n",
      "Iteration 56 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 400)]\n",
      "Input: 0.077 MB, Params: 4,480,402 (17.091 MB), Total: 17.17 MB, FLOPs: 295,628,062\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 56/1724 finished in 0m09s\n",
      "Total channels prunned so far: 56\n",
      "\n",
      "Iteration 57 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 165)]\n",
      "Input: 0.077 MB, Params: 4,475,905 (17.074 MB), Total: 17.15 MB, FLOPs: 295,547,182\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 66.667%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 50.820%\n",
      "Finished fine tuning.\n",
      "Iteration 57/1724 finished in 0m09s\n",
      "Total channels prunned so far: 57\n",
      "\n",
      "Iteration 58 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 155)]\n",
      "Input: 0.077 MB, Params: 4,471,408 (17.057 MB), Total: 17.13 MB, FLOPs: 295,466,302\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 51.366%\n",
      "Finished fine tuning.\n",
      "Iteration 58/1724 finished in 0m09s\n",
      "Total channels prunned so far: 58\n",
      "\n",
      "Iteration 59 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 196)]\n",
      "Input: 0.077 MB, Params: 4,464,647 (17.031 MB), Total: 17.11 MB, FLOPs: 295,194,796\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 57.377%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 57.377%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 59/1724 finished in 0m09s\n",
      "Total channels prunned so far: 59\n",
      "\n",
      "Iteration 60 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 128)]\n",
      "Input: 0.077 MB, Params: 4,457,886 (17.005 MB), Total: 17.08 MB, FLOPs: 294,923,290\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Finished fine tuning.\n",
      "Iteration 60/1724 finished in 0m09s\n",
      "Total channels prunned so far: 60\n",
      "\n",
      "Iteration 61 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 231)]\n",
      "Input: 0.077 MB, Params: 4,453,389 (16.988 MB), Total: 17.07 MB, FLOPs: 294,842,410\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 53.005%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 51.913%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 79.781%\n",
      "Finished fine tuning.\n",
      "Iteration 61/1724 finished in 0m09s\n",
      "Total channels prunned so far: 61\n",
      "\n",
      "Iteration 62 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 138)]\n",
      "Input: 0.077 MB, Params: 4,449,994 (16.975 MB), Total: 17.05 MB, FLOPs: 294,557,314\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 72.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 62/1724 finished in 0m09s\n",
      "Total channels prunned so far: 62\n",
      "\n",
      "Iteration 63 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 416)]\n",
      "Input: 0.077 MB, Params: 4,443,332 (16.950 MB), Total: 17.03 MB, FLOPs: 294,437,416\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 65.027%\n",
      "Finished fine tuning.\n",
      "Iteration 63/1724 finished in 0m09s\n",
      "Total channels prunned so far: 63\n",
      "\n",
      "Iteration 64 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 269)]\n",
      "Input: 0.077 MB, Params: 4,436,670 (16.925 MB), Total: 17.00 MB, FLOPs: 294,317,518\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 78.689%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 64/1724 finished in 0m09s\n",
      "Total channels prunned so far: 64\n",
      "\n",
      "Iteration 65 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 468)]\n",
      "Input: 0.077 MB, Params: 4,432,191 (16.907 MB), Total: 16.98 MB, FLOPs: 294,236,962\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 47.541%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 44.262%\n",
      "Finished fine tuning.\n",
      "Iteration 65/1724 finished in 0m09s\n",
      "Total channels prunned so far: 65\n",
      "\n",
      "Iteration 66 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 37)]\n",
      "Input: 0.077 MB, Params: 4,428,796 (16.895 MB), Total: 16.97 MB, FLOPs: 293,951,866\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 47.541%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 59.563%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 66.667%\n",
      "Finished fine tuning.\n",
      "Iteration 66/1724 finished in 0m09s\n",
      "Total channels prunned so far: 66\n",
      "\n",
      "Iteration 67 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 396)]\n",
      "Input: 0.077 MB, Params: 4,422,143 (16.869 MB), Total: 16.95 MB, FLOPs: 293,832,130\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 75.410%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 67/1724 finished in 0m09s\n",
      "Total channels prunned so far: 67\n",
      "\n",
      "Iteration 68 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 401)]\n",
      "Input: 0.077 MB, Params: 4,415,490 (16.844 MB), Total: 16.92 MB, FLOPs: 293,712,394\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 74.317%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 51.913%\n",
      "Finished fine tuning.\n",
      "Iteration 68/1724 finished in 0m09s\n",
      "Total channels prunned so far: 68\n",
      "\n",
      "Iteration 69 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 248)]\n",
      "Input: 0.077 MB, Params: 4,411,029 (16.827 MB), Total: 16.90 MB, FLOPs: 293,632,162\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 52.459%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 73.224%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Finished fine tuning.\n",
      "Iteration 69/1724 finished in 0m09s\n",
      "Total channels prunned so far: 69\n",
      "\n",
      "Iteration 70 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(14, 51)]\n",
      "Input: 0.077 MB, Params: 4,409,326 (16.820 MB), Total: 16.90 MB, FLOPs: 292,438,662\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 54.098%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 73.770%\n",
      "Finished fine tuning.\n",
      "Iteration 70/1724 finished in 0m09s\n",
      "Total channels prunned so far: 70\n",
      "\n",
      "Iteration 71 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 282)]\n",
      "Input: 0.077 MB, Params: 4,404,865 (16.803 MB), Total: 16.88 MB, FLOPs: 292,358,430\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 58.470%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 58.470%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 65.027%\n",
      "Finished fine tuning.\n",
      "Iteration 71/1724 finished in 0m09s\n",
      "Total channels prunned so far: 71\n",
      "\n",
      "Iteration 72 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 381)]\n",
      "Input: 0.077 MB, Params: 4,400,404 (16.786 MB), Total: 16.86 MB, FLOPs: 292,278,198\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 68.852%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 65.574%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Finished fine tuning.\n",
      "Iteration 72/1724 finished in 0m09s\n",
      "Total channels prunned so far: 72\n",
      "\n",
      "Iteration 73 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 43)]\n",
      "Input: 0.077 MB, Params: 4,398,710 (16.780 MB), Total: 16.86 MB, FLOPs: 291,685,648\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 74.317%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Finished fine tuning.\n",
      "Iteration 73/1724 finished in 0m09s\n",
      "Total channels prunned so far: 73\n",
      "\n",
      "Iteration 74 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 405)]\n",
      "Input: 0.077 MB, Params: 4,392,084 (16.754 MB), Total: 16.83 MB, FLOPs: 291,566,398\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 79.781%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Finished fine tuning.\n",
      "Iteration 74/1724 finished in 0m09s\n",
      "Total channels prunned so far: 74\n",
      "\n",
      "Iteration 75 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 493)]\n",
      "Input: 0.077 MB, Params: 4,385,458 (16.729 MB), Total: 16.81 MB, FLOPs: 291,447,148\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 50.820%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.060%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Finished fine tuning.\n",
      "Iteration 75/1724 finished in 0m09s\n",
      "Total channels prunned so far: 75\n",
      "\n",
      "Iteration 76 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 390)]\n",
      "Input: 0.077 MB, Params: 4,381,015 (16.712 MB), Total: 16.79 MB, FLOPs: 291,367,240\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 66.667%\n",
      "Finished fine tuning.\n",
      "Iteration 76/1724 finished in 0m09s\n",
      "Total channels prunned so far: 76\n",
      "\n",
      "Iteration 77 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 67)]\n",
      "Input: 0.077 MB, Params: 4,374,326 (16.687 MB), Total: 16.76 MB, FLOPs: 291,098,218\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 49.180%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 58.470%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 64.481%\n",
      "Finished fine tuning.\n",
      "Iteration 77/1724 finished in 0m09s\n",
      "Total channels prunned so far: 77\n",
      "\n",
      "Iteration 78 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(14, 28)]\n",
      "Input: 0.077 MB, Params: 4,372,632 (16.680 MB), Total: 16.76 MB, FLOPs: 289,907,868\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 44.262%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 46.995%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Finished fine tuning.\n",
      "Iteration 78/1724 finished in 0m09s\n",
      "Total channels prunned so far: 78\n",
      "\n",
      "Iteration 79 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 398)]\n",
      "Input: 0.077 MB, Params: 4,368,189 (16.663 MB), Total: 16.74 MB, FLOPs: 289,827,960\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 78.689%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.060%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.607%\n",
      "Finished fine tuning.\n",
      "Iteration 79/1724 finished in 0m09s\n",
      "Total channels prunned so far: 79\n",
      "\n",
      "Iteration 80 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 185)]\n",
      "Input: 0.077 MB, Params: 4,363,746 (16.646 MB), Total: 16.72 MB, FLOPs: 289,748,052\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 74.863%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 48.634%\n",
      "Finished fine tuning.\n",
      "Iteration 80/1724 finished in 0m09s\n",
      "Total channels prunned so far: 80\n",
      "\n",
      "Iteration 81 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 360)]\n",
      "Input: 0.077 MB, Params: 4,357,156 (16.621 MB), Total: 16.70 MB, FLOPs: 289,629,450\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 56.284%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 52.459%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 78.689%\n",
      "Finished fine tuning.\n",
      "Iteration 81/1724 finished in 0m09s\n",
      "Total channels prunned so far: 81\n",
      "\n",
      "Iteration 82 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 8)]\n",
      "Input: 0.077 MB, Params: 4,352,722 (16.604 MB), Total: 16.68 MB, FLOPs: 289,549,704\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 48.087%\n",
      "Finished fine tuning.\n",
      "Iteration 82/1724 finished in 0m09s\n",
      "Total channels prunned so far: 82\n",
      "\n",
      "Iteration 83 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 97)]\n",
      "Input: 0.077 MB, Params: 4,348,288 (16.587 MB), Total: 16.66 MB, FLOPs: 289,469,958\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 81.967%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 83/1724 finished in 0m09s\n",
      "Total channels prunned so far: 83\n",
      "\n",
      "Iteration 84 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 254)]\n",
      "Input: 0.077 MB, Params: 4,341,716 (16.562 MB), Total: 16.64 MB, FLOPs: 289,351,680\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 57.377%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 57.923%\n",
      "Finished fine tuning.\n",
      "Iteration 84/1724 finished in 0m09s\n",
      "Total channels prunned so far: 84\n",
      "\n",
      "Iteration 85 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 419)]\n",
      "Input: 0.077 MB, Params: 4,335,144 (16.537 MB), Total: 16.61 MB, FLOPs: 289,233,402\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 48.634%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 81.967%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Finished fine tuning.\n",
      "Iteration 85/1724 finished in 0m09s\n",
      "Total channels prunned so far: 85\n",
      "\n",
      "Iteration 86 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 278)]\n",
      "Input: 0.077 MB, Params: 4,328,572 (16.512 MB), Total: 16.59 MB, FLOPs: 289,115,124\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 86/1724 finished in 0m09s\n",
      "Total channels prunned so far: 86\n",
      "\n",
      "Iteration 87 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 227)]\n",
      "Input: 0.077 MB, Params: 4,321,919 (16.487 MB), Total: 16.56 MB, FLOPs: 288,846,750\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 79.781%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.328%\n",
      "Finished fine tuning.\n",
      "Iteration 87/1724 finished in 0m09s\n",
      "Total channels prunned so far: 87\n",
      "\n",
      "Iteration 88 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 114)]\n",
      "Input: 0.077 MB, Params: 4,318,542 (16.474 MB), Total: 16.55 MB, FLOPs: 288,563,166\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 48.087%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 53.005%\n",
      "Finished fine tuning.\n",
      "Iteration 88/1724 finished in 0m09s\n",
      "Total channels prunned so far: 88\n",
      "\n",
      "Iteration 89 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 478)]\n",
      "Input: 0.077 MB, Params: 4,314,135 (16.457 MB), Total: 16.53 MB, FLOPs: 288,483,906\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 66.667%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Finished fine tuning.\n",
      "Iteration 89/1724 finished in 0m09s\n",
      "Total channels prunned so far: 89\n",
      "\n",
      "Iteration 90 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 116)]\n",
      "Input: 0.077 MB, Params: 4,309,728 (16.440 MB), Total: 16.52 MB, FLOPs: 288,404,646\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 45.355%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 65.027%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 45.355%\n",
      "Finished fine tuning.\n",
      "Iteration 90/1724 finished in 0m09s\n",
      "Total channels prunned so far: 90\n",
      "\n",
      "Iteration 91 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 78)]\n",
      "Input: 0.077 MB, Params: 4,303,183 (16.415 MB), Total: 16.49 MB, FLOPs: 288,286,854\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 91/1724 finished in 0m09s\n",
      "Total channels prunned so far: 91\n",
      "\n",
      "Iteration 92 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 41)]\n",
      "Input: 0.077 MB, Params: 4,298,785 (16.399 MB), Total: 16.48 MB, FLOPs: 288,207,756\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 75.410%\n",
      "Finished fine tuning.\n",
      "Iteration 92/1724 finished in 0m09s\n",
      "Total channels prunned so far: 92\n",
      "\n",
      "Iteration 93 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 255)]\n",
      "Input: 0.077 MB, Params: 4,292,249 (16.374 MB), Total: 16.45 MB, FLOPs: 288,090,126\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 57.377%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 53.552%\n",
      "Finished fine tuning.\n",
      "Iteration 93/1724 finished in 0m09s\n",
      "Total channels prunned so far: 93\n",
      "\n",
      "Iteration 94 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 307)]\n",
      "Input: 0.077 MB, Params: 4,285,713 (16.349 MB), Total: 16.43 MB, FLOPs: 287,972,496\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 81.421%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 94/1724 finished in 0m09s\n",
      "Total channels prunned so far: 94\n",
      "\n",
      "Iteration 95 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 25)]\n",
      "Input: 0.077 MB, Params: 4,279,096 (16.323 MB), Total: 16.40 MB, FLOPs: 287,705,364\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 68.306%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 42.076%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 47.541%\n",
      "Finished fine tuning.\n",
      "Iteration 95/1724 finished in 0m09s\n",
      "Total channels prunned so far: 95\n",
      "\n",
      "Iteration 96 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 305)]\n",
      "Input: 0.077 MB, Params: 4,274,716 (16.307 MB), Total: 16.38 MB, FLOPs: 287,626,590\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 46.448%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 69.945%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 53.552%\n",
      "Finished fine tuning.\n",
      "Iteration 96/1724 finished in 0m09s\n",
      "Total channels prunned so far: 96\n",
      "\n",
      "Iteration 97 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 210)]\n",
      "Input: 0.077 MB, Params: 4,268,099 (16.282 MB), Total: 16.36 MB, FLOPs: 287,359,458\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Finished fine tuning.\n",
      "Iteration 97/1724 finished in 0m09s\n",
      "Total channels prunned so far: 97\n",
      "\n",
      "Iteration 98 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 262)]\n",
      "Input: 0.077 MB, Params: 4,261,590 (16.257 MB), Total: 16.33 MB, FLOPs: 287,242,314\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 78.142%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 78.689%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 61.202%\n",
      "Finished fine tuning.\n",
      "Iteration 98/1724 finished in 0m09s\n",
      "Total channels prunned so far: 98\n",
      "\n",
      "Iteration 99 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 87)]\n",
      "Input: 0.077 MB, Params: 4,254,982 (16.231 MB), Total: 16.31 MB, FLOPs: 286,975,344\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 54.645%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 66.667%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 99/1724 finished in 0m09s\n",
      "Total channels prunned so far: 99\n",
      "\n",
      "Iteration 100 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(7, 17)]\n",
      "Input: 0.077 MB, Params: 4,254,404 (16.229 MB), Total: 16.31 MB, FLOPs: 286,118,944\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 100/1724 finished in 0m09s\n",
      "Total channels prunned so far: 100\n",
      "\n",
      "Iteration 101 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 144)]\n",
      "Input: 0.077 MB, Params: 4,250,033 (16.213 MB), Total: 16.29 MB, FLOPs: 286,040,332\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 70.492%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 77.049%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 45.355%\n",
      "Finished fine tuning.\n",
      "Iteration 101/1724 finished in 0m09s\n",
      "Total channels prunned so far: 101\n",
      "\n",
      "Iteration 102 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(21, 105)]\n",
      "Input: 0.077 MB, Params: 4,246,665 (16.200 MB), Total: 16.28 MB, FLOPs: 285,457,652\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 61.749%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Finished fine tuning.\n",
      "Iteration 102/1724 finished in 0m09s\n",
      "Total channels prunned so far: 102\n",
      "\n",
      "Iteration 103 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 32)]\n",
      "Input: 0.077 MB, Params: 4,242,294 (16.183 MB), Total: 16.26 MB, FLOPs: 285,379,040\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 50.820%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 48.087%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 64.481%\n",
      "Finished fine tuning.\n",
      "Iteration 103/1724 finished in 0m09s\n",
      "Total channels prunned so far: 103\n",
      "\n",
      "Iteration 104 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 176)]\n",
      "Input: 0.077 MB, Params: 4,235,812 (16.158 MB), Total: 16.24 MB, FLOPs: 285,262,382\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 78.142%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 69.399%\n",
      "Finished fine tuning.\n",
      "Iteration 104/1724 finished in 0m09s\n",
      "Total channels prunned so far: 104\n",
      "\n",
      "Iteration 105 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 341)]\n",
      "Input: 0.077 MB, Params: 4,229,330 (16.134 MB), Total: 16.21 MB, FLOPs: 285,145,724\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 73.224%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Finished fine tuning.\n",
      "Iteration 105/1724 finished in 0m09s\n",
      "Total channels prunned so far: 105\n",
      "\n",
      "Iteration 106 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 438)]\n",
      "Input: 0.077 MB, Params: 4,224,977 (16.117 MB), Total: 16.19 MB, FLOPs: 285,067,436\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 75.956%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 54.098%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Finished fine tuning.\n",
      "Iteration 106/1724 finished in 0m09s\n",
      "Total channels prunned so far: 106\n",
      "\n",
      "Iteration 107 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 195)]\n",
      "Input: 0.077 MB, Params: 4,221,636 (16.104 MB), Total: 16.18 MB, FLOPs: 284,786,876\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 77.049%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 61.202%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 53.552%\n",
      "Finished fine tuning.\n",
      "Iteration 107/1724 finished in 0m09s\n",
      "Total channels prunned so far: 107\n",
      "\n",
      "Iteration 108 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 388)]\n",
      "Input: 0.077 MB, Params: 4,217,283 (16.088 MB), Total: 16.16 MB, FLOPs: 284,708,588\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.328%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 77.049%\n",
      "Finished fine tuning.\n",
      "Iteration 108/1724 finished in 0m09s\n",
      "Total channels prunned so far: 108\n",
      "\n",
      "Iteration 109 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 420)]\n",
      "Input: 0.077 MB, Params: 4,210,819 (16.063 MB), Total: 16.14 MB, FLOPs: 284,592,254\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.896%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Finished fine tuning.\n",
      "Iteration 109/1724 finished in 0m09s\n",
      "Total channels prunned so far: 109\n",
      "\n",
      "Iteration 110 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 143)]\n",
      "Input: 0.077 MB, Params: 4,207,478 (16.050 MB), Total: 16.13 MB, FLOPs: 284,311,694\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 76.503%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 48.087%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 65.027%\n",
      "Finished fine tuning.\n",
      "Iteration 110/1724 finished in 0m09s\n",
      "Total channels prunned so far: 110\n",
      "\n",
      "Iteration 111 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 152)]\n",
      "Input: 0.077 MB, Params: 4,201,014 (16.026 MB), Total: 16.10 MB, FLOPs: 284,195,360\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 72.678%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 62.842%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Finished fine tuning.\n",
      "Iteration 111/1724 finished in 0m09s\n",
      "Total channels prunned so far: 111\n",
      "\n",
      "Iteration 112 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 174)]\n",
      "Input: 0.077 MB, Params: 4,194,550 (16.001 MB), Total: 16.08 MB, FLOPs: 284,079,026\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Finished fine tuning.\n",
      "Iteration 112/1724 finished in 0m09s\n",
      "Total channels prunned so far: 112\n",
      "\n",
      "Iteration 113 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 21)]\n",
      "Input: 0.077 MB, Params: 4,188,005 (15.976 MB), Total: 16.05 MB, FLOPs: 283,814,378\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 45.902%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 47.541%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Finished fine tuning.\n",
      "Iteration 113/1724 finished in 0m09s\n",
      "Total channels prunned so far: 113\n",
      "\n",
      "Iteration 114 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 261)]\n",
      "Input: 0.077 MB, Params: 4,183,679 (15.959 MB), Total: 16.04 MB, FLOPs: 283,736,576\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 55.191%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 45.902%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 74.317%\n",
      "Finished fine tuning.\n",
      "Iteration 114/1724 finished in 0m09s\n",
      "Total channels prunned so far: 114\n",
      "\n",
      "Iteration 115 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 401)]\n",
      "Input: 0.077 MB, Params: 4,179,353 (15.943 MB), Total: 16.02 MB, FLOPs: 283,658,774\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 60.656%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 62.295%\n",
      "Finished fine tuning.\n",
      "Iteration 115/1724 finished in 0m09s\n",
      "Total channels prunned so far: 115\n",
      "\n",
      "Iteration 116 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 65)]\n",
      "Input: 0.077 MB, Params: 4,176,021 (15.930 MB), Total: 16.01 MB, FLOPs: 283,378,970\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 116/1724 finished in 0m09s\n",
      "Total channels prunned so far: 116\n",
      "\n",
      "Iteration 117 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 125)]\n",
      "Input: 0.077 MB, Params: 4,169,584 (15.906 MB), Total: 15.98 MB, FLOPs: 283,263,122\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 66.120%\n",
      "Finished fine tuning.\n",
      "Iteration 117/1724 finished in 0m09s\n",
      "Total channels prunned so far: 117\n",
      "\n",
      "Iteration 118 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 182)]\n",
      "Input: 0.077 MB, Params: 4,166,252 (15.893 MB), Total: 15.97 MB, FLOPs: 282,983,318\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Finished fine tuning.\n",
      "Iteration 118/1724 finished in 0m09s\n",
      "Total channels prunned so far: 118\n",
      "\n",
      "Iteration 119 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 306)]\n",
      "Input: 0.077 MB, Params: 4,161,935 (15.877 MB), Total: 15.95 MB, FLOPs: 282,905,678\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 119/1724 finished in 0m09s\n",
      "Total channels prunned so far: 119\n",
      "\n",
      "Iteration 120 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(21, 94)]\n",
      "Input: 0.077 MB, Params: 4,158,603 (15.864 MB), Total: 15.94 MB, FLOPs: 282,326,022\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 72.131%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 65.574%\n",
      "Finished fine tuning.\n",
      "Iteration 120/1724 finished in 0m09s\n",
      "Total channels prunned so far: 120\n",
      "\n",
      "Iteration 121 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 147)]\n",
      "Input: 0.077 MB, Params: 4,154,286 (15.847 MB), Total: 15.92 MB, FLOPs: 282,248,382\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 48.634%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 53.005%\n",
      "Finished fine tuning.\n",
      "Iteration 121/1724 finished in 0m09s\n",
      "Total channels prunned so far: 121\n",
      "\n",
      "Iteration 122 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 109)]\n",
      "Input: 0.077 MB, Params: 4,147,768 (15.822 MB), Total: 15.90 MB, FLOPs: 281,985,408\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.060%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 122/1724 finished in 0m09s\n",
      "Total channels prunned so far: 122\n",
      "\n",
      "Iteration 123 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 278)]\n",
      "Input: 0.077 MB, Params: 4,141,358 (15.798 MB), Total: 15.87 MB, FLOPs: 281,870,046\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Finished fine tuning.\n",
      "Iteration 123/1724 finished in 0m09s\n",
      "Total channels prunned so far: 123\n",
      "\n",
      "Iteration 124 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 7)]\n",
      "Input: 0.077 MB, Params: 4,138,044 (15.785 MB), Total: 15.86 MB, FLOPs: 281,591,754\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Finished fine tuning.\n",
      "Iteration 124/1724 finished in 0m09s\n",
      "Total channels prunned so far: 124\n",
      "\n",
      "Iteration 125 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 161)]\n",
      "Input: 0.077 MB, Params: 4,133,736 (15.769 MB), Total: 15.85 MB, FLOPs: 281,514,276\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Finished fine tuning.\n",
      "Iteration 125/1724 finished in 0m09s\n",
      "Total channels prunned so far: 125\n",
      "\n",
      "Iteration 126 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(21, 26)]\n",
      "Input: 0.077 MB, Params: 4,130,413 (15.756 MB), Total: 15.83 MB, FLOPs: 280,935,376\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Finished fine tuning.\n",
      "Iteration 126/1724 finished in 0m09s\n",
      "Total channels prunned so far: 126\n",
      "\n",
      "Iteration 127 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 161)]\n",
      "Input: 0.077 MB, Params: 4,126,105 (15.740 MB), Total: 15.82 MB, FLOPs: 280,857,898\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Finished fine tuning.\n",
      "Iteration 127/1724 finished in 0m09s\n",
      "Total channels prunned so far: 127\n",
      "\n",
      "Iteration 128 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 289)]\n",
      "Input: 0.077 MB, Params: 4,121,797 (15.723 MB), Total: 15.80 MB, FLOPs: 280,780,420\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Finished fine tuning.\n",
      "Iteration 128/1724 finished in 0m09s\n",
      "Total channels prunned so far: 128\n",
      "\n",
      "Iteration 129 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 147)]\n",
      "Input: 0.077 MB, Params: 4,117,489 (15.707 MB), Total: 15.78 MB, FLOPs: 280,702,942\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Finished fine tuning.\n",
      "Iteration 129/1724 finished in 0m09s\n",
      "Total channels prunned so far: 129\n",
      "\n",
      "Iteration 130 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 69)]\n",
      "Input: 0.077 MB, Params: 4,110,989 (15.682 MB), Total: 15.76 MB, FLOPs: 280,440,886\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Finished fine tuning.\n",
      "Iteration 130/1724 finished in 0m09s\n",
      "Total channels prunned so far: 130\n",
      "\n",
      "Iteration 131 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 412)]\n",
      "Input: 0.077 MB, Params: 4,104,624 (15.658 MB), Total: 15.73 MB, FLOPs: 280,326,334\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Finished fine tuning.\n",
      "Iteration 131/1724 finished in 0m09s\n",
      "Total channels prunned so far: 131\n",
      "\n",
      "Iteration 132 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 206)]\n",
      "Input: 0.077 MB, Params: 4,100,325 (15.641 MB), Total: 15.72 MB, FLOPs: 280,249,018\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Finished fine tuning.\n",
      "Iteration 132/1724 finished in 0m09s\n",
      "Total channels prunned so far: 132\n",
      "\n",
      "Iteration 133 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 149)]\n",
      "Input: 0.077 MB, Params: 4,096,026 (15.625 MB), Total: 15.70 MB, FLOPs: 280,171,702\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Finished fine tuning.\n",
      "Iteration 133/1724 finished in 0m09s\n",
      "Total channels prunned so far: 133\n",
      "\n",
      "Iteration 134 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 94)]\n",
      "Input: 0.077 MB, Params: 4,089,679 (15.601 MB), Total: 15.68 MB, FLOPs: 280,057,474\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Finished fine tuning.\n",
      "Iteration 134/1724 finished in 0m09s\n",
      "Total channels prunned so far: 134\n",
      "\n",
      "Iteration 135 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 445)]\n",
      "Input: 0.077 MB, Params: 4,083,332 (15.577 MB), Total: 15.65 MB, FLOPs: 279,943,246\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Finished fine tuning.\n",
      "Iteration 135/1724 finished in 0m09s\n",
      "Total channels prunned so far: 135\n",
      "\n",
      "Iteration 136 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 344)]\n",
      "Input: 0.077 MB, Params: 4,076,985 (15.552 MB), Total: 15.63 MB, FLOPs: 279,829,018\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 136/1724 finished in 0m09s\n",
      "Total channels prunned so far: 136\n",
      "\n",
      "Iteration 137 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 84)]\n",
      "Input: 0.077 MB, Params: 4,075,327 (15.546 MB), Total: 15.62 MB, FLOPs: 279,249,068\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Finished fine tuning.\n",
      "Iteration 137/1724 finished in 0m09s\n",
      "Total channels prunned so far: 137\n",
      "\n",
      "Iteration 138 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 24)]\n",
      "Input: 0.077 MB, Params: 4,073,669 (15.540 MB), Total: 15.62 MB, FLOPs: 278,669,118\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 138/1724 finished in 0m09s\n",
      "Total channels prunned so far: 138\n",
      "\n",
      "Iteration 139 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(21, 94)]\n",
      "Input: 0.077 MB, Params: 4,070,364 (15.527 MB), Total: 15.60 MB, FLOPs: 278,096,518\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 139/1724 finished in 0m09s\n",
      "Total channels prunned so far: 139\n",
      "\n",
      "Iteration 140 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 122)]\n",
      "Input: 0.077 MB, Params: 4,068,715 (15.521 MB), Total: 15.60 MB, FLOPs: 277,519,718\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Finished fine tuning.\n",
      "Iteration 140/1724 finished in 0m09s\n",
      "Total channels prunned so far: 140\n",
      "\n",
      "Iteration 141 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 327)]\n",
      "Input: 0.077 MB, Params: 4,062,368 (15.497 MB), Total: 15.57 MB, FLOPs: 277,405,490\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.607%\n",
      "Finished fine tuning.\n",
      "Iteration 141/1724 finished in 0m09s\n",
      "Total channels prunned so far: 141\n",
      "\n",
      "Iteration 142 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 420)]\n",
      "Input: 0.077 MB, Params: 4,056,021 (15.472 MB), Total: 15.55 MB, FLOPs: 277,291,262\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 142/1724 finished in 0m09s\n",
      "Total channels prunned so far: 142\n",
      "\n",
      "Iteration 143 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 35)]\n",
      "Input: 0.077 MB, Params: 4,049,674 (15.448 MB), Total: 15.53 MB, FLOPs: 277,177,034\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Finished fine tuning.\n",
      "Iteration 143/1724 finished in 0m09s\n",
      "Total channels prunned so far: 143\n",
      "\n",
      "Iteration 144 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 0)]\n",
      "Input: 0.077 MB, Params: 4,043,327 (15.424 MB), Total: 15.50 MB, FLOPs: 277,062,806\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Finished fine tuning.\n",
      "Iteration 144/1724 finished in 0m09s\n",
      "Total channels prunned so far: 144\n",
      "\n",
      "Iteration 145 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 55)]\n",
      "Input: 0.077 MB, Params: 4,036,980 (15.400 MB), Total: 15.48 MB, FLOPs: 276,948,578\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 145/1724 finished in 0m09s\n",
      "Total channels prunned so far: 145\n",
      "\n",
      "Iteration 146 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 364)]\n",
      "Input: 0.077 MB, Params: 4,032,753 (15.384 MB), Total: 15.46 MB, FLOPs: 276,872,558\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 81.421%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Finished fine tuning.\n",
      "Iteration 146/1724 finished in 0m09s\n",
      "Total channels prunned so far: 146\n",
      "\n",
      "Iteration 147 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 78)]\n",
      "Input: 0.077 MB, Params: 4,026,334 (15.359 MB), Total: 15.44 MB, FLOPs: 276,611,960\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 147/1724 finished in 0m09s\n",
      "Total channels prunned so far: 147\n",
      "\n",
      "Iteration 148 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 177)]\n",
      "Input: 0.077 MB, Params: 4,022,107 (15.343 MB), Total: 15.42 MB, FLOPs: 276,535,940\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 148/1724 finished in 0m09s\n",
      "Total channels prunned so far: 148\n",
      "\n",
      "Iteration 149 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 80)]\n",
      "Input: 0.077 MB, Params: 4,015,688 (15.319 MB), Total: 15.40 MB, FLOPs: 276,275,342\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 149/1724 finished in 0m09s\n",
      "Total channels prunned so far: 149\n",
      "\n",
      "Iteration 150 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 308)]\n",
      "Input: 0.077 MB, Params: 4,011,461 (15.303 MB), Total: 15.38 MB, FLOPs: 276,199,322\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.896%\n",
      "Finished fine tuning.\n",
      "Iteration 150/1724 finished in 0m09s\n",
      "Total channels prunned so far: 150\n",
      "\n",
      "Iteration 151 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 406)]\n",
      "Input: 0.077 MB, Params: 4,005,159 (15.278 MB), Total: 15.36 MB, FLOPs: 276,085,904\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.896%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Finished fine tuning.\n",
      "Iteration 151/1724 finished in 0m09s\n",
      "Total channels prunned so far: 151\n",
      "\n",
      "Iteration 152 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 16)]\n",
      "Input: 0.077 MB, Params: 3,998,749 (15.254 MB), Total: 15.33 MB, FLOPs: 275,825,468\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Finished fine tuning.\n",
      "Iteration 152/1724 finished in 0m09s\n",
      "Total channels prunned so far: 152\n",
      "\n",
      "Iteration 153 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 152)]\n",
      "Input: 0.077 MB, Params: 3,992,339 (15.230 MB), Total: 15.31 MB, FLOPs: 275,565,032\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 153/1724 finished in 0m09s\n",
      "Total channels prunned so far: 153\n",
      "\n",
      "Iteration 154 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 56)]\n",
      "Input: 0.077 MB, Params: 3,992,297 (15.229 MB), Total: 15.31 MB, FLOPs: 275,325,638\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Finished fine tuning.\n",
      "Iteration 154/1724 finished in 0m09s\n",
      "Total channels prunned so far: 154\n",
      "\n",
      "Iteration 155 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 457)]\n",
      "Input: 0.077 MB, Params: 3,988,079 (15.213 MB), Total: 15.29 MB, FLOPs: 275,249,780\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Finished fine tuning.\n",
      "Iteration 155/1724 finished in 0m09s\n",
      "Total channels prunned so far: 155\n",
      "\n",
      "Iteration 156 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 129)]\n",
      "Input: 0.077 MB, Params: 3,983,861 (15.197 MB), Total: 15.27 MB, FLOPs: 275,173,922\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 156/1724 finished in 0m09s\n",
      "Total channels prunned so far: 156\n",
      "\n",
      "Iteration 157 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 235)]\n",
      "Input: 0.077 MB, Params: 3,979,643 (15.181 MB), Total: 15.26 MB, FLOPs: 275,098,064\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 157/1724 finished in 0m09s\n",
      "Total channels prunned so far: 157\n",
      "\n",
      "Iteration 158 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 351)]\n",
      "Input: 0.077 MB, Params: 3,973,386 (15.157 MB), Total: 15.23 MB, FLOPs: 274,985,456\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 158/1724 finished in 0m09s\n",
      "Total channels prunned so far: 158\n",
      "\n",
      "Iteration 159 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 347)]\n",
      "Input: 0.077 MB, Params: 3,969,177 (15.141 MB), Total: 15.22 MB, FLOPs: 274,909,760\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.607%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 82.514%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Finished fine tuning.\n",
      "Iteration 159/1724 finished in 0m09s\n",
      "Total channels prunned so far: 159\n",
      "\n",
      "Iteration 160 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 16)]\n",
      "Input: 0.077 MB, Params: 3,967,528 (15.135 MB), Total: 15.21 MB, FLOPs: 274,332,960\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Finished fine tuning.\n",
      "Iteration 160/1724 finished in 0m09s\n",
      "Total channels prunned so far: 160\n",
      "\n",
      "Iteration 161 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 11)]\n",
      "Input: 0.077 MB, Params: 3,963,319 (15.119 MB), Total: 15.20 MB, FLOPs: 274,257,264\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 161/1724 finished in 0m09s\n",
      "Total channels prunned so far: 161\n",
      "\n",
      "Iteration 162 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 49)]\n",
      "Input: 0.077 MB, Params: 3,963,277 (15.119 MB), Total: 15.20 MB, FLOPs: 256,963,846\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 162/1724 finished in 0m09s\n",
      "Total channels prunned so far: 162\n",
      "\n",
      "Iteration 163 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 44)]\n",
      "Input: 0.077 MB, Params: 3,959,068 (15.103 MB), Total: 15.18 MB, FLOPs: 256,888,150\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 163/1724 finished in 0m09s\n",
      "Total channels prunned so far: 163\n",
      "\n",
      "Iteration 164 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 85)]\n",
      "Input: 0.077 MB, Params: 3,952,838 (15.079 MB), Total: 15.16 MB, FLOPs: 256,776,028\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 164/1724 finished in 0m09s\n",
      "Total channels prunned so far: 164\n",
      "\n",
      "Iteration 165 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 141)]\n",
      "Input: 0.077 MB, Params: 3,946,608 (15.055 MB), Total: 15.13 MB, FLOPs: 256,663,906\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.060%\n",
      "Finished fine tuning.\n",
      "Iteration 165/1724 finished in 0m09s\n",
      "Total channels prunned so far: 165\n",
      "\n",
      "Iteration 166 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 44)]\n",
      "Input: 0.077 MB, Params: 3,940,378 (15.031 MB), Total: 15.11 MB, FLOPs: 256,551,784\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 79.781%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 76.503%\n",
      "Finished fine tuning.\n",
      "Iteration 166/1724 finished in 0m09s\n",
      "Total channels prunned so far: 166\n",
      "\n",
      "Iteration 167 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(14, 32)]\n",
      "Input: 0.077 MB, Params: 3,938,720 (15.025 MB), Total: 15.10 MB, FLOPs: 255,429,759\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.328%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 167/1724 finished in 0m09s\n",
      "Total channels prunned so far: 167\n",
      "\n",
      "Iteration 168 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 351)]\n",
      "Input: 0.077 MB, Params: 3,934,538 (15.009 MB), Total: 15.09 MB, FLOPs: 255,354,549\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Finished fine tuning.\n",
      "Iteration 168/1724 finished in 0m09s\n",
      "Total channels prunned so far: 168\n",
      "\n",
      "Iteration 169 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 185)]\n",
      "Input: 0.077 MB, Params: 3,928,317 (14.985 MB), Total: 15.06 MB, FLOPs: 255,242,589\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 169/1724 finished in 0m09s\n",
      "Total channels prunned so far: 169\n",
      "\n",
      "Iteration 170 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 195)]\n",
      "Input: 0.077 MB, Params: 3,922,096 (14.962 MB), Total: 15.04 MB, FLOPs: 255,130,629\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Finished fine tuning.\n",
      "Iteration 170/1724 finished in 0m09s\n",
      "Total channels prunned so far: 170\n",
      "\n",
      "Iteration 171 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 42)]\n",
      "Input: 0.077 MB, Params: 3,915,875 (14.938 MB), Total: 15.01 MB, FLOPs: 255,018,669\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 171/1724 finished in 0m09s\n",
      "Total channels prunned so far: 171\n",
      "\n",
      "Iteration 172 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 96)]\n",
      "Input: 0.077 MB, Params: 3,911,720 (14.922 MB), Total: 15.00 MB, FLOPs: 254,943,945\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Finished fine tuning.\n",
      "Iteration 172/1724 finished in 0m09s\n",
      "Total channels prunned so far: 172\n",
      "\n",
      "Iteration 173 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 101)]\n",
      "Input: 0.077 MB, Params: 3,905,508 (14.898 MB), Total: 14.98 MB, FLOPs: 254,832,147\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Finished fine tuning.\n",
      "Iteration 173/1724 finished in 0m09s\n",
      "Total channels prunned so far: 173\n",
      "\n",
      "Iteration 174 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 284)]\n",
      "Input: 0.077 MB, Params: 3,899,296 (14.875 MB), Total: 14.95 MB, FLOPs: 254,720,349\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Finished fine tuning.\n",
      "Iteration 174/1724 finished in 0m09s\n",
      "Total channels prunned so far: 174\n",
      "\n",
      "Iteration 175 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 16)]\n",
      "Input: 0.077 MB, Params: 3,893,084 (14.851 MB), Total: 14.93 MB, FLOPs: 254,608,551\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 175/1724 finished in 0m09s\n",
      "Total channels prunned so far: 175\n",
      "\n",
      "Iteration 176 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(14, 28)]\n",
      "Input: 0.077 MB, Params: 3,891,426 (14.845 MB), Total: 14.92 MB, FLOPs: 253,486,526\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.607%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Finished fine tuning.\n",
      "Iteration 176/1724 finished in 0m09s\n",
      "Total channels prunned so far: 176\n",
      "\n",
      "Iteration 177 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 229)]\n",
      "Input: 0.077 MB, Params: 3,885,214 (14.821 MB), Total: 14.90 MB, FLOPs: 253,374,728\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 177/1724 finished in 0m09s\n",
      "Total channels prunned so far: 177\n",
      "\n",
      "Iteration 178 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 328)]\n",
      "Input: 0.077 MB, Params: 3,881,095 (14.805 MB), Total: 14.88 MB, FLOPs: 253,300,652\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Finished fine tuning.\n",
      "Iteration 178/1724 finished in 0m09s\n",
      "Total channels prunned so far: 178\n",
      "\n",
      "Iteration 179 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 152)]\n",
      "Input: 0.077 MB, Params: 3,876,976 (14.789 MB), Total: 14.87 MB, FLOPs: 253,226,576\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Finished fine tuning.\n",
      "Iteration 179/1724 finished in 0m09s\n",
      "Total channels prunned so far: 179\n",
      "\n",
      "Iteration 180 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 6)]\n",
      "Input: 0.077 MB, Params: 3,872,857 (14.774 MB), Total: 14.85 MB, FLOPs: 253,152,500\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 75.956%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Finished fine tuning.\n",
      "Iteration 180/1724 finished in 0m09s\n",
      "Total channels prunned so far: 180\n",
      "\n",
      "Iteration 181 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 413)]\n",
      "Input: 0.077 MB, Params: 3,866,672 (14.750 MB), Total: 14.83 MB, FLOPs: 253,041,188\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 181/1724 finished in 0m09s\n",
      "Total channels prunned so far: 181\n",
      "\n",
      "Iteration 182 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 45)]\n",
      "Input: 0.077 MB, Params: 3,865,041 (14.744 MB), Total: 14.82 MB, FLOPs: 252,511,438\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 182/1724 finished in 0m09s\n",
      "Total channels prunned so far: 182\n",
      "\n",
      "Iteration 183 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 231)]\n",
      "Input: 0.077 MB, Params: 3,861,790 (14.732 MB), Total: 14.81 MB, FLOPs: 252,277,438\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 183/1724 finished in 0m09s\n",
      "Total channels prunned so far: 183\n",
      "\n",
      "Iteration 184 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 109)]\n",
      "Input: 0.077 MB, Params: 3,857,680 (14.716 MB), Total: 14.79 MB, FLOPs: 252,203,524\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Finished fine tuning.\n",
      "Iteration 184/1724 finished in 0m09s\n",
      "Total channels prunned so far: 184\n",
      "\n",
      "Iteration 185 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 409)]\n",
      "Input: 0.077 MB, Params: 3,853,570 (14.700 MB), Total: 14.78 MB, FLOPs: 252,129,610\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 185/1724 finished in 0m09s\n",
      "Total channels prunned so far: 185\n",
      "\n",
      "Iteration 186 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 318)]\n",
      "Input: 0.077 MB, Params: 3,849,460 (14.685 MB), Total: 14.76 MB, FLOPs: 252,055,696\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 186/1724 finished in 0m09s\n",
      "Total channels prunned so far: 186\n",
      "\n",
      "Iteration 187 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 215)]\n",
      "Input: 0.077 MB, Params: 3,843,167 (14.661 MB), Total: 14.74 MB, FLOPs: 251,824,216\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Finished fine tuning.\n",
      "Iteration 187/1724 finished in 0m09s\n",
      "Total channels prunned so far: 187\n",
      "\n",
      "Iteration 188 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 352)]\n",
      "Input: 0.077 MB, Params: 3,837,018 (14.637 MB), Total: 14.71 MB, FLOPs: 251,713,552\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Finished fine tuning.\n",
      "Iteration 188/1724 finished in 0m09s\n",
      "Total channels prunned so far: 188\n",
      "\n",
      "Iteration 189 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 52)]\n",
      "Input: 0.077 MB, Params: 3,832,917 (14.621 MB), Total: 14.70 MB, FLOPs: 251,639,800\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Finished fine tuning.\n",
      "Iteration 189/1724 finished in 0m09s\n",
      "Total channels prunned so far: 189\n",
      "\n",
      "Iteration 190 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 189)]\n",
      "Input: 0.077 MB, Params: 3,826,777 (14.598 MB), Total: 14.67 MB, FLOPs: 251,529,298\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Finished fine tuning.\n",
      "Iteration 190/1724 finished in 0m09s\n",
      "Total channels prunned so far: 190\n",
      "\n",
      "Iteration 191 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 385)]\n",
      "Input: 0.077 MB, Params: 3,820,637 (14.575 MB), Total: 14.65 MB, FLOPs: 251,418,796\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.874%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 191/1724 finished in 0m09s\n",
      "Total channels prunned so far: 191\n",
      "\n",
      "Iteration 192 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 80)]\n",
      "Input: 0.077 MB, Params: 3,816,554 (14.559 MB), Total: 14.64 MB, FLOPs: 251,345,368\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 71.038%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 72.131%\n",
      "Finished fine tuning.\n",
      "Iteration 192/1724 finished in 0m09s\n",
      "Total channels prunned so far: 192\n",
      "\n",
      "Iteration 193 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 345)]\n",
      "Input: 0.077 MB, Params: 3,810,423 (14.536 MB), Total: 14.61 MB, FLOPs: 251,235,028\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.874%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 193/1724 finished in 0m09s\n",
      "Total channels prunned so far: 193\n",
      "\n",
      "Iteration 194 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 177)]\n",
      "Input: 0.077 MB, Params: 3,806,349 (14.520 MB), Total: 14.60 MB, FLOPs: 251,161,762\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 194/1724 finished in 0m09s\n",
      "Total channels prunned so far: 194\n",
      "\n",
      "Iteration 195 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 197)]\n",
      "Input: 0.077 MB, Params: 3,800,092 (14.496 MB), Total: 14.57 MB, FLOPs: 250,930,930\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Finished fine tuning.\n",
      "Iteration 195/1724 finished in 0m09s\n",
      "Total channels prunned so far: 195\n",
      "\n",
      "Iteration 196 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 25)]\n",
      "Input: 0.077 MB, Params: 3,793,979 (14.473 MB), Total: 14.55 MB, FLOPs: 250,820,914\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 196/1724 finished in 0m09s\n",
      "Total channels prunned so far: 196\n",
      "\n",
      "Iteration 197 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 312)]\n",
      "Input: 0.077 MB, Params: 3,789,914 (14.457 MB), Total: 14.53 MB, FLOPs: 250,747,810\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Finished fine tuning.\n",
      "Iteration 197/1724 finished in 0m09s\n",
      "Total channels prunned so far: 197\n",
      "\n",
      "Iteration 198 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 123)]\n",
      "Input: 0.077 MB, Params: 3,783,810 (14.434 MB), Total: 14.51 MB, FLOPs: 250,637,956\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 198/1724 finished in 0m09s\n",
      "Total channels prunned so far: 198\n",
      "\n",
      "Iteration 199 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 71)]\n",
      "Input: 0.077 MB, Params: 3,780,577 (14.422 MB), Total: 14.50 MB, FLOPs: 250,405,252\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 82.514%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 82.514%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 199/1724 finished in 0m09s\n",
      "Total channels prunned so far: 199\n",
      "\n",
      "Iteration 200 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 164)]\n",
      "Input: 0.077 MB, Params: 3,774,473 (14.398 MB), Total: 14.48 MB, FLOPs: 250,295,398\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Finished fine tuning.\n",
      "Iteration 200/1724 finished in 0m09s\n",
      "Total channels prunned so far: 200\n",
      "\n",
      "Iteration 201 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 11)]\n",
      "Input: 0.077 MB, Params: 3,768,252 (14.375 MB), Total: 14.45 MB, FLOPs: 250,065,700\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Finished fine tuning.\n",
      "Iteration 201/1724 finished in 0m09s\n",
      "Total channels prunned so far: 201\n",
      "\n",
      "Iteration 202 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 395)]\n",
      "Input: 0.077 MB, Params: 3,764,205 (14.359 MB), Total: 14.44 MB, FLOPs: 249,992,920\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Finished fine tuning.\n",
      "Iteration 202/1724 finished in 0m09s\n",
      "Total channels prunned so far: 202\n",
      "\n",
      "Iteration 203 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 447)]\n",
      "Input: 0.077 MB, Params: 3,758,119 (14.336 MB), Total: 14.41 MB, FLOPs: 249,883,390\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Finished fine tuning.\n",
      "Iteration 203/1724 finished in 0m09s\n",
      "Total channels prunned so far: 203\n",
      "\n",
      "Iteration 204 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 206)]\n",
      "Input: 0.077 MB, Params: 3,754,081 (14.321 MB), Total: 14.40 MB, FLOPs: 249,810,772\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.896%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.896%\n",
      "Finished fine tuning.\n",
      "Iteration 204/1724 finished in 0m09s\n",
      "Total channels prunned so far: 204\n",
      "\n",
      "Iteration 205 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 397)]\n",
      "Input: 0.077 MB, Params: 3,750,043 (14.305 MB), Total: 14.38 MB, FLOPs: 249,738,154\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.989%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 95.082%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 95.082%\n",
      "Finished fine tuning.\n",
      "Iteration 205/1724 finished in 0m09s\n",
      "Total channels prunned so far: 205\n",
      "\n",
      "Iteration 206 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 2)]\n",
      "Input: 0.077 MB, Params: 3,743,975 (14.282 MB), Total: 14.36 MB, FLOPs: 249,628,948\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.443%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.443%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.896%\n",
      "Finished fine tuning.\n",
      "Iteration 206/1724 finished in 0m09s\n",
      "Total channels prunned so far: 206\n",
      "\n",
      "Iteration 207 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 64)]\n",
      "Input: 0.077 MB, Params: 3,740,751 (14.270 MB), Total: 14.35 MB, FLOPs: 249,396,892\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.896%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Finished fine tuning.\n",
      "Iteration 207/1724 finished in 0m09s\n",
      "Total channels prunned so far: 207\n",
      "\n",
      "Iteration 208 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 284)]\n",
      "Input: 0.077 MB, Params: 3,734,683 (14.247 MB), Total: 14.32 MB, FLOPs: 249,287,686\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Finished fine tuning.\n",
      "Iteration 208/1724 finished in 0m09s\n",
      "Total channels prunned so far: 208\n",
      "\n",
      "Iteration 209 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 65)]\n",
      "Input: 0.077 MB, Params: 3,730,663 (14.231 MB), Total: 14.31 MB, FLOPs: 249,215,392\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Finished fine tuning.\n",
      "Iteration 209/1724 finished in 0m09s\n",
      "Total channels prunned so far: 209\n",
      "\n",
      "Iteration 210 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 132)]\n",
      "Input: 0.077 MB, Params: 3,724,604 (14.208 MB), Total: 14.29 MB, FLOPs: 249,106,348\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Finished fine tuning.\n",
      "Iteration 210/1724 finished in 0m09s\n",
      "Total channels prunned so far: 210\n",
      "\n",
      "Iteration 211 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 66)]\n",
      "Input: 0.077 MB, Params: 3,718,428 (14.185 MB), Total: 14.26 MB, FLOPs: 248,877,946\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.989%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 211/1724 finished in 0m09s\n",
      "Total channels prunned so far: 211\n",
      "\n",
      "Iteration 212 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(21, 84)]\n",
      "Input: 0.077 MB, Params: 3,715,177 (14.172 MB), Total: 14.25 MB, FLOPs: 248,370,165\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Finished fine tuning.\n",
      "Iteration 212/1724 finished in 0m09s\n",
      "Total channels prunned so far: 212\n",
      "\n",
      "Iteration 213 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 425)]\n",
      "Input: 0.077 MB, Params: 3,709,127 (14.149 MB), Total: 14.23 MB, FLOPs: 248,261,283\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Finished fine tuning.\n",
      "Iteration 213/1724 finished in 0m09s\n",
      "Total channels prunned so far: 213\n",
      "\n",
      "Iteration 214 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 355)]\n",
      "Input: 0.077 MB, Params: 3,705,125 (14.134 MB), Total: 14.21 MB, FLOPs: 248,189,313\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Finished fine tuning.\n",
      "Iteration 214/1724 finished in 0m09s\n",
      "Total channels prunned so far: 214\n",
      "\n",
      "Iteration 215 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 435)]\n",
      "Input: 0.077 MB, Params: 3,701,123 (14.119 MB), Total: 14.20 MB, FLOPs: 248,117,343\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 95.082%\n",
      "Finished fine tuning.\n",
      "Iteration 215/1724 finished in 0m09s\n",
      "Total channels prunned so far: 215\n",
      "\n",
      "Iteration 216 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 94)]\n",
      "Input: 0.077 MB, Params: 3,694,956 (14.095 MB), Total: 14.17 MB, FLOPs: 247,889,103\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 94.536%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 216/1724 finished in 0m09s\n",
      "Total channels prunned so far: 216\n",
      "\n",
      "Iteration 217 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(11, 21)]\n",
      "Input: 0.077 MB, Params: 3,694,171 (14.092 MB), Total: 14.17 MB, FLOPs: 246,830,703\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.896%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.443%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.989%\n",
      "Finished fine tuning.\n",
      "Iteration 217/1724 finished in 0m09s\n",
      "Total channels prunned so far: 217\n",
      "\n",
      "Iteration 218 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 52)]\n",
      "Input: 0.077 MB, Params: 3,688,148 (14.069 MB), Total: 14.15 MB, FLOPs: 246,722,307\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.443%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.896%\n",
      "Finished fine tuning.\n",
      "Iteration 218/1724 finished in 0m09s\n",
      "Total channels prunned so far: 218\n",
      "\n",
      "Iteration 219 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 139)]\n",
      "Input: 0.077 MB, Params: 3,682,125 (14.046 MB), Total: 14.12 MB, FLOPs: 246,613,911\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Finished fine tuning.\n",
      "Iteration 219/1724 finished in 0m09s\n",
      "Total channels prunned so far: 219\n",
      "\n",
      "Iteration 220 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(21, 81)]\n",
      "Input: 0.077 MB, Params: 3,678,874 (14.034 MB), Total: 14.11 MB, FLOPs: 246,106,130\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.896%\n",
      "Finished fine tuning.\n",
      "Iteration 220/1724 finished in 0m09s\n",
      "Total channels prunned so far: 220\n",
      "\n",
      "Iteration 221 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 94)]\n",
      "Input: 0.077 MB, Params: 3,672,725 (14.010 MB), Total: 14.09 MB, FLOPs: 245,878,214\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.896%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Finished fine tuning.\n",
      "Iteration 221/1724 finished in 0m09s\n",
      "Total channels prunned so far: 221\n",
      "\n",
      "Iteration 222 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 177)]\n",
      "Input: 0.077 MB, Params: 3,666,711 (13.987 MB), Total: 14.06 MB, FLOPs: 245,769,980\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.989%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.443%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Finished fine tuning.\n",
      "Iteration 222/1724 finished in 0m09s\n",
      "Total channels prunned so far: 222\n",
      "\n",
      "Iteration 223 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 95)]\n",
      "Input: 0.077 MB, Params: 3,663,532 (13.975 MB), Total: 14.05 MB, FLOPs: 245,541,164\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 223/1724 finished in 0m09s\n",
      "Total channels prunned so far: 223\n",
      "\n",
      "Iteration 224 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 11)]\n",
      "Input: 0.077 MB, Params: 3,663,490 (13.975 MB), Total: 14.05 MB, FLOPs: 245,301,770\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.607%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Finished fine tuning.\n",
      "Iteration 224/1724 finished in 0m09s\n",
      "Total channels prunned so far: 224\n",
      "\n",
      "Iteration 225 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 18)]\n",
      "Input: 0.077 MB, Params: 3,657,359 (13.952 MB), Total: 14.03 MB, FLOPs: 245,074,664\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 79.235%\n",
      "Finished fine tuning.\n",
      "Iteration 225/1724 finished in 0m09s\n",
      "Total channels prunned so far: 225\n",
      "\n",
      "Iteration 226 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 108)]\n",
      "Input: 0.077 MB, Params: 3,651,228 (13.928 MB), Total: 14.01 MB, FLOPs: 244,847,558\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 79.235%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 75.410%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Finished fine tuning.\n",
      "Iteration 226/1724 finished in 0m09s\n",
      "Total channels prunned so far: 226\n",
      "\n",
      "Iteration 227 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 78)]\n",
      "Input: 0.077 MB, Params: 3,645,232 (13.905 MB), Total: 13.98 MB, FLOPs: 244,739,648\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 81.421%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Finished fine tuning.\n",
      "Iteration 227/1724 finished in 0m09s\n",
      "Total channels prunned so far: 227\n",
      "\n",
      "Iteration 228 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 242)]\n",
      "Input: 0.077 MB, Params: 3,639,236 (13.883 MB), Total: 13.96 MB, FLOPs: 244,631,738\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 75.956%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 68.306%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 74.863%\n",
      "Finished fine tuning.\n",
      "Iteration 228/1724 finished in 0m09s\n",
      "Total channels prunned so far: 228\n",
      "\n",
      "Iteration 229 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 107)]\n",
      "Input: 0.077 MB, Params: 3,633,123 (13.859 MB), Total: 13.94 MB, FLOPs: 244,404,956\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 64.481%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 76.503%\n",
      "Finished fine tuning.\n",
      "Iteration 229/1724 finished in 0m09s\n",
      "Total channels prunned so far: 229\n",
      "\n",
      "Iteration 230 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(21, 115)]\n",
      "Input: 0.077 MB, Params: 3,629,881 (13.847 MB), Total: 13.92 MB, FLOPs: 243,897,823\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 79.781%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.328%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Finished fine tuning.\n",
      "Iteration 230/1724 finished in 0m09s\n",
      "Total channels prunned so far: 230\n",
      "\n",
      "Iteration 231 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 222)]\n",
      "Input: 0.077 MB, Params: 3,623,768 (13.824 MB), Total: 13.90 MB, FLOPs: 243,671,041\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.874%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.328%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Finished fine tuning.\n",
      "Iteration 231/1724 finished in 0m09s\n",
      "Total channels prunned so far: 231\n",
      "\n",
      "Iteration 232 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 228)]\n",
      "Input: 0.077 MB, Params: 3,620,634 (13.812 MB), Total: 13.89 MB, FLOPs: 243,445,465\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 232/1724 finished in 0m09s\n",
      "Total channels prunned so far: 232\n",
      "\n",
      "Iteration 233 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 32)]\n",
      "Input: 0.077 MB, Params: 3,619,030 (13.806 MB), Total: 13.88 MB, FLOPs: 242,924,490\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 82.514%\n",
      "Finished fine tuning.\n",
      "Iteration 233/1724 finished in 0m09s\n",
      "Total channels prunned so far: 233\n",
      "\n",
      "Iteration 234 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 216)]\n",
      "Input: 0.077 MB, Params: 3,613,052 (13.783 MB), Total: 13.86 MB, FLOPs: 242,816,904\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 77.596%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Finished fine tuning.\n",
      "Iteration 234/1724 finished in 0m09s\n",
      "Total channels prunned so far: 234\n",
      "\n",
      "Iteration 235 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 184)]\n",
      "Input: 0.077 MB, Params: 3,607,074 (13.760 MB), Total: 13.84 MB, FLOPs: 242,709,318\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 235/1724 finished in 0m09s\n",
      "Total channels prunned so far: 235\n",
      "\n",
      "Iteration 236 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 104)]\n",
      "Input: 0.077 MB, Params: 3,601,096 (13.737 MB), Total: 13.81 MB, FLOPs: 242,601,732\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 78.689%\n",
      "Finished fine tuning.\n",
      "Iteration 236/1724 finished in 0m09s\n",
      "Total channels prunned so far: 236\n",
      "\n",
      "Iteration 237 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 71)]\n",
      "Input: 0.077 MB, Params: 3,595,118 (13.714 MB), Total: 13.79 MB, FLOPs: 242,494,146\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 81.421%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Finished fine tuning.\n",
      "Iteration 237/1724 finished in 0m09s\n",
      "Total channels prunned so far: 237\n",
      "\n",
      "Iteration 238 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 246)]\n",
      "Input: 0.077 MB, Params: 3,591,197 (13.699 MB), Total: 13.78 MB, FLOPs: 242,423,634\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.607%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.060%\n",
      "Finished fine tuning.\n",
      "Iteration 238/1724 finished in 0m09s\n",
      "Total channels prunned so far: 238\n",
      "\n",
      "Iteration 239 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 6)]\n",
      "Input: 0.077 MB, Params: 3,585,129 (13.676 MB), Total: 13.75 MB, FLOPs: 242,198,148\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.607%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.874%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 77.049%\n",
      "Finished fine tuning.\n",
      "Iteration 239/1724 finished in 0m09s\n",
      "Total channels prunned so far: 239\n",
      "\n",
      "Iteration 240 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 160)]\n",
      "Input: 0.077 MB, Params: 3,579,169 (13.653 MB), Total: 13.73 MB, FLOPs: 242,090,886\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 81.967%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.060%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 82.514%\n",
      "Finished fine tuning.\n",
      "Iteration 240/1724 finished in 0m09s\n",
      "Total channels prunned so far: 240\n",
      "\n",
      "Iteration 241 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 349)]\n",
      "Input: 0.077 MB, Params: 3,573,209 (13.631 MB), Total: 13.71 MB, FLOPs: 241,983,624\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 81.967%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Finished fine tuning.\n",
      "Iteration 241/1724 finished in 0m09s\n",
      "Total channels prunned so far: 241\n",
      "\n",
      "Iteration 242 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 86)]\n",
      "Input: 0.077 MB, Params: 3,567,159 (13.608 MB), Total: 13.68 MB, FLOPs: 241,758,462\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Finished fine tuning.\n",
      "Iteration 242/1724 finished in 0m09s\n",
      "Total channels prunned so far: 242\n",
      "\n",
      "Iteration 243 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 39)]\n",
      "Input: 0.077 MB, Params: 3,561,109 (13.585 MB), Total: 13.66 MB, FLOPs: 241,533,300\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Finished fine tuning.\n",
      "Iteration 243/1724 finished in 0m09s\n",
      "Total channels prunned so far: 243\n",
      "\n",
      "Iteration 244 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 355)]\n",
      "Input: 0.077 MB, Params: 3,557,206 (13.570 MB), Total: 13.65 MB, FLOPs: 241,463,112\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 79.781%\n",
      "Finished fine tuning.\n",
      "Iteration 244/1724 finished in 0m09s\n",
      "Total channels prunned so far: 244\n",
      "\n",
      "Iteration 245 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 308)]\n",
      "Input: 0.077 MB, Params: 3,553,303 (13.555 MB), Total: 13.63 MB, FLOPs: 241,392,924\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 81.421%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 71.585%\n",
      "Finished fine tuning.\n",
      "Iteration 245/1724 finished in 0m09s\n",
      "Total channels prunned so far: 245\n",
      "\n",
      "Iteration 246 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 209)]\n",
      "Input: 0.077 MB, Params: 3,550,196 (13.543 MB), Total: 13.62 MB, FLOPs: 241,169,292\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.328%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Finished fine tuning.\n",
      "Iteration 246/1724 finished in 0m09s\n",
      "Total channels prunned so far: 246\n",
      "\n",
      "Iteration 247 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 253)]\n",
      "Input: 0.077 MB, Params: 3,544,272 (13.520 MB), Total: 13.60 MB, FLOPs: 241,062,678\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Finished fine tuning.\n",
      "Iteration 247/1724 finished in 0m09s\n",
      "Total channels prunned so far: 247\n",
      "\n",
      "Iteration 248 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 47)]\n",
      "Input: 0.077 MB, Params: 3,541,165 (13.508 MB), Total: 13.59 MB, FLOPs: 240,839,046\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Finished fine tuning.\n",
      "Iteration 248/1724 finished in 0m09s\n",
      "Total channels prunned so far: 248\n",
      "\n",
      "Iteration 249 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 45)]\n",
      "Input: 0.077 MB, Params: 3,538,058 (13.497 MB), Total: 13.57 MB, FLOPs: 240,615,414\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Finished fine tuning.\n",
      "Iteration 249/1724 finished in 0m09s\n",
      "Total channels prunned so far: 249\n",
      "\n",
      "Iteration 250 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 37)]\n",
      "Input: 0.077 MB, Params: 3,532,134 (13.474 MB), Total: 13.55 MB, FLOPs: 240,508,800\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.607%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Finished fine tuning.\n",
      "Iteration 250/1724 finished in 0m09s\n",
      "Total channels prunned so far: 250\n",
      "\n",
      "Iteration 251 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 173)]\n",
      "Input: 0.077 MB, Params: 3,526,210 (13.451 MB), Total: 13.53 MB, FLOPs: 240,402,186\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 78.689%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.060%\n",
      "Finished fine tuning.\n",
      "Iteration 251/1724 finished in 0m09s\n",
      "Total channels prunned so far: 251\n",
      "\n",
      "Iteration 252 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 121)]\n",
      "Input: 0.077 MB, Params: 3,523,103 (13.440 MB), Total: 13.52 MB, FLOPs: 240,178,554\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.607%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 78.142%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 75.956%\n",
      "Finished fine tuning.\n",
      "Iteration 252/1724 finished in 0m09s\n",
      "Total channels prunned so far: 252\n",
      "\n",
      "Iteration 253 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 154)]\n",
      "Input: 0.077 MB, Params: 3,517,116 (13.417 MB), Total: 13.49 MB, FLOPs: 239,956,470\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Finished fine tuning.\n",
      "Iteration 253/1724 finished in 0m09s\n",
      "Total channels prunned so far: 253\n",
      "\n",
      "Iteration 254 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 313)]\n",
      "Input: 0.077 MB, Params: 3,511,201 (13.394 MB), Total: 13.47 MB, FLOPs: 239,850,018\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.060%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Finished fine tuning.\n",
      "Iteration 254/1724 finished in 0m09s\n",
      "Total channels prunned so far: 254\n",
      "\n",
      "Iteration 255 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 219)]\n",
      "Input: 0.077 MB, Params: 3,507,334 (13.379 MB), Total: 13.46 MB, FLOPs: 239,780,478\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 82.514%\n",
      "Finished fine tuning.\n",
      "Iteration 255/1724 finished in 0m09s\n",
      "Total channels prunned so far: 255\n",
      "\n",
      "Iteration 256 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 84)]\n",
      "Input: 0.077 MB, Params: 3,501,356 (13.357 MB), Total: 13.43 MB, FLOPs: 239,558,556\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.607%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 256/1724 finished in 0m09s\n",
      "Total channels prunned so far: 256\n",
      "\n",
      "Iteration 257 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 197)]\n",
      "Input: 0.077 MB, Params: 3,498,267 (13.345 MB), Total: 13.42 MB, FLOPs: 239,336,220\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 257/1724 finished in 0m09s\n",
      "Total channels prunned so far: 257\n",
      "\n",
      "Iteration 258 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 229)]\n",
      "Input: 0.077 MB, Params: 3,495,178 (13.333 MB), Total: 13.41 MB, FLOPs: 239,113,884\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Finished fine tuning.\n",
      "Iteration 258/1724 finished in 0m09s\n",
      "Total channels prunned so far: 258\n",
      "\n",
      "Iteration 259 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 405)]\n",
      "Input: 0.077 MB, Params: 3,491,311 (13.318 MB), Total: 13.40 MB, FLOPs: 239,044,344\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 75.956%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Finished fine tuning.\n",
      "Iteration 259/1724 finished in 0m09s\n",
      "Total channels prunned so far: 259\n",
      "\n",
      "Iteration 260 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 42)]\n",
      "Input: 0.077 MB, Params: 3,487,444 (13.304 MB), Total: 13.38 MB, FLOPs: 238,974,804\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.060%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 81.421%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Finished fine tuning.\n",
      "Iteration 260/1724 finished in 0m09s\n",
      "Total channels prunned so far: 260\n",
      "\n",
      "Iteration 261 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 25)]\n",
      "Input: 0.077 MB, Params: 3,483,577 (13.289 MB), Total: 13.37 MB, FLOPs: 238,905,264\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 78.689%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Finished fine tuning.\n",
      "Iteration 261/1724 finished in 0m09s\n",
      "Total channels prunned so far: 261\n",
      "\n",
      "Iteration 262 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 345)]\n",
      "Input: 0.077 MB, Params: 3,477,707 (13.266 MB), Total: 13.34 MB, FLOPs: 238,799,622\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 262/1724 finished in 0m09s\n",
      "Total channels prunned so far: 262\n",
      "\n",
      "Iteration 263 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 248)]\n",
      "Input: 0.077 MB, Params: 3,471,837 (13.244 MB), Total: 13.32 MB, FLOPs: 238,693,980\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Finished fine tuning.\n",
      "Iteration 263/1724 finished in 0m09s\n",
      "Total channels prunned so far: 263\n",
      "\n",
      "Iteration 264 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 406)]\n",
      "Input: 0.077 MB, Params: 3,465,967 (13.222 MB), Total: 13.30 MB, FLOPs: 238,588,338\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 264/1724 finished in 0m09s\n",
      "Total channels prunned so far: 264\n",
      "\n",
      "Iteration 265 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 391)]\n",
      "Input: 0.077 MB, Params: 3,460,097 (13.199 MB), Total: 13.28 MB, FLOPs: 238,482,696\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 74.317%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.885%\n",
      "Finished fine tuning.\n",
      "Iteration 265/1724 finished in 0m09s\n",
      "Total channels prunned so far: 265\n",
      "\n",
      "Iteration 266 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 101)]\n",
      "Input: 0.077 MB, Params: 3,458,493 (13.193 MB), Total: 13.27 MB, FLOPs: 237,961,721\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Finished fine tuning.\n",
      "Iteration 266/1724 finished in 0m09s\n",
      "Total channels prunned so far: 266\n",
      "\n",
      "Iteration 267 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 70)]\n",
      "Input: 0.077 MB, Params: 3,452,569 (13.171 MB), Total: 13.25 MB, FLOPs: 237,741,743\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 267/1724 finished in 0m09s\n",
      "Total channels prunned so far: 267\n",
      "\n",
      "Iteration 268 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 208)]\n",
      "Input: 0.077 MB, Params: 3,449,489 (13.159 MB), Total: 13.24 MB, FLOPs: 237,520,055\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 74.317%\n",
      "Finished fine tuning.\n",
      "Iteration 268/1724 finished in 0m09s\n",
      "Total channels prunned so far: 268\n",
      "\n",
      "Iteration 269 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 211)]\n",
      "Input: 0.077 MB, Params: 3,443,574 (13.136 MB), Total: 13.21 MB, FLOPs: 237,300,725\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 81.967%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Finished fine tuning.\n",
      "Iteration 269/1724 finished in 0m09s\n",
      "Total channels prunned so far: 269\n",
      "\n",
      "Iteration 270 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 230)]\n",
      "Input: 0.077 MB, Params: 3,439,743 (13.122 MB), Total: 13.20 MB, FLOPs: 237,231,833\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.328%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 270/1724 finished in 0m09s\n",
      "Total channels prunned so far: 270\n",
      "\n",
      "Iteration 271 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 262)]\n",
      "Input: 0.077 MB, Params: 3,435,912 (13.107 MB), Total: 13.18 MB, FLOPs: 237,162,941\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Finished fine tuning.\n",
      "Iteration 271/1724 finished in 0m09s\n",
      "Total channels prunned so far: 271\n",
      "\n",
      "Iteration 272 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 2)]\n",
      "Input: 0.077 MB, Params: 3,432,081 (13.092 MB), Total: 13.17 MB, FLOPs: 237,094,049\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.896%\n",
      "Finished fine tuning.\n",
      "Iteration 272/1724 finished in 0m09s\n",
      "Total channels prunned so far: 272\n",
      "\n",
      "Iteration 273 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 283)]\n",
      "Input: 0.077 MB, Params: 3,426,256 (13.070 MB), Total: 13.15 MB, FLOPs: 236,989,217\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.989%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Finished fine tuning.\n",
      "Iteration 273/1724 finished in 0m09s\n",
      "Total channels prunned so far: 273\n",
      "\n",
      "Iteration 274 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 153)]\n",
      "Input: 0.077 MB, Params: 3,422,434 (13.056 MB), Total: 13.13 MB, FLOPs: 236,920,487\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Finished fine tuning.\n",
      "Iteration 274/1724 finished in 0m09s\n",
      "Total channels prunned so far: 274\n",
      "\n",
      "Iteration 275 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 130)]\n",
      "Input: 0.077 MB, Params: 3,416,528 (13.033 MB), Total: 13.11 MB, FLOPs: 236,701,319\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Finished fine tuning.\n",
      "Iteration 275/1724 finished in 0m09s\n",
      "Total channels prunned so far: 275\n",
      "\n",
      "Iteration 276 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 12)]\n",
      "Input: 0.077 MB, Params: 3,412,706 (13.018 MB), Total: 13.10 MB, FLOPs: 236,632,589\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 276/1724 finished in 0m09s\n",
      "Total channels prunned so far: 276\n",
      "\n",
      "Iteration 277 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 128)]\n",
      "Input: 0.077 MB, Params: 3,408,884 (13.004 MB), Total: 13.08 MB, FLOPs: 236,563,859\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Finished fine tuning.\n",
      "Iteration 277/1724 finished in 0m09s\n",
      "Total channels prunned so far: 277\n",
      "\n",
      "Iteration 278 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 383)]\n",
      "Input: 0.077 MB, Params: 3,403,095 (12.982 MB), Total: 13.06 MB, FLOPs: 236,459,675\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 80.874%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 278/1724 finished in 0m09s\n",
      "Total channels prunned so far: 278\n",
      "\n",
      "Iteration 279 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 266)]\n",
      "Input: 0.077 MB, Params: 3,397,306 (12.960 MB), Total: 13.04 MB, FLOPs: 236,355,491\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Finished fine tuning.\n",
      "Iteration 279/1724 finished in 0m09s\n",
      "Total channels prunned so far: 279\n",
      "\n",
      "Iteration 280 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 186)]\n",
      "Input: 0.077 MB, Params: 3,393,502 (12.945 MB), Total: 13.02 MB, FLOPs: 236,287,085\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 280/1724 finished in 0m09s\n",
      "Total channels prunned so far: 280\n",
      "\n",
      "Iteration 281 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 201)]\n",
      "Input: 0.077 MB, Params: 3,387,722 (12.923 MB), Total: 13.00 MB, FLOPs: 236,183,063\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 281/1724 finished in 0m09s\n",
      "Total channels prunned so far: 281\n",
      "\n",
      "Iteration 282 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 182)]\n",
      "Input: 0.077 MB, Params: 3,383,927 (12.909 MB), Total: 12.99 MB, FLOPs: 236,114,819\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Finished fine tuning.\n",
      "Iteration 282/1724 finished in 0m09s\n",
      "Total channels prunned so far: 282\n",
      "\n",
      "Iteration 283 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 310)]\n",
      "Input: 0.077 MB, Params: 3,380,132 (12.894 MB), Total: 12.97 MB, FLOPs: 236,046,575\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.153%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Finished fine tuning.\n",
      "Iteration 283/1724 finished in 0m09s\n",
      "Total channels prunned so far: 283\n",
      "\n",
      "Iteration 284 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 160)]\n",
      "Input: 0.077 MB, Params: 3,374,370 (12.872 MB), Total: 12.95 MB, FLOPs: 235,942,877\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 79.235%\n",
      "Finished fine tuning.\n",
      "Iteration 284/1724 finished in 0m09s\n",
      "Total channels prunned so far: 284\n",
      "\n",
      "Iteration 285 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 210)]\n",
      "Input: 0.077 MB, Params: 3,371,308 (12.861 MB), Total: 12.94 MB, FLOPs: 235,722,485\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 84.699%\n",
      "Finished fine tuning.\n",
      "Iteration 285/1724 finished in 0m09s\n",
      "Total channels prunned so far: 285\n",
      "\n",
      "Iteration 286 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 290)]\n",
      "Input: 0.077 MB, Params: 3,365,546 (12.839 MB), Total: 12.92 MB, FLOPs: 235,618,787\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 286/1724 finished in 0m09s\n",
      "Total channels prunned so far: 286\n",
      "\n",
      "Iteration 287 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 367)]\n",
      "Input: 0.077 MB, Params: 3,359,784 (12.817 MB), Total: 12.89 MB, FLOPs: 235,515,089\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.443%\n",
      "Finished fine tuning.\n",
      "Iteration 287/1724 finished in 0m09s\n",
      "Total channels prunned so far: 287\n",
      "\n",
      "Iteration 288 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 136)]\n",
      "Input: 0.077 MB, Params: 3,354,022 (12.795 MB), Total: 12.87 MB, FLOPs: 235,411,391\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 93.989%\n",
      "Finished fine tuning.\n",
      "Iteration 288/1724 finished in 0m09s\n",
      "Total channels prunned so far: 288\n",
      "\n",
      "Iteration 289 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(28, 155)]\n",
      "Input: 0.077 MB, Params: 3,348,188 (12.772 MB), Total: 12.85 MB, FLOPs: 235,194,005\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.896%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Finished fine tuning.\n",
      "Iteration 289/1724 finished in 0m09s\n",
      "Total channels prunned so far: 289\n",
      "\n",
      "Iteration 290 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 226)]\n",
      "Input: 0.077 MB, Params: 3,342,435 (12.750 MB), Total: 12.83 MB, FLOPs: 235,090,469\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 92.350%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 290/1724 finished in 0m09s\n",
      "Total channels prunned so far: 290\n",
      "\n",
      "Iteration 291 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 60)]\n",
      "Input: 0.077 MB, Params: 3,340,831 (12.744 MB), Total: 12.82 MB, FLOPs: 234,569,494\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 291/1724 finished in 0m09s\n",
      "Total channels prunned so far: 291\n",
      "\n",
      "Iteration 292 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 400)]\n",
      "Input: 0.077 MB, Params: 3,337,081 (12.730 MB), Total: 12.81 MB, FLOPs: 234,502,060\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 86.339%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 292/1724 finished in 0m09s\n",
      "Total channels prunned so far: 292\n",
      "\n",
      "Iteration 293 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 120)]\n",
      "Input: 0.077 MB, Params: 3,333,331 (12.716 MB), Total: 12.79 MB, FLOPs: 234,434,626\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Finished fine tuning.\n",
      "Iteration 293/1724 finished in 0m09s\n",
      "Total channels prunned so far: 293\n",
      "\n",
      "Iteration 294 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 343)]\n",
      "Input: 0.077 MB, Params: 3,327,596 (12.694 MB), Total: 12.77 MB, FLOPs: 234,331,414\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 294/1724 finished in 0m09s\n",
      "Total channels prunned so far: 294\n",
      "\n",
      "Iteration 295 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 61)]\n",
      "Input: 0.077 MB, Params: 3,324,543 (12.682 MB), Total: 12.76 MB, FLOPs: 234,111,670\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Finished fine tuning.\n",
      "Iteration 295/1724 finished in 0m09s\n",
      "Total channels prunned so far: 295\n",
      "\n",
      "Iteration 296 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(3, 10)]\n",
      "Input: 0.077 MB, Params: 3,324,501 (12.682 MB), Total: 12.76 MB, FLOPs: 231,433,326\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 296/1724 finished in 0m09s\n",
      "Total channels prunned so far: 296\n",
      "\n",
      "Iteration 297 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 261)]\n",
      "Input: 0.077 MB, Params: 3,320,760 (12.668 MB), Total: 12.74 MB, FLOPs: 231,366,054\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Finished fine tuning.\n",
      "Iteration 297/1724 finished in 0m09s\n",
      "Total channels prunned so far: 297\n",
      "\n",
      "Iteration 298 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(21, 87)]\n",
      "Input: 0.077 MB, Params: 3,317,635 (12.656 MB), Total: 12.73 MB, FLOPs: 230,874,176\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Finished fine tuning.\n",
      "Iteration 298/1724 finished in 0m09s\n",
      "Total channels prunned so far: 298\n",
      "\n",
      "Iteration 299 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(7, 14)]\n",
      "Input: 0.077 MB, Params: 3,317,066 (12.654 MB), Total: 12.73 MB, FLOPs: 230,090,576\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 31.148%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 53.552%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 75.956%\n",
      "Finished fine tuning.\n",
      "Iteration 299/1724 finished in 0m09s\n",
      "Total channels prunned so far: 299\n",
      "\n",
      "Iteration 300 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(25, 144)]\n",
      "Input: 0.077 MB, Params: 3,314,022 (12.642 MB), Total: 12.72 MB, FLOPs: 229,871,480\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Finished fine tuning.\n",
      "Iteration 300/1724 finished in 0m09s\n",
      "Total channels prunned so far: 300\n",
      "\n",
      "Iteration 301 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 258)]\n",
      "Input: 0.077 MB, Params: 3,310,281 (12.628 MB), Total: 12.70 MB, FLOPs: 229,804,208\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Finished fine tuning.\n",
      "Iteration 301/1724 finished in 0m09s\n",
      "Total channels prunned so far: 301\n",
      "\n",
      "Iteration 302 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 355)]\n",
      "Input: 0.077 MB, Params: 3,304,564 (12.606 MB), Total: 12.68 MB, FLOPs: 229,701,320\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.803%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Finished fine tuning.\n",
      "Iteration 302/1724 finished in 0m09s\n",
      "Total channels prunned so far: 302\n",
      "\n",
      "Iteration 303 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 111)]\n",
      "Input: 0.077 MB, Params: 3,300,832 (12.592 MB), Total: 12.67 MB, FLOPs: 229,634,210\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.710%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 78.689%\n",
      "Finished fine tuning.\n",
      "Iteration 303/1724 finished in 0m09s\n",
      "Total channels prunned so far: 303\n",
      "\n",
      "Iteration 304 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(18, 86)]\n",
      "Input: 0.077 MB, Params: 3,299,237 (12.586 MB), Total: 12.66 MB, FLOPs: 229,116,160\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 83.060%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 304/1724 finished in 0m09s\n",
      "Total channels prunned so far: 304\n",
      "\n",
      "Iteration 305 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 116)]\n",
      "Input: 0.077 MB, Params: 3,295,505 (12.571 MB), Total: 12.65 MB, FLOPs: 229,049,050\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Finished fine tuning.\n",
      "Iteration 305/1724 finished in 0m09s\n",
      "Total channels prunned so far: 305\n",
      "\n",
      "Iteration 306 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 32)]\n",
      "Input: 0.077 MB, Params: 3,291,773 (12.557 MB), Total: 12.63 MB, FLOPs: 228,981,940\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 77.596%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 306/1724 finished in 0m09s\n",
      "Total channels prunned so far: 306\n",
      "\n",
      "Iteration 307 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 321)]\n",
      "Input: 0.077 MB, Params: 3,286,083 (12.535 MB), Total: 12.61 MB, FLOPs: 228,879,538\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.432%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 307/1724 finished in 0m09s\n",
      "Total channels prunned so far: 307\n",
      "\n",
      "Iteration 308 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 379)]\n",
      "Input: 0.077 MB, Params: 3,282,360 (12.521 MB), Total: 12.60 MB, FLOPs: 228,812,590\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.246%\n",
      "Finished fine tuning.\n",
      "Iteration 308/1724 finished in 0m09s\n",
      "Total channels prunned so far: 308\n",
      "\n",
      "Iteration 309 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 117)]\n",
      "Input: 0.077 MB, Params: 3,276,679 (12.500 MB), Total: 12.58 MB, FLOPs: 228,710,350\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 73.224%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 87.978%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n",
      "Finished fine tuning.\n",
      "Iteration 309/1724 finished in 0m09s\n",
      "Total channels prunned so far: 309\n",
      "\n",
      "Iteration 310 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(35, 264)]\n",
      "Input: 0.077 MB, Params: 3,272,965 (12.485 MB), Total: 12.56 MB, FLOPs: 228,643,564\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.617%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 89.071%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 90.164%\n",
      "Finished fine tuning.\n",
      "Iteration 310/1724 finished in 0m09s\n",
      "Total channels prunned so far: 310\n",
      "\n",
      "Iteration 311 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 274)]\n",
      "Input: 0.077 MB, Params: 3,267,293 (12.464 MB), Total: 12.54 MB, FLOPs: 228,541,486\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 73.224%\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 85.792%\n",
      "Finished fine tuning.\n",
      "Iteration 311/1724 finished in 0m09s\n",
      "Total channels prunned so far: 311\n",
      "\n",
      "Iteration 312 of 1722 starts..\n",
      "Ranking channels.. \n",
      "Pruning channels: [(32, 356)]\n",
      "Input: 0.077 MB, Params: 3,261,621 (12.442 MB), Total: 12.52 MB, FLOPs: 228,439,408\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 91.257%\n",
      "Fine tuning 2 epochs to recover from prunning iteration.\n",
      "Current Testing Performance - Val: Loss nan  Acc(top1) 88.525%\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cbe351-bc11-4e98-89ce-6c0531393ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c660e0-e1c1-4f30-be4f-d8d322c93eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
