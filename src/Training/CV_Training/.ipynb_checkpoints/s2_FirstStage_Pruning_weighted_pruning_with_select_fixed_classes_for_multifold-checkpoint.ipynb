{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9872611-fa8c-4f4d-89dc-ebea772c16a8",
   "metadata": {},
   "source": [
    "# Weight Pruning Records and Notes\n",
    "## setting\n",
    "- current pruning ratio : 0.85\n",
    "- result: to-add how to observe weight-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15eb05a5-0216-4c21-9226-27bca5bb4b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "import os;\n",
    "import glob;\n",
    "import math;\n",
    "import numpy as np;\n",
    "import random;\n",
    "import time;\n",
    "import torch\n",
    "import torch.optim as optim;\n",
    "import torch.nn as nn;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4fd5d8-421c-438e-a7b9-ea48ef39fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../../src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886f8b70-1961-4c4f-823f-55393ff9c03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.utils as U;\n",
    "import common.opts as opt;\n",
    "import th.resources.models as models;\n",
    "import th.resources.calculator as calc;\n",
    "import th.resources.train_generator as train_generator;\n",
    "import th.resources.pruning_tools.weight_pruning as weight_pruner;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd58c73b-11bf-48dc-895b-0959625f38ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import common.tlopts as tlopts\n",
    "import th.resources.calculator as calc;\n",
    "from datetime import datetime;\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb31f54-0d24-4942-ad4c-f498900e61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log file object\n",
    "logObj = None;\n",
    "def ChkAndCreateSingleDir(dir_path):\n",
    "    if not pathlib.Path(dir_path).is_dir():\n",
    "        os.mkdir(dir_path);\n",
    "        print(f\"'{dir_path}' folder is created.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "944e5af4-56fa-45e2-8ef8-a1b3beb25156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genDataTimeStr():\n",
    "    return datetime.today().strftime('%Y-%m-%d %H:%M:%S').replace('-',\"\").replace(' ',\"\").replace(':',\"\");\n",
    "\n",
    "def getDateStr():\n",
    "    return datetime.today().strftime('%Y-%m-%d %H').replace('-',\"\").replace(' ',\"\")#.replace(':',\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3728c5a-7f01-4efe-b118-e16744d2de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42;\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed);\n",
    "torch.backends.cudnn.deterministic = True;\n",
    "torch.backends.cudnn.benchmark = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02f626e5-5ed6-4aef-908f-2a39342bb0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLGenerator():\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples, labels, options):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        print(f\"length of samples:{len(samples)}\")\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = dict([('52',1),('56',2),('71',3),('99',4)])\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "        #return len(self.samples);\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch_select_fixed_class(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            # print(f\"sound length after U.mix is {len(sound)}\")\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[str(label1)]- 1\n",
    "            idx2 = self.mapdict[str(label2)] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        \n",
    "        return sounds, labels;\n",
    "\n",
    "    def generate_batch_select_fixed_class(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        #two variables recording alarm and moaning sounds count\n",
    "        alarm_selected = 0;\n",
    "        moaning_selected = 0;\n",
    "        help_eng_selected = 0;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                # print(\"enter while true\")\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                lbl1_int = np.int16(label1);\n",
    "                lbl2_int = np.int16(label2);\n",
    "                # print(f\"label1:{label1} and label2:{label2}\")\n",
    "                # print(f\"label1:{type(label1)} and label2:{type(label2)}\")\n",
    "                if (lbl1_int == 52 and lbl2_int == 99) or (lbl1_int == 99 and lbl2_int ==52):\n",
    "                        # print(\"enter 52 second layer if\");\n",
    "                        # if (alarm_selected < moaning_selected) or (alarm_selected == moaning_selected):\n",
    "                    if (alarm_selected == moaning_selected) and (alarm_selected == help_eng_selected):\n",
    "                        alarm_selected += 1;\n",
    "                        break;\n",
    "                if (lbl1_int == 56 and lbl2_int == 99) or (lbl1_int == 99 and lbl2_int == 56):\n",
    "                    if (moaning_selected < alarm_selected) and (moaning_selected == help_eng_selected):\n",
    "                        moaning_selected += 1;\n",
    "                        break;\n",
    "                if (lbl1_int == 71 and lbl2_int == 99) or (lbl1_int == 99 and lbl2_int == 71):\n",
    "                    if (help_eng_selected < alarm_selected) and (help_eng_selected < moaning_selected):\n",
    "                        help_eng_selected += 1;\n",
    "                        break;\n",
    "            # print(f\"escape for loop\");\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random());\n",
    "            #######make wanted class mix ration above 0.5##########\n",
    "            # iLbl1 = np.int16(label1);\n",
    "            # iLbl2 = np.int16(label2);\n",
    "            # r = 1.0;\n",
    "            # p_ratio1 = 0.4\n",
    "            # p_ratio2 = 0.6\n",
    "            # while True:\n",
    "            #     r = np.array(random.random());\n",
    "            #     if r > p_ratio1 and iLbl1 != 99 :\n",
    "            #         break;\n",
    "            #     if r < p_ratio2 and iLbl2 != 99 :\n",
    "            #         break;\n",
    "            #######################End#######################\n",
    "            # print(f\"r:{r}, lbl1:{label1}, lbl2:{label2}\")  \n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[str(label1)]- 1\n",
    "            idx2 = self.mapdict[str(label2)] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            \n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        # print(f\"batchIndex is {batchIndex}, total sounds is {len(sounds)}\")\n",
    "        # print(f\"labels in generate_batch is:\\n{labels}\")\n",
    "        # print(f\"alarm_selected:{alarm_selected}, moaning_selected:{moaning_selected}\");\n",
    "        return sounds, labels;\n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8fb59-b7bf-4cef-b39d-aaf5104c9bf1",
   "metadata": {},
   "source": [
    "### ACDNetV2 define"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2be0009-c832-424a-be9c-666f6af65b4d",
   "metadata": {},
   "source": [
    "### Customed_ACDNetV2 Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d298ee4-f3f7-4001-8224-54b8817a2171",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42;\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed);\n",
    "torch.backends.cudnn.deterministic = True;\n",
    "torch.backends.cudnn.benchmark = False;\n",
    "###########################################\n",
    "\n",
    "class Customed_ACDNetV2(nn.Module):\n",
    "    def __init__(self, input_length, n_class, sr, ch_conf=None):\n",
    "        super(Customed_ACDNetV2, self).__init__();\n",
    "        self.input_length = input_length;\n",
    "        self.ch_config = ch_conf;\n",
    "\n",
    "        stride1 = 2;\n",
    "        stride2 = 2;\n",
    "        channels = 8;\n",
    "        k_size = (3, 3);\n",
    "        n_frames = (sr/1000)*10; #No of frames per 10ms\n",
    "\n",
    "        sfeb_pool_size = int(n_frames/(stride1*stride2));\n",
    "        # tfeb_pool_size = (2,2);\n",
    "        if self.ch_config is None:\n",
    "            self.ch_config = [channels, channels*8, channels*4, channels*8, channels*8, channels*16, channels*16, channels*32, channels*32, channels*64, channels*64, n_class];\n",
    "        # avg_pool_kernel_size = (1,4) if self.ch_config[1] < 64 else (2,4);\n",
    "        fcn_no_of_inputs =  self.ch_config[-1]#n_class #self.ch_config[-1];\n",
    "        # ch_confing_10 = 512 #8 * 64\n",
    "        # ch_n_class = #n_class\n",
    "        conv1, bn1 = self.make_layers(1, self.ch_config[0], (1, 9), (1, stride1));\n",
    "        conv2, bn2 = self.make_layers(self.ch_config[0], self.ch_config[1], (1, 5), (1, stride2));\n",
    "        conv3, bn3 = self.make_layers(1, self.ch_config[2], k_size, padding=1);\n",
    "        conv4, bn4 = self.make_layers(self.ch_config[2], self.ch_config[3], k_size, padding=1);\n",
    "        conv5, bn5 = self.make_layers(self.ch_config[3], self.ch_config[4], k_size, padding=1);\n",
    "        conv6, bn6 = self.make_layers(self.ch_config[4], self.ch_config[5], k_size, padding=1);\n",
    "        conv7, bn7 = self.make_layers(self.ch_config[5], self.ch_config[6], k_size, padding=1);\n",
    "        conv8, bn8 = self.make_layers(self.ch_config[6], self.ch_config[7], k_size, padding=1);\n",
    "        conv9, bn9 = self.make_layers(self.ch_config[7], self.ch_config[8], k_size, padding=1);\n",
    "        conv10, bn10 = self.make_layers(self.ch_config[8], self.ch_config[9], k_size, padding=1);\n",
    "        conv11, bn11 = self.make_layers(self.ch_config[9], self.ch_config[10], k_size, padding=1);\n",
    "        conv12, bn12 = self.make_layers(self.ch_config[10], self.ch_config[11], (1, 1));\n",
    "        fcn = nn.Linear(fcn_no_of_inputs, n_class);\n",
    "        nn.init.kaiming_normal_(fcn.weight, nonlinearity='sigmoid') # kaiming with sigoid is equivalent to lecun_normal in keras\n",
    "\n",
    "        self.sfeb = nn.Sequential(\n",
    "            #Start: Filter bank\n",
    "            conv1, bn1, nn.ReLU(),\\\n",
    "            conv2, bn2, nn.ReLU(),\\\n",
    "            nn.MaxPool2d(kernel_size=(1, sfeb_pool_size))\n",
    "        );\n",
    "\n",
    "        tfeb_modules = [];\n",
    "        self.tfeb_width = int(((self.input_length / sr)*1000)/10); # 10ms frames of audio length in seconds\n",
    "        tfeb_pool_sizes = self.get_tfeb_pool_sizes(self.ch_config[1], self.tfeb_width);\n",
    "        p_index = 0;\n",
    "        for i in [3,4,6,8,10]:\n",
    "            tfeb_modules.extend([eval('conv{}'.format(i)), eval('bn{}'.format(i)), nn.ReLU()]);\n",
    "\n",
    "            if i != 3:\n",
    "                tfeb_modules.extend([eval('conv{}'.format(i+1)), eval('bn{}'.format(i+1)), nn.ReLU()]);\n",
    "\n",
    "            h, w = tfeb_pool_sizes[p_index];\n",
    "            if h>1 or w>1:\n",
    "                tfeb_modules.append(nn.MaxPool2d(kernel_size = (h,w)));\n",
    "            p_index += 1;\n",
    "\n",
    "        tfeb_modules.append(nn.Dropout(0.2));\n",
    "        tfeb_modules.extend([conv12, bn12, nn.ReLU()]);\n",
    "        h, w = tfeb_pool_sizes[-1];\n",
    "        if h>1 or w>1:\n",
    "            tfeb_modules.append(nn.AvgPool2d(kernel_size = (h,w)));\n",
    "        tfeb_modules.extend([nn.Flatten(), fcn]);\n",
    "\n",
    "        self.tfeb = nn.Sequential(*tfeb_modules);\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Softmax(dim=1)\n",
    "        );\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"sfeb:\\n{list(self.sfeb.children())}\");\n",
    "        # print(f\"input x shape:{x.size()}\");\n",
    "        \"\"\"\n",
    "        input dim should be input x shape:torch.Size([32, 1, 1, 30225])\n",
    "        if you got input x shape:[32, 30225, 1, 1], that is wrong.\n",
    "        \"\"\"\n",
    "        x = self.sfeb(x);\n",
    "        #swapaxes\n",
    "        x = x.permute((0, 2, 1, 3));\n",
    "        # print(f\"x shape:{x.size()}\")\n",
    "        x = self.tfeb(x);\n",
    "        y = self.output[0](x);\n",
    "        return y;\n",
    "\n",
    "    def make_layers(self, in_channels, out_channels, kernel_size, stride=(1,1), padding=0, bias=False):\n",
    "        conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias);\n",
    "        nn.init.kaiming_normal_(conv.weight, nonlinearity='relu'); # kaiming with relu is equivalent to he_normal in keras\n",
    "        bn = nn.BatchNorm2d(out_channels);\n",
    "        return conv, bn;\n",
    "\n",
    "    def get_tfeb_pool_sizes(self, con2_ch, width):\n",
    "        h = self.get_tfeb_pool_size_component(con2_ch);\n",
    "        w = self.get_tfeb_pool_size_component(width);\n",
    "        # print(w);\n",
    "        pool_size = [];\n",
    "        for  (h1, w1) in zip(h, w):\n",
    "            pool_size.append((h1, w1));\n",
    "        return pool_size;\n",
    "\n",
    "    def get_tfeb_pool_size_component(self, length):\n",
    "        # print(length);\n",
    "        c = [];\n",
    "        index = 1;\n",
    "        while index <= 6:\n",
    "            if length >= 2:\n",
    "                if index == 6:\n",
    "                    c.append(length);\n",
    "                else:\n",
    "                    c.append(2);\n",
    "                    length = length // 2;\n",
    "            else:\n",
    "               c.append(1);\n",
    "\n",
    "            index += 1;\n",
    "\n",
    "        return c;\n",
    "\n",
    "def GetCustomedACDNetModel(input_len=20150, nclass=4, sr=20000, channel_config=None):\n",
    "    net = Customed_ACDNetV2(input_len, nclass, sr, ch_conf=channel_config);\n",
    "    return net;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22423f68-d925-450f-a24b-567422ef736b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='TLACDNet',  required=False);\n",
    "    parser.add_argument('--data', default='../datasets/processed/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args()\n",
    "    #Leqarning settings\n",
    "    # opt.device=\"cpu\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        opt.device=\"mps\"; #for apple m2 gpu\n",
    "    elif torch.cuda.is_available():\n",
    "        opt.device=\"cuda:0\"; #for nVidia gpu\n",
    "    else:\n",
    "        opt.device=\"cpu\"\n",
    "    opt.batchSize = 64;\n",
    "    opt.LR = 0.01;\n",
    "    opt.momentum = 0.09;\n",
    "    opt.weightDecay = 5e-4;\n",
    "    # opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];#default:[0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "    opt.schedule = [0.02, 0.030, 0.045, 0.060, 0.075];#default:[0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "    opt.warmup = 10;\n",
    "    opt.nEpochs = 1200;\n",
    "    # opt.LR = 0.1;\n",
    "    # opt.momentum = 0.09;\n",
    "    # opt.nEpochs = 1000;#2000;\n",
    "    # opt.schedule = [0.3, 0.6, 0.9];\n",
    "    # opt.warmup = 10;\n",
    "\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = 4#50;\n",
    "    opt.nFolds = 1;#5;\n",
    "    opt.split = 1#[i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 20150;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d6e457-9c01-4aeb-bf23-6ae046b2d781",
   "metadata": {},
   "source": [
    "- <font size=2 color='#FF6600'>For the accuracy and model generation capacity it is better to add more data to the training and validation datasets.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48a4c5d5-e2b4-4775-b7f6-4de0c5147e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainGen(opt=None, split=None):\n",
    "    # dataset = np.load(os.path.join(opt.data, opt.dataset, 'wav{}.npz'.format(opt.sr // 1000)), allow_pickle=True);\n",
    "    # dataset = np.load(\"../datasets/fold1_test16000.npz\", allow_pickle=True);\n",
    "    dataset = np.load(opt.train_data, allow_pickle=True);\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    # train_sounds = [dataset['x'][i][0] for i in range(len(dataset['x']))]\n",
    "    # train_labels = [dataset['y'][i][0] for i in range(len(dataset['y']))]\n",
    "    train_sounds = dataset['fold{}'.format(5)].item()['sounds']\n",
    "    train_labels = dataset['fold{}'.format(5)].item()['labels']\n",
    "    # print(train_sounds)\n",
    "\n",
    "    trainGen = TLGenerator(train_sounds, train_labels, opt);\n",
    "    return trainGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec8011c4-e99c-4675-b692-204ee0cfd268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PruningTrainer:\n",
    "    global logObj\n",
    "    def __init__(self, opt):\n",
    "        self.opt = opt;\n",
    "        #Conditional compression settings\n",
    "        # self.opt.LR = 0.01;\n",
    "        # self.opt.momentum = 0.09;\n",
    "        # self.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "        # self.opt.warmup = 0;\n",
    "        self.opt.prune_algo = 'l0norm';\n",
    "        self.opt.prune_interval = 1;\n",
    "        # self.opt.nEpochs = 1000;\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "        self.bestAcc = 0.0;\n",
    "        self.bestAccEpoch = 0;\n",
    "        self.trainGen = getTrainGen(opt)#train_generator.setup(self.opt, self.opt.split);\n",
    "        if torch.device == \"cuda:0\":\n",
    "            self.device = '\"cuda:0\"'\n",
    "        elif torch.device == \"mps\":\n",
    "            self.device = \"mps\"\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "        # self.device=\"cuda:0\"\n",
    "        print(f\"In PruningTrainer:: current used device:{self.opt.device}\")\n",
    "        # self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\");\n",
    "        self.start_time = time.time();\n",
    "\n",
    "    def PruneAndTrain(self):\n",
    "        self.load_test_data();\n",
    "        print(self.device);\n",
    "        loss_func = torch.nn.KLDivLoss(reduction='batchmean');\n",
    "\n",
    "        #Load saved model dict\n",
    "        base_model_path = \"../../../trained_models/step_1_base_train/multifold/onesecond/base4C_cv_fold5_lr0.1_bs64_wd5e-05_20240722120121/selected/Base4C_fold5_hacc97.26776123046875_valacc_96.72130584716797_tracc_94.88636363636364_1078th_epoch.pt\"\n",
    "        state= torch.load(base_model_path, map_location=self.opt.device);#['weight']\n",
    "        net = GetCustomedACDNetModel(channel_config=state[\"config\"]).to(self.opt.device);#GetACDNetModel()\n",
    "        net.load_state_dict(state['weight']);\n",
    "        calc.summary(net, (1,1,self.opt.inputLength))\n",
    "        net.eval();\n",
    "        val_acc, val_loss = self.__validate(net, loss_func);\n",
    "        print('Testing - Val: Loss {:.3f}  Acc(top1) {:.3f}%'.format(val_loss, val_acc));\n",
    "        net.train();\n",
    "        optimizer = optim.SGD(net.parameters(), lr=self.opt.LR, weight_decay=self.opt.weightDecay, momentum=self.opt.momentum, nesterov=True)\n",
    "\n",
    "        weight_name = [\"weight\"]# if not self.opt.factorize else [\"weightA\", \"weightB\", \"weightC\"]\n",
    "        layers_n = weight_pruner.layers_n(net, param_name=[\"weight\"])[1];\n",
    "        all_num = sum(layers_n.values());\n",
    "        print(\"\\t TOTAL PRUNABLE PARAMS: {}\".format(all_num));\n",
    "        print(\"\\t PRUNE RATIO :{}\".format(self.opt.prune_ratio));\n",
    "        sparse_factor = int(all_num * (1-self.opt.prune_ratio));\n",
    "        print(\"\\t SPARSE FACTOR: {}\".format(sparse_factor));\n",
    "        model_size = (sparse_factor * 4)/1024**2;\n",
    "        print(\"\\t MODEL SIZE: {:.2f} MB\".format(model_size));\n",
    "        prune_algo = getattr(weight_pruner, self.opt.prune_algo);\n",
    "        prune_func = lambda m: prune_algo(m, sparse_factor, param_name=weight_name);\n",
    "\n",
    "        for epoch_idx in range(self.opt.nEpochs):\n",
    "            epoch_start_time = time.time();\n",
    "            optimizer.param_groups[0]['lr'] = self.__get_lr(epoch_idx+1);\n",
    "            cur_lr = optimizer.param_groups[0]['lr'];\n",
    "            running_loss = 0.0;\n",
    "            running_acc = 0.0;\n",
    "            n_batches = math.ceil(len(self.trainGen.data)/self.opt.batchSize);\n",
    "            net.train();\n",
    "            for batch_idx in range(n_batches):\n",
    "                # with torch.no_grad():\n",
    "                # print(f\"enter train epoch:{epoch_idx}; batch:{batch_idx}\")\n",
    "                x,y = self.trainGen.__getitem__(batch_idx)\n",
    "                x = torch.tensor(np.moveaxis(x, 3, 1)).to(self.device);\n",
    "                y = torch.tensor(y).to(self.device);\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad();\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                # outputs = net(x);#in office and use cpu\n",
    "                x = x.type(torch.cuda.FloatTensor)\n",
    "                outputs = net(x);\n",
    "                res_y = y.argmax(dim=1)\n",
    "                res_y = res_y.type(torch.cuda.FloatTensor)\n",
    "                running_acc += ((( outputs.data.argmax(dim=1) == res_y)*1).float().mean()).item();\n",
    "                y = y.type(torch.cuda.FloatTensor)\n",
    "                loss = loss_func(outputs.log(), y);\n",
    "\n",
    "                loss.backward();\n",
    "                optimizer.step();\n",
    "\n",
    "                running_loss += loss.item();\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    prune_func(net);\n",
    "                # print(f\"run prune_fun;\")\n",
    "\n",
    "            prune_func(net)\n",
    "\n",
    "            tr_acc = (running_acc / n_batches)*100;\n",
    "            tr_loss = running_loss / n_batches;\n",
    "\n",
    "            #Epoch wise validation Validation\n",
    "            epoch_train_time = time.time() - epoch_start_time;\n",
    "            net.eval();\n",
    "            val_acc, val_loss = self.__validate(net, loss_func);\n",
    "            #Save best model\n",
    "            self.__save_model(val_acc, tr_acc, epoch_idx, net);\n",
    "\n",
    "            self.__on_epoch_end(epoch_start_time, epoch_train_time, epoch_idx, cur_lr, tr_loss, tr_acc, val_loss, val_acc);\n",
    "\n",
    "            running_loss = 0;\n",
    "            running_acc = 0;\n",
    "            net.train();\n",
    "\n",
    "        total_time_taken = time.time() - self.start_time;\n",
    "        print(\"Execution finished in: {}\".format(U.to_hms(total_time_taken)));\n",
    "\n",
    "    def load_test_data(self):\n",
    "        if(self.testX is None):\n",
    "            data = np.load(self.opt.val_data, allow_pickle=True);\n",
    "            dataX = np.moveaxis(data['x'], 3, 1).astype(np.float32);\n",
    "            self.testX = torch.tensor(dataX).to(self.opt.device);\n",
    "            self.testY = torch.FloatTensor(data['y']).to(self.opt.device);\n",
    "\n",
    "    def __get_lr(self, epoch):\n",
    "        divide_epoch = np.array([self.opt.nEpochs * i for i in self.opt.schedule]);\n",
    "        decay = sum(epoch > divide_epoch);\n",
    "        if epoch <= self.opt.warmup:\n",
    "            decay = 1;\n",
    "        return self.opt.LR * np.power(0.1, decay);\n",
    "\n",
    "    # def __validate(self, net, lossFunc):\n",
    "    #     if self.testX is None:\n",
    "    #         self.load_test_data();\n",
    "    #     net.eval();\n",
    "    #     with torch.no_grad():\n",
    "    #         y_pred = None;\n",
    "    #         batch_size = len(self.testX);\n",
    "    #         x = self.testX[:];\n",
    "    #         scores = net(x);\n",
    "    #         y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "    #         acc, loss = self.__compute_accuracy(y_pred, self.testY, lossFunc);\n",
    "    #     net.train();\n",
    "    #     return acc, loss;\n",
    "\n",
    "    def __validate(self, net, lossFunc):\n",
    "        # print(\"enter __validate\")\n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = (self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops\n",
    "            for idx in range(math.ceil(len(self.testX)/batch_size)):\n",
    "                x = self.testX[idx*batch_size : (idx+1)*batch_size];\n",
    "                x = torch.tensor(x)\n",
    "                x = x.type(torch.cuda.FloatTensor) # use apple mp2\n",
    "                scores = net(x);\n",
    "                y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "            acc, loss = self.__compute_accuracy(y_pred, self.testY, lossFunc);\n",
    "        return acc, loss;\n",
    "\n",
    "    #Calculating average prediction (10 crops) and final accuracy\n",
    "    def __compute_accuracy(self, y_pred, y_target, lossFunc):\n",
    "        with torch.no_grad():\n",
    "            #Reshape to shape theme like each sample comtains 10 samples, calculate mean and find theindices that has highest average value for each sample\n",
    "            y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "            y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "            # print(f\"y_pred:{type(y_pred)}, y_target:{type(y_target)}\")\n",
    "            y_target = y_target.cuda(); #use apple m2, in office use cuda\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "            # valLossFunc = torch.nn.KLDivLoss();\n",
    "            loss = lossFunc(y_pred.float().log(), y_target.float()).item();\n",
    "            # loss = 0.0;\n",
    "        return acc, loss;\n",
    "\n",
    "    def __on_epoch_end(self, epoch_start_time, train_time, epochIdx, lr, tr_loss, tr_acc, val_loss, val_acc):\n",
    "        epoch_time = time.time() - epoch_start_time;\n",
    "        val_time = epoch_time - train_time;\n",
    "        total_time = time.time() - self.start_time;\n",
    "        line = '{} Epoch: {}/{} | Time: {} (Train {}  Val {}) | Train: LR {}  Loss {:.2f}  Acc {:.2f}% | Val: Loss {:.2f}  Acc(top1) {:.2f}% | HA {:.2f}@{}\\n'.format(\n",
    "            U.to_hms(total_time), epochIdx+1, self.opt.nEpochs, U.to_hms(epoch_time), U.to_hms(train_time), U.to_hms(val_time),\n",
    "            lr, tr_loss, tr_acc, val_loss, val_acc, self.bestAcc, self.bestAccEpoch);\n",
    "        # print(line)\n",
    "        sys.stdout.write(line);\n",
    "        sys.stdout.flush();\n",
    "\n",
    "    def __save_model(self, acc, train_acc, epochIdx, net):\n",
    "        if acc > self.bestAcc and acc > self.opt.first_save_acc:\n",
    "            self.bestAcc = acc;\n",
    "            self.bestAccEpoch = epochIdx +1;\n",
    "            self.__do_save_model(acc, train_acc, self.bestAccEpoch, net);\n",
    "        else:\n",
    "            if acc > self.opt.save_val_acc and train_acc > self.opt.save_train_acc: \n",
    "                self.__do_save_model(acc, train_acc, epochIdx, net);\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def __do_save_model(self, acc, tr_acc, epochIdx, net):\n",
    "        save_model_name = self.opt.model_name.format(self.bestAcc, acc, tr_acc, epochIdx, genDataTimeStr());\n",
    "        save_model_fullpath = self.opt.save_dir + save_model_name;\n",
    "        print(f\"save model to {save_model_fullpath}\")\n",
    "        torch.save({'weight':net.state_dict(), 'config':net.ch_config}, save_model_fullpath);\n",
    "        logObj.write(f\"save model:{self.opt.model_name}, bestAcc:{self.bestAcc}, valAcc:{acc}, trainAcc:{tr_acc}, record@{epochIdx}-epoch\");\n",
    "        logObj.write(\"\\n\");\n",
    "        logObj.flush();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb69075-8799-469d-b9c1-a545cee964d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n-----------------------------------------------------------------------------------------------\\npruning ration : 0.9\\nfinal accuracy : \\nepoch: \\nself.opt.LR = 0.01;\\nopt.momentum = 0.9;\\nself.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\\nself.opt.warmup = 0;\\nself.opt.prune_algo = 'l0norm';\\nself.opt.prune_interval = 1;\\nself.opt.nEpochs = 1000;\\n########################## Trainig Data Version 4 ##########################\\nopt.batchSize = 64;\\nopt.LR = 0.1;\\nopt.momentum = 0.9;\\nopt.weightDecay = 5e-4;\\nopt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];#default:[0.15, 0.30, 0.45, 0.60, 0.75];\\nopt.warmup = 10;\\nopt.nEpochs = 600;\\n==============================================================\\n########################## Trainig Data Version 11 ##########################\\nfinal accuracy: train: val: \\nepoch:\\ndataset:version15-fold1\\nopt.batchSize = 64;\\nopt.LR = 0.01;\\nopt.momentum = 0.09;\\nopt.weightDecay = 5e-4;\\nopt.schedule = [0.02, 0.030, 0.045, 0.060, 0.075];#default:[0.15, 0.30, 0.45, 0.60, 0.75];\\nopt.warmup = 10;\\nopt.nEpochs = 600;\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------------\n",
    "pruning ration : 0.9\n",
    "final accuracy : \n",
    "epoch: \n",
    "self.opt.LR = 0.01;\n",
    "opt.momentum = 0.9;\n",
    "self.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "self.opt.warmup = 0;\n",
    "self.opt.prune_algo = 'l0norm';\n",
    "self.opt.prune_interval = 1;\n",
    "self.opt.nEpochs = 1000;\n",
    "########################## Trainig Data Version 4 ##########################\n",
    "opt.batchSize = 64;\n",
    "opt.LR = 0.1;\n",
    "opt.momentum = 0.9;\n",
    "opt.weightDecay = 5e-4;\n",
    "opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];#default:[0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "opt.warmup = 10;\n",
    "opt.nEpochs = 600;\n",
    "==============================================================\n",
    "########################## Trainig Data Version 11 ##########################\n",
    "final accuracy: train: val: \n",
    "epoch:\n",
    "dataset:version15-fold1\n",
    "opt.batchSize = 64;\n",
    "opt.LR = 0.01;\n",
    "opt.momentum = 0.09;\n",
    "opt.weightDecay = 5e-4;\n",
    "opt.schedule = [0.02, 0.030, 0.045, 0.060, 0.075];#default:[0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "opt.warmup = 10;\n",
    "opt.nEpochs = 600;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc95c36e-911a-4242-a895-524867eb8485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_fold_val_dataset_file = \"../../../../uec_iot_models_datasets/multifold/val/version15_multifold_home_fold1/final_fold1_val_version15_multifold_home.npz\"\n",
    "# ver12_single_val_dataset_file = \"../../../../uec_iot_models_datasets/version12_4class_onesecond_input_office/final_single_val_20240619112602.npz\"\n",
    "# multi_fold_val_data = np.load(multi_fold_val_dataset_file, allow_pickle=True);\n",
    "# ver12_single_val_data = np.load(ver12_single_val_dataset_file, allow_pickle=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9fd15f0-cbdf-4b2f-b3c9-96f694eb4a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global logObj;\n",
    "    opt = getOpts()\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 20150;\n",
    "    opt.trainer = None\n",
    "    opt.prune_ratio = 0.85;\n",
    "    opt.first_save_acc = 85.0;\n",
    "    opt.save_val_acc = 85.0;\n",
    "    opt.save_train_acc = 85.0;\n",
    "    opt.train_data = \"../../../../uec_iot_models_datasets/multifold/train/version15_multifold_home_fold5/fold5_train_20240721013721.npz\";\n",
    "    opt.val_data = \"../../../../uec_iot_models_datasets/multifold/val/version15_multifold_home_fold5/final_fold5_val_version15_multifold_home_20240721024402.npz\";\n",
    "    # opt.val_data = \"../../../../uec_iot_models_datasets/version12_4class_onesecond_input_office/final_single_val_20240619112602.npz\"\n",
    "    trainStartTime = getDateStr();\n",
    "    opt.current_fold = \"fold1\"\n",
    "    save_dir = \"../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_{}_{}_prunratio{}/\".format(opt.current_fold,trainStartTime,opt.prune_ratio*100)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    opt.save_dir = save_dir;\n",
    "    opt.model_name = \"uec_4C_weight_prun_{}_\".format(opt.current_fold)+\"haacc_{}_valacc{}_tracc{}_epoch_{}_{}.pt\";\n",
    "    print(\"Initializing PruneAndTrain Object.....\")\n",
    "    trainer = PruningTrainer(opt)#TLTrainer(opt)\n",
    "    print(\"Start to pruning.....\")\n",
    "    logSaveDir = \"./first_stage_pruning_logs/\"\n",
    "    ChkAndCreateSingleDir(logSaveDir);\n",
    "    logName = \"FirstPruningLog_{}.log\".format(trainStartTime);\n",
    "    logObj = open(os.path.join(logSaveDir,logName),'w');\n",
    "    trainer.PruneAndTrain();\n",
    "    logObj.flush();\n",
    "    logObj.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8780ef94-ad0c-46bc-abb9-a5573b8b3480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PruneAndTrain Object.....\n",
      "length of samples:704\n",
      "In PruningTrainer:: current used device:cuda:0\n",
      "Start to pruning.....\n",
      "cpu\n",
      "+----------------------------------------------------------------------------+\n",
      "+                           Pytorch Model Summary                            +\n",
      "------------------------------------------------------------------------------\n",
      "   Layer (type)       Input Shape      Output Shape    Param #      FLOPS #\n",
      "==============================================================================\n",
      "       Conv2d-1     (1, 1, 20150)     (8, 1, 10071)         72      725,112\n",
      "  BatchNorm2d-2     (8, 1, 10071)     (8, 1, 10071)         16            0\n",
      "         ReLu-3     (8, 1, 10071)     (8, 1, 10071)          0       80,568\n",
      "       Conv2d-4     (8, 1, 10071)     (64, 1, 5034)      2,560   12,887,040\n",
      "  BatchNorm2d-5     (64, 1, 5034)     (64, 1, 5034)        128            0\n",
      "         ReLu-6     (64, 1, 5034)     (64, 1, 5034)          0      322,176\n",
      "    MaxPool2d-7     (64, 1, 5034)      (64, 1, 100)          0      320,000\n",
      "      Permute-8      (64, 1, 100)      (1, 64, 100)          0            0\n",
      "       Conv2d-9      (1, 64, 100)     (32, 64, 100)        288    1,843,200\n",
      " BatchNorm2d-10     (32, 64, 100)     (32, 64, 100)         64            0\n",
      "        ReLu-11     (32, 64, 100)     (32, 64, 100)          0      204,800\n",
      "   MaxPool2d-12     (32, 64, 100)      (32, 32, 50)          0      204,800\n",
      "      Conv2d-13      (32, 32, 50)      (64, 32, 50)     18,432   29,491,200\n",
      " BatchNorm2d-14      (64, 32, 50)      (64, 32, 50)        128            0\n",
      "        ReLu-15      (64, 32, 50)      (64, 32, 50)          0      102,400\n",
      "      Conv2d-16      (64, 32, 50)      (64, 32, 50)     36,864   58,982,400\n",
      " BatchNorm2d-17      (64, 32, 50)      (64, 32, 50)        128            0\n",
      "        ReLu-18      (64, 32, 50)      (64, 32, 50)          0      102,400\n",
      "   MaxPool2d-19      (64, 32, 50)      (64, 16, 25)          0      102,400\n",
      "      Conv2d-20      (64, 16, 25)     (128, 16, 25)     73,728   29,491,200\n",
      " BatchNorm2d-21     (128, 16, 25)     (128, 16, 25)        256            0\n",
      "        ReLu-22     (128, 16, 25)     (128, 16, 25)          0       51,200\n",
      "      Conv2d-23     (128, 16, 25)     (128, 16, 25)    147,456   58,982,400\n",
      " BatchNorm2d-24     (128, 16, 25)     (128, 16, 25)        256            0\n",
      "        ReLu-25     (128, 16, 25)     (128, 16, 25)          0       51,200\n",
      "   MaxPool2d-26     (128, 16, 25)      (128, 8, 12)          0       49,152\n",
      "      Conv2d-27      (128, 8, 12)      (256, 8, 12)    294,912   28,311,552\n",
      " BatchNorm2d-28      (256, 8, 12)      (256, 8, 12)        512            0\n",
      "        ReLu-29      (256, 8, 12)      (256, 8, 12)          0       24,576\n",
      "      Conv2d-30      (256, 8, 12)      (256, 8, 12)    589,824   56,623,104\n",
      " BatchNorm2d-31      (256, 8, 12)      (256, 8, 12)        512            0\n",
      "        ReLu-32      (256, 8, 12)      (256, 8, 12)          0       24,576\n",
      "   MaxPool2d-33      (256, 8, 12)       (256, 4, 6)          0       24,576\n",
      "      Conv2d-34       (256, 4, 6)       (512, 4, 6)  1,179,648   28,311,552\n",
      " BatchNorm2d-35       (512, 4, 6)       (512, 4, 6)      1,024            0\n",
      "        ReLu-36       (512, 4, 6)       (512, 4, 6)          0       12,288\n",
      "      Conv2d-37       (512, 4, 6)       (512, 4, 6)  2,359,296   56,623,104\n",
      " BatchNorm2d-38       (512, 4, 6)       (512, 4, 6)      1,024            0\n",
      "        ReLu-39       (512, 4, 6)       (512, 4, 6)          0       12,288\n",
      "   MaxPool2d-40       (512, 4, 6)       (512, 2, 3)          0       12,288\n",
      "      Conv2d-41       (512, 2, 3)         (4, 2, 3)      2,048       12,288\n",
      " BatchNorm2d-42         (4, 2, 3)         (4, 2, 3)          8            0\n",
      "        ReLu-43         (4, 2, 3)         (4, 2, 3)          0           24\n",
      "   AvgPool2d-44         (4, 2, 3)         (4, 1, 1)          0           24\n",
      "     Flatten-45         (4, 1, 1)            (1, 4)          0            0\n",
      "      Linear-46            (1, 4)            (1, 4)         20           20\n",
      "     Softmax-47            (1, 4)            (1, 4)          0            4\n",
      "==============================================================================\n",
      "Total Params: 4,709,204\n",
      "Total FLOPs : 363,985,912\n",
      "------------------------------------------------------------------------------\n",
      "Input size (MB) : 0.08\n",
      "Params size (MB): 17.96\n",
      "Total size (MB) : 18.04\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39915/3617269135.py:151: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing - Val: Loss nan  Acc(top1) 96.721%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/miniconda3/envs/acdnetenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t TOTAL PRUNABLE PARAMS: 4705144\n",
      "\t PRUNE RATIO :0.85\n",
      "\t SPARSE FACTOR: 705771\n",
      "\t MODEL SIZE: 2.69 MB\n",
      "0m04s Epoch: 1/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.14  Acc 83.66% | Val: Loss nan  Acc(top1) 70.49% | HA 0.00@0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39915/3617269135.py:151: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m08s Epoch: 2/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.14  Acc 86.08% | Val: Loss nan  Acc(top1) 36.07% | HA 0.00@0\n",
      "0m11s Epoch: 3/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.15  Acc 85.65% | Val: Loss nan  Acc(top1) 37.70% | HA 0.00@0\n",
      "0m15s Epoch: 4/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.13  Acc 85.37% | Val: Loss nan  Acc(top1) 36.07% | HA 0.00@0\n",
      "0m18s Epoch: 5/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.14  Acc 83.66% | Val: Loss nan  Acc(top1) 49.18% | HA 0.00@0\n",
      "0m22s Epoch: 6/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.13  Acc 87.22% | Val: Loss nan  Acc(top1) 42.08% | HA 0.00@0\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_86.33879089355469_valacc86.33879089355469_tracc85.6534090909091_epoch_7_20240723101743.pt\n",
      "0m25s Epoch: 7/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.14  Acc 85.65% | Val: Loss nan  Acc(top1) 86.34% | HA 86.34@7\n",
      "0m28s Epoch: 8/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.13  Acc 83.95% | Val: Loss nan  Acc(top1) 37.16% | HA 86.34@7\n",
      "0m32s Epoch: 9/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.13  Acc 88.35% | Val: Loss nan  Acc(top1) 33.88% | HA 86.34@7\n",
      "0m35s Epoch: 10/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.14  Acc 87.36% | Val: Loss nan  Acc(top1) 33.33% | HA 86.34@7\n",
      "0m39s Epoch: 11/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.01  Loss 0.11  Acc 87.07% | Val: Loss nan  Acc(top1) 45.36% | HA 86.34@7\n",
      "0m42s Epoch: 12/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.01  Loss 0.11  Acc 87.22% | Val: Loss nan  Acc(top1) 27.87% | HA 86.34@7\n",
      "0m46s Epoch: 13/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.01  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 37.70% | HA 86.34@7\n",
      "0m49s Epoch: 14/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.01  Loss 0.11  Acc 86.79% | Val: Loss nan  Acc(top1) 47.54% | HA 86.34@7\n",
      "0m52s Epoch: 15/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.01  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 35.52% | HA 86.34@7\n",
      "0m56s Epoch: 16/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.01  Loss 0.12  Acc 89.63% | Val: Loss nan  Acc(top1) 36.61% | HA 86.34@7\n",
      "0m59s Epoch: 17/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.01  Loss 0.11  Acc 89.91% | Val: Loss nan  Acc(top1) 51.37% | HA 86.34@7\n",
      "1m03s Epoch: 18/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.01  Loss 0.12  Acc 85.65% | Val: Loss nan  Acc(top1) 73.77% | HA 86.34@7\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc95.08197021484375_tracc86.36363636363636_epoch_19_20240723101824.pt\n",
      "1m06s Epoch: 19/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.01  Loss 0.10  Acc 86.36% | Val: Loss nan  Acc(top1) 95.08% | HA 95.08@19\n",
      "1m10s Epoch: 20/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.01  Loss 0.11  Acc 87.50% | Val: Loss nan  Acc(top1) 57.92% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc85.79234313964844_tracc87.2159090909091_epoch_20_20240723101831.pt\n",
      "1m13s Epoch: 21/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.01  Loss 0.10  Acc 87.22% | Val: Loss nan  Acc(top1) 85.79% | HA 95.08@19\n",
      "1m16s Epoch: 22/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.01  Loss 0.10  Acc 86.08% | Val: Loss nan  Acc(top1) 51.91% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc89.63068181818183_epoch_22_20240723101838.pt\n",
      "1m20s Epoch: 23/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.01  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc94.53551483154297_tracc88.21022727272727_epoch_23_20240723101842.pt\n",
      "1m23s Epoch: 24/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.01  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 94.54% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc87.92613636363636_epoch_24_20240723101845.pt\n",
      "1m27s Epoch: 25/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "1m30s Epoch: 26/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc86.33879089355469_tracc85.22727272727273_epoch_26_20240723101852.pt\n",
      "1m34s Epoch: 27/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.10  Acc 85.23% | Val: Loss nan  Acc(top1) 86.34% | HA 95.08@19\n",
      "1m37s Epoch: 28/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 67.21% | HA 95.08@19\n",
      "1m41s Epoch: 29/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.10  Acc 86.79% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "1m44s Epoch: 30/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "1m47s Epoch: 31/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 82.51% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc88.21022727272727_epoch_31_20240723101909.pt\n",
      "1m51s Epoch: 32/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "1m54s Epoch: 33/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc91.80327606201172_tracc87.07386363636364_epoch_33_20240723101916.pt\n",
      "1m58s Epoch: 34/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.09  Acc 87.07% | Val: Loss nan  Acc(top1) 91.80% | HA 95.08@19\n",
      "2m01s Epoch: 35/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.11  Acc 89.06% | Val: Loss nan  Acc(top1) 68.31% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc88.21022727272727_epoch_35_20240723101923.pt\n",
      "2m04s Epoch: 36/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.001  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "2m08s Epoch: 37/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "2m11s Epoch: 38/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.10  Acc 91.05% | Val: Loss nan  Acc(top1) 60.66% | HA 95.08@19\n",
      "2m15s Epoch: 39/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "2m18s Epoch: 40/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.08  Acc 88.21% | Val: Loss nan  Acc(top1) 68.31% | HA 95.08@19\n",
      "2m22s Epoch: 41/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 77.05% | HA 95.08@19\n",
      "2m25s Epoch: 42/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 56.83% | HA 95.08@19\n",
      "2m28s Epoch: 43/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.12  Acc 89.20% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "2m32s Epoch: 44/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 82.51% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc90.16393280029297_tracc89.0625_epoch_44_20240723101953.pt\n",
      "2m35s Epoch: 45/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.11  Acc 89.06% | Val: Loss nan  Acc(top1) 90.16% | HA 95.08@19\n",
      "2m39s Epoch: 46/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 56.28% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc89.07103729248047_tracc88.35227272727273_epoch_46_20240723102000.pt\n",
      "2m42s Epoch: 47/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 89.07% | HA 95.08@19\n",
      "2m45s Epoch: 48/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 59.56% | HA 95.08@19\n",
      "2m49s Epoch: 49/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 82.51% | HA 95.08@19\n",
      "2m52s Epoch: 50/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.09  Acc 90.06% | Val: Loss nan  Acc(top1) 83.61% | HA 95.08@19\n",
      "2m56s Epoch: 51/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "2m59s Epoch: 52/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "3m02s Epoch: 53/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.09  Acc 88.35% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc88.52458953857422_tracc88.35227272727273_epoch_53_20240723102024.pt\n",
      "3m06s Epoch: 54/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 0.00010000000000000002  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 88.52% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc87.97814178466797_tracc88.49431818181817_epoch_54_20240723102027.pt\n",
      "3m09s Epoch: 55/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 87.98% | HA 95.08@19\n",
      "3m13s Epoch: 56/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 61.20% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc87.97814178466797_tracc89.20454545454545_epoch_56_20240723102034.pt\n",
      "3m16s Epoch: 57/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 87.98% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc92.34972381591797_tracc89.63068181818183_epoch_57_20240723102038.pt\n",
      "3m20s Epoch: 58/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.08  Acc 89.63% | Val: Loss nan  Acc(top1) 92.35% | HA 95.08@19\n",
      "3m23s Epoch: 59/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 54.64% | HA 95.08@19\n",
      "3m26s Epoch: 60/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "3m30s Epoch: 61/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 60.66% | HA 95.08@19\n",
      "3m33s Epoch: 62/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "3m37s Epoch: 63/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.11  Acc 87.22% | Val: Loss nan  Acc(top1) 60.66% | HA 95.08@19\n",
      "3m40s Epoch: 64/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "3m43s Epoch: 65/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 78.69% | HA 95.08@19\n",
      "3m47s Epoch: 66/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "3m50s Epoch: 67/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.09  Acc 90.48% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "3m54s Epoch: 68/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 90.34% | Val: Loss nan  Acc(top1) 31.15% | HA 95.08@19\n",
      "3m57s Epoch: 69/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "4m00s Epoch: 70/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 55.74% | HA 95.08@19\n",
      "4m04s Epoch: 71/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.09  Acc 90.77% | Val: Loss nan  Acc(top1) 65.03% | HA 95.08@19\n",
      "4m07s Epoch: 72/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000003e-05  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 31.15% | HA 95.08@19\n",
      "4m11s Epoch: 73/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.11  Acc 88.64% | Val: Loss nan  Acc(top1) 43.17% | HA 95.08@19\n",
      "4m14s Epoch: 74/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 67.76% | HA 95.08@19\n",
      "4m18s Epoch: 75/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 81.97% | HA 95.08@19\n",
      "4m21s Epoch: 76/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 77.05% | HA 95.08@19\n",
      "4m24s Epoch: 77/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.09  Acc 88.49% | Val: Loss nan  Acc(top1) 79.78% | HA 95.08@19\n",
      "4m28s Epoch: 78/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 63.93% | HA 95.08@19\n",
      "4m31s Epoch: 79/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 70.49% | HA 95.08@19\n",
      "4m35s Epoch: 80/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.10  Acc 90.20% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "4m38s Epoch: 81/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.10  Acc 90.77% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "4m41s Epoch: 82/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.08  Acc 90.91% | Val: Loss nan  Acc(top1) 73.22% | HA 95.08@19\n",
      "4m45s Epoch: 83/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.10  Acc 90.20% | Val: Loss nan  Acc(top1) 34.97% | HA 95.08@19\n",
      "4m48s Epoch: 84/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 47.54% | HA 95.08@19\n",
      "4m52s Epoch: 85/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.11  Acc 90.34% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "4m55s Epoch: 86/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 48.09% | HA 95.08@19\n",
      "4m58s Epoch: 87/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.08  Acc 90.91% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "5m02s Epoch: 88/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "5m05s Epoch: 89/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "5m09s Epoch: 90/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-06  Loss 0.09  Acc 90.77% | Val: Loss nan  Acc(top1) 51.91% | HA 95.08@19\n",
      "5m12s Epoch: 91/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "5m15s Epoch: 92/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 49.18% | HA 95.08@19\n",
      "5m19s Epoch: 93/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "5m22s Epoch: 94/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 48.63% | HA 95.08@19\n",
      "5m26s Epoch: 95/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 76.50% | HA 95.08@19\n",
      "5m29s Epoch: 96/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 81.97% | HA 95.08@19\n",
      "5m33s Epoch: 97/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 34.43% | HA 95.08@19\n",
      "5m36s Epoch: 98/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "5m39s Epoch: 99/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 34.97% | HA 95.08@19\n",
      "5m43s Epoch: 100/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 35.52% | HA 95.08@19\n",
      "5m46s Epoch: 101/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.93% | Val: Loss nan  Acc(top1) 37.16% | HA 95.08@19\n",
      "5m50s Epoch: 102/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "5m53s Epoch: 103/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.07% | Val: Loss nan  Acc(top1) 71.58% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc87.97814178466797_tracc89.0625_epoch_103_20240723102315.pt\n",
      "5m57s Epoch: 104/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 87.98% | HA 95.08@19\n",
      "6m00s Epoch: 105/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.20% | Val: Loss nan  Acc(top1) 76.50% | HA 95.08@19\n",
      "6m03s Epoch: 106/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.77% | Val: Loss nan  Acc(top1) 69.95% | HA 95.08@19\n",
      "6m07s Epoch: 107/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "6m10s Epoch: 108/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.63% | Val: Loss nan  Acc(top1) 34.43% | HA 95.08@19\n",
      "6m14s Epoch: 109/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 85.65% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "6m17s Epoch: 110/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 92.05% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "6m21s Epoch: 111/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 90.34% | Val: Loss nan  Acc(top1) 55.19% | HA 95.08@19\n",
      "6m24s Epoch: 112/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 67.76% | HA 95.08@19\n",
      "6m28s Epoch: 113/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.62% | Val: Loss nan  Acc(top1) 73.77% | HA 95.08@19\n",
      "6m31s Epoch: 114/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.21% | Val: Loss nan  Acc(top1) 55.74% | HA 95.08@19\n",
      "6m35s Epoch: 115/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 56.28% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc89.07103729248047_tracc87.2159090909091_epoch_115_20240723102356.pt\n",
      "6m38s Epoch: 116/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.22% | Val: Loss nan  Acc(top1) 89.07% | HA 95.08@19\n",
      "6m42s Epoch: 117/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.36% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "6m45s Epoch: 118/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "6m48s Epoch: 119/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.50% | Val: Loss nan  Acc(top1) 77.05% | HA 95.08@19\n",
      "6m52s Epoch: 120/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 56.28% | HA 95.08@19\n",
      "6m55s Epoch: 121/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 91.19% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "6m59s Epoch: 122/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 54.64% | HA 95.08@19\n",
      "7m02s Epoch: 123/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.78% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc88.52458953857422_tracc89.0625_epoch_123_20240723102424.pt\n",
      "7m06s Epoch: 124/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 88.52% | HA 95.08@19\n",
      "7m09s Epoch: 125/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 48.09% | HA 95.08@19\n",
      "7m13s Epoch: 126/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.34% | Val: Loss nan  Acc(top1) 65.57% | HA 95.08@19\n",
      "7m16s Epoch: 127/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 60.66% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc89.61748504638672_tracc88.49431818181817_epoch_127_20240723102438.pt\n",
      "7m20s Epoch: 128/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 89.62% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc88.52458953857422_tracc91.19318181818183_epoch_128_20240723102441.pt\n",
      "7m23s Epoch: 129/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.19% | Val: Loss nan  Acc(top1) 88.52% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc92.89617156982422_tracc88.92045454545455_epoch_129_20240723102445.pt\n",
      "7m27s Epoch: 130/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 92.90% | HA 95.08@19\n",
      "7m30s Epoch: 131/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "7m34s Epoch: 132/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.36% | Val: Loss nan  Acc(top1) 56.28% | HA 95.08@19\n",
      "7m37s Epoch: 133/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc86.33879089355469_tracc89.63068181818183_epoch_133_20240723102459.pt\n",
      "7m40s Epoch: 134/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 89.63% | Val: Loss nan  Acc(top1) 86.34% | HA 95.08@19\n",
      "7m44s Epoch: 135/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.78% | Val: Loss nan  Acc(top1) 61.20% | HA 95.08@19\n",
      "7m47s Epoch: 136/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 42.08% | HA 95.08@19\n",
      "7m51s Epoch: 137/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.77% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "7m54s Epoch: 138/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 85.94% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc87.97814178466797_tracc91.05113636363636_epoch_138_20240723102516.pt\n",
      "7m58s Epoch: 139/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.05% | Val: Loss nan  Acc(top1) 87.98% | HA 95.08@19\n",
      "8m01s Epoch: 140/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 82.51% | HA 95.08@19\n",
      "8m05s Epoch: 141/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "8m08s Epoch: 142/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "8m12s Epoch: 143/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "8m15s Epoch: 144/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "8m19s Epoch: 145/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.05% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "8m22s Epoch: 146/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 86.93% | Val: Loss nan  Acc(top1) 57.92% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc86.33879089355469_tracc89.63068181818183_epoch_146_20240723102544.pt\n",
      "8m26s Epoch: 147/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 86.34% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc90.05681818181817_epoch_147_20240723102547.pt\n",
      "8m29s Epoch: 148/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 90.06% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc89.07103729248047_tracc88.49431818181817_epoch_148_20240723102551.pt\n",
      "8m33s Epoch: 149/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 89.07% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc95.08197021484375_tracc89.0625_epoch_149_20240723102554.pt\n",
      "8m36s Epoch: 150/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.06% | Val: Loss nan  Acc(top1) 95.08% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc88.06818181818183_epoch_150_20240723102558.pt\n",
      "8m40s Epoch: 151/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "8m43s Epoch: 152/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 57.38% | HA 95.08@19\n",
      "8m47s Epoch: 153/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.62% | Val: Loss nan  Acc(top1) 47.54% | HA 95.08@19\n",
      "8m50s Epoch: 154/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 43.72% | HA 95.08@19\n",
      "8m53s Epoch: 155/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "8m57s Epoch: 156/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "9m00s Epoch: 157/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 68.85% | HA 95.08@19\n",
      "9m04s Epoch: 158/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 91.19% | Val: Loss nan  Acc(top1) 44.81% | HA 95.08@19\n",
      "9m07s Epoch: 159/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.62% | Val: Loss nan  Acc(top1) 44.26% | HA 95.08@19\n",
      "9m10s Epoch: 160/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 37.70% | HA 95.08@19\n",
      "9m14s Epoch: 161/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 87.64% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "9m17s Epoch: 162/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 58.47% | HA 95.08@19\n",
      "9m21s Epoch: 163/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.48% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "9m24s Epoch: 164/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 91.62% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "9m27s Epoch: 165/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 52.46% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc92.34972381591797_tracc87.2159090909091_epoch_165_20240723102649.pt\n",
      "9m31s Epoch: 166/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.22% | Val: Loss nan  Acc(top1) 92.35% | HA 95.08@19\n",
      "9m34s Epoch: 167/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.64% | Val: Loss nan  Acc(top1) 77.60% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc92.89617156982422_tracc89.3465909090909_epoch_167_20240723102656.pt\n",
      "9m38s Epoch: 168/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 92.90% | HA 95.08@19\n",
      "9m41s Epoch: 169/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 65.57% | HA 95.08@19\n",
      "9m45s Epoch: 170/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.36% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "9m48s Epoch: 171/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 39.89% | HA 95.08@19\n",
      "9m52s Epoch: 172/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.91% | Val: Loss nan  Acc(top1) 63.93% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc95.08197021484375_tracc88.35227272727273_epoch_172_20240723102713.pt\n",
      "9m55s Epoch: 173/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 95.08% | HA 95.08@19\n",
      "9m59s Epoch: 174/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.91% | Val: Loss nan  Acc(top1) 42.62% | HA 95.08@19\n",
      "10m02s Epoch: 175/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 87.07% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "10m05s Epoch: 176/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "10m09s Epoch: 177/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 73.22% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc90.16393280029297_tracc87.5_epoch_177_20240723102731.pt\n",
      "10m12s Epoch: 178/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.50% | Val: Loss nan  Acc(top1) 90.16% | HA 95.08@19\n",
      "10m16s Epoch: 179/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.49% | Val: Loss nan  Acc(top1) 69.40% | HA 95.08@19\n",
      "10m19s Epoch: 180/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 83.61% | HA 95.08@19\n",
      "10m23s Epoch: 181/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "10m26s Epoch: 182/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 42.08% | HA 95.08@19\n",
      "10m30s Epoch: 183/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.51% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "10m33s Epoch: 184/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.93% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc87.97814178466797_tracc89.48863636363636_epoch_184_20240723102755.pt\n",
      "10m37s Epoch: 185/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 87.98% | HA 95.08@19\n",
      "10m40s Epoch: 186/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 88.78% | Val: Loss nan  Acc(top1) 60.11% | HA 95.08@19\n",
      "10m44s Epoch: 187/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "10m47s Epoch: 188/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 69.40% | HA 95.08@19\n",
      "10m50s Epoch: 189/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "10m54s Epoch: 190/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.35% | Val: Loss nan  Acc(top1) 68.85% | HA 95.08@19\n",
      "10m57s Epoch: 191/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "11m01s Epoch: 192/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.93% | Val: Loss nan  Acc(top1) 62.30% | HA 95.08@19\n",
      "11m04s Epoch: 193/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 69.95% | HA 95.08@19\n",
      "11m08s Epoch: 194/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.92% | Val: Loss nan  Acc(top1) 36.61% | HA 95.08@19\n",
      "11m11s Epoch: 195/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 28.42% | HA 95.08@19\n",
      "11m15s Epoch: 196/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 38.25% | HA 95.08@19\n",
      "11m18s Epoch: 197/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "11m21s Epoch: 198/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 53.55% | HA 95.08@19\n",
      "11m25s Epoch: 199/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.06% | Val: Loss nan  Acc(top1) 61.20% | HA 95.08@19\n",
      "11m28s Epoch: 200/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 54.64% | HA 95.08@19\n",
      "11m32s Epoch: 201/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 55.74% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.44261932373047_tracc89.63068181818183_epoch_201_20240723102853.pt\n",
      "11m35s Epoch: 202/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 93.44% | HA 95.08@19\n",
      "11m39s Epoch: 203/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 56.28% | HA 95.08@19\n",
      "11m42s Epoch: 204/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.35% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "11m46s Epoch: 205/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.07% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "11m49s Epoch: 206/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 91.34% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "11m53s Epoch: 207/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.36% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc87.97814178466797_tracc90.3409090909091_epoch_207_20240723102914.pt\n",
      "11m56s Epoch: 208/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 87.98% | HA 95.08@19\n",
      "11m59s Epoch: 209/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 44.26% | HA 95.08@19\n",
      "12m03s Epoch: 210/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.78% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "12m06s Epoch: 211/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.20% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "12m10s Epoch: 212/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.20% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "12m13s Epoch: 213/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 88.21% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "12m17s Epoch: 214/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "12m20s Epoch: 215/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.64% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "12m24s Epoch: 216/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 66.12% | HA 95.08@19\n",
      "12m27s Epoch: 217/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 58.47% | HA 95.08@19\n",
      "12m30s Epoch: 218/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.48% | Val: Loss nan  Acc(top1) 57.92% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc87.97814178466797_tracc88.35227272727273_epoch_218_20240723102952.pt\n",
      "12m34s Epoch: 219/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.35% | Val: Loss nan  Acc(top1) 87.98% | HA 95.08@19\n",
      "12m37s Epoch: 220/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 73.22% | HA 95.08@19\n",
      "12m41s Epoch: 221/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 68.85% | HA 95.08@19\n",
      "12m44s Epoch: 222/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.77% | Val: Loss nan  Acc(top1) 75.96% | HA 95.08@19\n",
      "12m47s Epoch: 223/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 40.44% | HA 95.08@19\n",
      "12m51s Epoch: 224/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "12m54s Epoch: 225/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 71.04% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc86.88524627685547_tracc91.76136363636364_epoch_225_20240723103016.pt\n",
      "12m58s Epoch: 226/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.76% | Val: Loss nan  Acc(top1) 86.89% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc92.89617156982422_tracc88.63636363636364_epoch_226_20240723103019.pt\n",
      "13m01s Epoch: 227/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 92.90% | HA 95.08@19\n",
      "13m04s Epoch: 228/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "13m08s Epoch: 229/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 42.62% | HA 95.08@19\n",
      "13m11s Epoch: 230/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.93% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "13m15s Epoch: 231/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 51.91% | HA 95.08@19\n",
      "13m18s Epoch: 232/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.50% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "13m22s Epoch: 233/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "13m25s Epoch: 234/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.22% | Val: Loss nan  Acc(top1) 59.02% | HA 95.08@19\n",
      "13m29s Epoch: 235/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.48% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "13m32s Epoch: 236/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.62% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "13m35s Epoch: 237/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.64% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "13m39s Epoch: 238/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 59.56% | HA 95.08@19\n",
      "13m42s Epoch: 239/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.93% | Val: Loss nan  Acc(top1) 51.91% | HA 95.08@19\n",
      "13m46s Epoch: 240/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 40.98% | HA 95.08@19\n",
      "13m49s Epoch: 241/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.06% | Val: Loss nan  Acc(top1) 47.54% | HA 95.08@19\n",
      "13m53s Epoch: 242/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 40.98% | HA 95.08@19\n",
      "13m56s Epoch: 243/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 55.74% | HA 95.08@19\n",
      "13m59s Epoch: 244/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 59.02% | HA 95.08@19\n",
      "14m03s Epoch: 245/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.90% | Val: Loss nan  Acc(top1) 57.38% | HA 95.08@19\n",
      "14m06s Epoch: 246/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 37.16% | HA 95.08@19\n",
      "14m10s Epoch: 247/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.35% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "14m13s Epoch: 248/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.34% | Val: Loss nan  Acc(top1) 82.51% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc89.91477272727273_epoch_248_20240723103135.pt\n",
      "14m17s Epoch: 249/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.91% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "14m20s Epoch: 250/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.91% | Val: Loss nan  Acc(top1) 59.02% | HA 95.08@19\n",
      "14m24s Epoch: 251/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "14m27s Epoch: 252/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.79% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "14m31s Epoch: 253/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "14m34s Epoch: 254/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 53.55% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc90.05681818181817_epoch_254_20240723103156.pt\n",
      "14m38s Epoch: 255/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.06% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc91.25682830810547_tracc89.48863636363636_epoch_255_20240723103159.pt\n",
      "14m41s Epoch: 256/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 91.26% | HA 95.08@19\n",
      "14m44s Epoch: 257/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.35% | Val: Loss nan  Acc(top1) 56.28% | HA 95.08@19\n",
      "14m48s Epoch: 258/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.91% | Val: Loss nan  Acc(top1) 74.32% | HA 95.08@19\n",
      "14m51s Epoch: 259/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 66.12% | HA 95.08@19\n",
      "14m55s Epoch: 260/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.91% | Val: Loss nan  Acc(top1) 58.47% | HA 95.08@19\n",
      "14m58s Epoch: 261/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.49% | Val: Loss nan  Acc(top1) 72.13% | HA 95.08@19\n",
      "15m01s Epoch: 262/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 86.79% | Val: Loss nan  Acc(top1) 72.68% | HA 95.08@19\n",
      "15m05s Epoch: 263/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.20% | Val: Loss nan  Acc(top1) 40.98% | HA 95.08@19\n",
      "15m08s Epoch: 264/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 44.81% | HA 95.08@19\n",
      "15m12s Epoch: 265/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 48.09% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc88.52458953857422_tracc88.35227272727273_epoch_265_20240723103233.pt\n",
      "15m15s Epoch: 266/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 88.52% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc86.88524627685547_tracc89.0625_epoch_266_20240723103237.pt\n",
      "15m19s Epoch: 267/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 86.89% | HA 95.08@19\n",
      "15m22s Epoch: 268/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 34.97% | HA 95.08@19\n",
      "15m25s Epoch: 269/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.22% | Val: Loss nan  Acc(top1) 52.46% | HA 95.08@19\n",
      "15m29s Epoch: 270/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc88.52458953857422_tracc89.20454545454545_epoch_270_20240723103251.pt\n",
      "15m32s Epoch: 271/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 88.52% | HA 95.08@19\n",
      "15m36s Epoch: 272/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.34% | Val: Loss nan  Acc(top1) 78.14% | HA 95.08@19\n",
      "15m39s Epoch: 273/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 57.92% | HA 95.08@19\n",
      "15m43s Epoch: 274/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 67.21% | HA 95.08@19\n",
      "15m46s Epoch: 275/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 72.13% | HA 95.08@19\n",
      "15m50s Epoch: 276/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.62% | Val: Loss nan  Acc(top1) 53.55% | HA 95.08@19\n",
      "15m53s Epoch: 277/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.91% | Val: Loss nan  Acc(top1) 73.77% | HA 95.08@19\n",
      "15m57s Epoch: 278/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "16m00s Epoch: 279/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "16m03s Epoch: 280/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 60.66% | HA 95.08@19\n",
      "16m07s Epoch: 281/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.93% | Val: Loss nan  Acc(top1) 54.64% | HA 95.08@19\n",
      "16m10s Epoch: 282/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 58.47% | HA 95.08@19\n",
      "16m14s Epoch: 283/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.05% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "16m17s Epoch: 284/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.20% | Val: Loss nan  Acc(top1) 28.42% | HA 95.08@19\n",
      "16m21s Epoch: 285/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "16m24s Epoch: 286/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.91% | Val: Loss nan  Acc(top1) 43.17% | HA 95.08@19\n",
      "16m28s Epoch: 287/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "16m31s Epoch: 288/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 89.63% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "16m34s Epoch: 289/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.78% | Val: Loss nan  Acc(top1) 77.60% | HA 95.08@19\n",
      "16m38s Epoch: 290/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 73.22% | HA 95.08@19\n",
      "16m41s Epoch: 291/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 51.91% | HA 95.08@19\n",
      "16m45s Epoch: 292/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.62% | Val: Loss nan  Acc(top1) 49.18% | HA 95.08@19\n",
      "16m48s Epoch: 293/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 55.74% | HA 95.08@19\n",
      "16m52s Epoch: 294/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.19% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "16m55s Epoch: 295/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.93% | Val: Loss nan  Acc(top1) 47.54% | HA 95.08@19\n",
      "16m59s Epoch: 296/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.62% | Val: Loss nan  Acc(top1) 47.54% | HA 95.08@19\n",
      "17m02s Epoch: 297/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 81.97% | HA 95.08@19\n",
      "17m06s Epoch: 298/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 62.30% | HA 95.08@19\n",
      "17m09s Epoch: 299/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.77% | Val: Loss nan  Acc(top1) 60.66% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc86.33879089355469_tracc87.07386363636364_epoch_299_20240723103431.pt\n",
      "17m12s Epoch: 300/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.07% | Val: Loss nan  Acc(top1) 86.34% | HA 95.08@19\n",
      "17m16s Epoch: 301/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "17m19s Epoch: 302/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "17m23s Epoch: 303/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 33.88% | HA 95.08@19\n",
      "17m26s Epoch: 304/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.91% | Val: Loss nan  Acc(top1) 61.75% | HA 95.08@19\n",
      "17m30s Epoch: 305/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc88.52458953857422_tracc87.7840909090909_epoch_305_20240723103451.pt\n",
      "17m33s Epoch: 306/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 88.52% | HA 95.08@19\n",
      "17m37s Epoch: 307/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 43.17% | HA 95.08@19\n",
      "17m40s Epoch: 308/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 39.34% | HA 95.08@19\n",
      "17m44s Epoch: 309/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "17m47s Epoch: 310/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.77% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "17m51s Epoch: 311/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 75.41% | HA 95.08@19\n",
      "17m54s Epoch: 312/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "17m57s Epoch: 313/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc87.97814178466797_tracc88.49431818181817_epoch_313_20240723103519.pt\n",
      "18m01s Epoch: 314/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 87.98% | HA 95.08@19\n",
      "18m04s Epoch: 315/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 63.93% | HA 95.08@19\n",
      "18m08s Epoch: 316/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 91.34% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "18m11s Epoch: 317/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.78% | Val: Loss nan  Acc(top1) 44.26% | HA 95.08@19\n",
      "18m15s Epoch: 318/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 54.10% | HA 95.08@19\n",
      "18m18s Epoch: 319/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.19% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "18m22s Epoch: 320/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 86.79% | Val: Loss nan  Acc(top1) 47.54% | HA 95.08@19\n",
      "18m25s Epoch: 321/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 48.09% | HA 95.08@19\n",
      "18m29s Epoch: 322/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.93% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "18m32s Epoch: 323/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 57.38% | HA 95.08@19\n",
      "18m36s Epoch: 324/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "18m39s Epoch: 325/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 90.06% | Val: Loss nan  Acc(top1) 73.77% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc90.71038055419922_tracc89.91477272727273_epoch_325_20240723103601.pt\n",
      "18m43s Epoch: 326/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.91% | Val: Loss nan  Acc(top1) 90.71% | HA 95.08@19\n",
      "18m46s Epoch: 327/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "18m49s Epoch: 328/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.91% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "18m53s Epoch: 329/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.62% | Val: Loss nan  Acc(top1) 37.70% | HA 95.08@19\n",
      "18m56s Epoch: 330/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 48.09% | HA 95.08@19\n",
      "19m00s Epoch: 331/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "19m03s Epoch: 332/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.62% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc89.61748504638672_tracc88.06818181818183_epoch_332_20240723103625.pt\n",
      "19m07s Epoch: 333/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 89.62% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc90.3409090909091_epoch_333_20240723103628.pt\n",
      "19m10s Epoch: 334/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "19m14s Epoch: 335/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 90.91% | Val: Loss nan  Acc(top1) 82.51% | HA 95.08@19\n",
      "19m17s Epoch: 336/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 64.48% | HA 95.08@19\n",
      "19m20s Epoch: 337/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 31.15% | HA 95.08@19\n",
      "19m24s Epoch: 338/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 37.16% | HA 95.08@19\n",
      "19m27s Epoch: 339/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 90.77% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "19m31s Epoch: 340/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "19m34s Epoch: 341/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 34.97% | HA 95.08@19\n",
      "19m38s Epoch: 342/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 69.40% | HA 95.08@19\n",
      "19m41s Epoch: 343/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 74.32% | HA 95.08@19\n",
      "19m44s Epoch: 344/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 45.36% | HA 95.08@19\n",
      "19m48s Epoch: 345/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.64% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "19m51s Epoch: 346/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 70.49% | HA 95.08@19\n",
      "19m55s Epoch: 347/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 90.91% | Val: Loss nan  Acc(top1) 72.13% | HA 95.08@19\n",
      "19m58s Epoch: 348/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 69.40% | HA 95.08@19\n",
      "20m02s Epoch: 349/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 60.66% | HA 95.08@19\n",
      "20m05s Epoch: 350/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 92.33% | Val: Loss nan  Acc(top1) 68.85% | HA 95.08@19\n",
      "20m09s Epoch: 351/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.91% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc87.97814178466797_tracc87.7840909090909_epoch_351_20240723103730.pt\n",
      "20m12s Epoch: 352/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 87.98% | HA 95.08@19\n",
      "20m15s Epoch: 353/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 57.92% | HA 95.08@19\n",
      "20m19s Epoch: 354/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 89.91% | Val: Loss nan  Acc(top1) 54.10% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc90.16393280029297_tracc88.92045454545455_epoch_354_20240723103740.pt\n",
      "20m22s Epoch: 355/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 90.16% | HA 95.08@19\n",
      "20m26s Epoch: 356/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 82.51% | HA 95.08@19\n",
      "20m29s Epoch: 357/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 92.05% | Val: Loss nan  Acc(top1) 72.68% | HA 95.08@19\n",
      "20m32s Epoch: 358/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "20m36s Epoch: 359/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.36% | Val: Loss nan  Acc(top1) 74.32% | HA 95.08@19\n",
      "20m39s Epoch: 360/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.78% | Val: Loss nan  Acc(top1) 49.18% | HA 95.08@19\n",
      "20m43s Epoch: 361/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 64.48% | HA 95.08@19\n",
      "20m46s Epoch: 362/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 30.60% | HA 95.08@19\n",
      "20m50s Epoch: 363/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 43.17% | HA 95.08@19\n",
      "20m53s Epoch: 364/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 62.84% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc94.53551483154297_tracc91.33522727272727_epoch_364_20240723103815.pt\n",
      "20m57s Epoch: 365/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.34% | Val: Loss nan  Acc(top1) 94.54% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc89.48863636363636_epoch_365_20240723103818.pt\n",
      "21m00s Epoch: 366/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "21m04s Epoch: 367/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.91% | Val: Loss nan  Acc(top1) 72.13% | HA 95.08@19\n",
      "21m07s Epoch: 368/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.77% | Val: Loss nan  Acc(top1) 47.54% | HA 95.08@19\n",
      "21m10s Epoch: 369/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.20% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "21m14s Epoch: 370/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.77% | Val: Loss nan  Acc(top1) 34.97% | HA 95.08@19\n",
      "21m17s Epoch: 371/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.76% | Val: Loss nan  Acc(top1) 31.69% | HA 95.08@19\n",
      "21m21s Epoch: 372/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.62% | Val: Loss nan  Acc(top1) 53.01% | HA 95.08@19\n",
      "21m24s Epoch: 373/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.21% | Val: Loss nan  Acc(top1) 56.83% | HA 95.08@19\n",
      "21m28s Epoch: 374/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 60.66% | HA 95.08@19\n",
      "21m31s Epoch: 375/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.77% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "21m35s Epoch: 376/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 39.34% | HA 95.08@19\n",
      "21m38s Epoch: 377/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "21m41s Epoch: 378/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "21m45s Epoch: 379/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "21m48s Epoch: 380/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "21m52s Epoch: 381/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 49.18% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc90.3409090909091_epoch_381_20240723103913.pt\n",
      "21m55s Epoch: 382/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.34% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc89.07103729248047_tracc90.05681818181817_epoch_382_20240723103917.pt\n",
      "21m59s Epoch: 383/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.06% | Val: Loss nan  Acc(top1) 89.07% | HA 95.08@19\n",
      "22m02s Epoch: 384/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "22m06s Epoch: 385/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "22m09s Epoch: 386/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 69.95% | HA 95.08@19\n",
      "22m13s Epoch: 387/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 73.22% | HA 95.08@19\n",
      "22m16s Epoch: 388/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 81.97% | HA 95.08@19\n",
      "22m19s Epoch: 389/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.77% | Val: Loss nan  Acc(top1) 77.05% | HA 95.08@19\n",
      "22m23s Epoch: 390/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "22m26s Epoch: 391/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.49% | Val: Loss nan  Acc(top1) 32.79% | HA 95.08@19\n",
      "22m30s Epoch: 392/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.91% | Val: Loss nan  Acc(top1) 32.24% | HA 95.08@19\n",
      "22m33s Epoch: 393/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 91.34% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "22m37s Epoch: 394/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 74.32% | HA 95.08@19\n",
      "22m40s Epoch: 395/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 88.07% | Val: Loss nan  Acc(top1) 68.31% | HA 95.08@19\n",
      "22m44s Epoch: 396/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 63.93% | HA 95.08@19\n",
      "22m47s Epoch: 397/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "22m50s Epoch: 398/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.93% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "22m54s Epoch: 399/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 56.83% | HA 95.08@19\n",
      "22m57s Epoch: 400/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 68.31% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc89.3465909090909_epoch_400_20240723104019.pt\n",
      "23m01s Epoch: 401/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "23m04s Epoch: 402/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.20% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "23m08s Epoch: 403/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.36% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "23m11s Epoch: 404/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "23m15s Epoch: 405/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 76.50% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc88.7784090909091_epoch_405_20240723104036.pt\n",
      "23m18s Epoch: 406/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "23m22s Epoch: 407/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "23m25s Epoch: 408/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 59.02% | HA 95.08@19\n",
      "23m29s Epoch: 409/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "23m32s Epoch: 410/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.07% | Val: Loss nan  Acc(top1) 45.36% | HA 95.08@19\n",
      "23m35s Epoch: 411/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "23m39s Epoch: 412/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 57.38% | HA 95.08@19\n",
      "23m42s Epoch: 413/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "23m46s Epoch: 414/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "23m49s Epoch: 415/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.35% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "23m53s Epoch: 416/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.36% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "23m56s Epoch: 417/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 56.28% | HA 95.08@19\n",
      "24m00s Epoch: 418/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "24m03s Epoch: 419/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 80.33% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc94.53551483154297_tracc88.06818181818183_epoch_419_20240723104125.pt\n",
      "24m07s Epoch: 420/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 94.54% | HA 95.08@19\n",
      "24m10s Epoch: 421/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.93% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "24m14s Epoch: 422/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.93% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "24m17s Epoch: 423/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.62% | Val: Loss nan  Acc(top1) 34.97% | HA 95.08@19\n",
      "24m20s Epoch: 424/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.20% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "24m24s Epoch: 425/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.77% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "24m27s Epoch: 426/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.49% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "24m31s Epoch: 427/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "24m34s Epoch: 428/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 58.47% | HA 95.08@19\n",
      "24m38s Epoch: 429/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 88.35% | Val: Loss nan  Acc(top1) 34.97% | HA 95.08@19\n",
      "24m41s Epoch: 430/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "24m44s Epoch: 431/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 57.38% | HA 95.08@19\n",
      "24m48s Epoch: 432/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.50% | Val: Loss nan  Acc(top1) 71.04% | HA 95.08@19\n",
      "24m51s Epoch: 433/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 91.19% | Val: Loss nan  Acc(top1) 71.58% | HA 95.08@19\n",
      "24m55s Epoch: 434/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.91% | Val: Loss nan  Acc(top1) 53.01% | HA 95.08@19\n",
      "24m58s Epoch: 435/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.78% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "25m02s Epoch: 436/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 91.05% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "25m05s Epoch: 437/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 28.42% | HA 95.08@19\n",
      "25m09s Epoch: 438/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.20% | Val: Loss nan  Acc(top1) 34.43% | HA 95.08@19\n",
      "25m12s Epoch: 439/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.79% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "25m15s Epoch: 440/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.07% | Val: Loss nan  Acc(top1) 51.91% | HA 95.08@19\n",
      "25m19s Epoch: 441/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 48.09% | HA 95.08@19\n",
      "25m22s Epoch: 442/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "25m26s Epoch: 443/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 58.47% | HA 95.08@19\n",
      "25m29s Epoch: 444/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 57.92% | HA 95.08@19\n",
      "25m33s Epoch: 445/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "25m36s Epoch: 446/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.91% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "25m40s Epoch: 447/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 57.38% | HA 95.08@19\n",
      "25m43s Epoch: 448/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "25m47s Epoch: 449/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 66.67% | HA 95.08@19\n",
      "25m50s Epoch: 450/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.91% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "25m53s Epoch: 451/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 53.55% | HA 95.08@19\n",
      "25m57s Epoch: 452/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.62% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "26m00s Epoch: 453/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 64.48% | HA 95.08@19\n",
      "26m04s Epoch: 454/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 86.79% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "26m07s Epoch: 455/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.07% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "26m11s Epoch: 456/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc89.0625_epoch_456_20240723104332.pt\n",
      "26m14s Epoch: 457/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "26m18s Epoch: 458/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 89.20% | Val: Loss nan  Acc(top1) 83.06% | HA 95.08@19\n",
      "26m21s Epoch: 459/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.06% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "26m24s Epoch: 460/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 89.06% | Val: Loss nan  Acc(top1) 59.02% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc88.52458953857422_tracc89.0625_epoch_460_20240723104346.pt\n",
      "26m28s Epoch: 461/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 88.52% | HA 95.08@19\n",
      "26m31s Epoch: 462/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 73.77% | HA 95.08@19\n",
      "26m35s Epoch: 463/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 89.63% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "26m38s Epoch: 464/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "26m42s Epoch: 465/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.91% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "26m45s Epoch: 466/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.78% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "26m49s Epoch: 467/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.22% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "26m52s Epoch: 468/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 54.64% | HA 95.08@19\n",
      "26m55s Epoch: 469/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.22% | Val: Loss nan  Acc(top1) 53.01% | HA 95.08@19\n",
      "26m59s Epoch: 470/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 73.22% | HA 95.08@19\n",
      "27m02s Epoch: 471/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 60.11% | HA 95.08@19\n",
      "27m06s Epoch: 472/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 37.70% | HA 95.08@19\n",
      "27m09s Epoch: 473/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.49% | Val: Loss nan  Acc(top1) 38.25% | HA 95.08@19\n",
      "27m13s Epoch: 474/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 71.04% | HA 95.08@19\n",
      "27m16s Epoch: 475/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 90.77% | Val: Loss nan  Acc(top1) 37.70% | HA 95.08@19\n",
      "27m19s Epoch: 476/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.07  Acc 89.35% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "27m23s Epoch: 477/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.19% | Val: Loss nan  Acc(top1) 52.46% | HA 95.08@19\n",
      "27m26s Epoch: 478/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.77% | Val: Loss nan  Acc(top1) 62.30% | HA 95.08@19\n",
      "27m30s Epoch: 479/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.36% | Val: Loss nan  Acc(top1) 57.38% | HA 95.08@19\n",
      "27m33s Epoch: 480/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.20% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "27m37s Epoch: 481/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 74.32% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc94.53551483154297_tracc89.63068181818183_epoch_481_20240723104458.pt\n",
      "27m40s Epoch: 482/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 94.54% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc90.76704545454545_epoch_482_20240723104502.pt\n",
      "27m43s Epoch: 483/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.77% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "27m47s Epoch: 484/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 57.92% | HA 95.08@19\n",
      "27m50s Epoch: 485/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 83.61% | HA 95.08@19\n",
      "27m54s Epoch: 486/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.22% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "27m57s Epoch: 487/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 89.63% | Val: Loss nan  Acc(top1) 67.76% | HA 95.08@19\n",
      "28m01s Epoch: 488/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 88.92% | Val: Loss nan  Acc(top1) 81.42% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc89.07103729248047_tracc90.76704545454545_epoch_488_20240723104522.pt\n",
      "28m04s Epoch: 489/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.77% | Val: Loss nan  Acc(top1) 89.07% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc85.24590301513672_tracc88.63636363636364_epoch_489_20240723104526.pt\n",
      "28m08s Epoch: 490/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 85.25% | HA 95.08@19\n",
      "28m11s Epoch: 491/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.48% | Val: Loss nan  Acc(top1) 78.14% | HA 95.08@19\n",
      "28m14s Epoch: 492/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 72.13% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc92.89617156982422_tracc89.0625_epoch_492_20240723104536.pt\n",
      "28m18s Epoch: 493/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 89.06% | Val: Loss nan  Acc(top1) 92.90% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc88.52458953857422_tracc88.63636363636364_epoch_493_20240723104539.pt\n",
      "28m21s Epoch: 494/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 88.52% | HA 95.08@19\n",
      "28m25s Epoch: 495/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 86.65% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "28m28s Epoch: 496/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.77% | Val: Loss nan  Acc(top1) 37.70% | HA 95.08@19\n",
      "28m31s Epoch: 497/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.20% | Val: Loss nan  Acc(top1) 37.70% | HA 95.08@19\n",
      "28m35s Epoch: 498/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 33.88% | HA 95.08@19\n",
      "28m38s Epoch: 499/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "28m42s Epoch: 500/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.93% | Val: Loss nan  Acc(top1) 48.63% | HA 95.08@19\n",
      "28m45s Epoch: 501/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 48.63% | HA 95.08@19\n",
      "28m48s Epoch: 502/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.77% | Val: Loss nan  Acc(top1) 70.49% | HA 95.08@19\n",
      "28m52s Epoch: 503/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "28m55s Epoch: 504/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "28m59s Epoch: 505/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 71.04% | HA 95.08@19\n",
      "29m02s Epoch: 506/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.91% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "29m05s Epoch: 507/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 54.64% | HA 95.08@19\n",
      "29m09s Epoch: 508/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 88.35% | Val: Loss nan  Acc(top1) 84.15% | HA 95.08@19\n",
      "29m12s Epoch: 509/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 90.06% | Val: Loss nan  Acc(top1) 53.55% | HA 95.08@19\n",
      "29m16s Epoch: 510/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 88.78% | Val: Loss nan  Acc(top1) 48.09% | HA 95.08@19\n",
      "29m19s Epoch: 511/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc87.43169403076172_tracc90.3409090909091_epoch_511_20240723104641.pt\n",
      "29m22s Epoch: 512/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 87.43% | HA 95.08@19\n",
      "29m26s Epoch: 513/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 86.51% | Val: Loss nan  Acc(top1) 61.75% | HA 95.08@19\n",
      "29m29s Epoch: 514/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 88.07% | Val: Loss nan  Acc(top1) 51.91% | HA 95.08@19\n",
      "29m33s Epoch: 515/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 55.19% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc89.07103729248047_tracc91.19318181818183_epoch_515_20240723104654.pt\n",
      "29m36s Epoch: 516/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.19% | Val: Loss nan  Acc(top1) 89.07% | HA 95.08@19\n",
      "29m40s Epoch: 517/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.64% | Val: Loss nan  Acc(top1) 51.91% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc86.88524627685547_tracc89.77272727272727_epoch_517_20240723104701.pt\n",
      "29m43s Epoch: 518/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 89.77% | Val: Loss nan  Acc(top1) 86.89% | HA 95.08@19\n",
      "29m46s Epoch: 519/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 88.21% | Val: Loss nan  Acc(top1) 55.74% | HA 95.08@19\n",
      "29m50s Epoch: 520/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 69.95% | HA 95.08@19\n",
      "29m53s Epoch: 521/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 89.20% | Val: Loss nan  Acc(top1) 70.49% | HA 95.08@19\n",
      "29m57s Epoch: 522/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 44.81% | HA 95.08@19\n",
      "30m00s Epoch: 523/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.49% | Val: Loss nan  Acc(top1) 62.30% | HA 95.08@19\n",
      "30m04s Epoch: 524/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.91% | Val: Loss nan  Acc(top1) 35.52% | HA 95.08@19\n",
      "30m07s Epoch: 525/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "30m11s Epoch: 526/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.65% | Val: Loss nan  Acc(top1) 55.19% | HA 95.08@19\n",
      "30m14s Epoch: 527/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "30m17s Epoch: 528/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 65.03% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc87.2159090909091_epoch_528_20240723104739.pt\n",
      "30m21s Epoch: 529/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.22% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "30m24s Epoch: 530/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.62% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "30m28s Epoch: 531/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 66.12% | HA 95.08@19\n",
      "30m31s Epoch: 532/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.77% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "30m35s Epoch: 533/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.35% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc90.16393280029297_tracc87.5_epoch_533_20240723104756.pt\n",
      "30m38s Epoch: 534/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.50% | Val: Loss nan  Acc(top1) 90.16% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc88.52458953857422_tracc86.78977272727273_epoch_534_20240723104800.pt\n",
      "30m42s Epoch: 535/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.79% | Val: Loss nan  Acc(top1) 88.52% | HA 95.08@19\n",
      "30m45s Epoch: 536/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 83.06% | HA 95.08@19\n",
      "30m49s Epoch: 537/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 53.01% | HA 95.08@19\n",
      "30m52s Epoch: 538/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.91% | Val: Loss nan  Acc(top1) 81.97% | HA 95.08@19\n",
      "30m55s Epoch: 539/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 33.88% | HA 95.08@19\n",
      "30m59s Epoch: 540/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.62% | Val: Loss nan  Acc(top1) 39.89% | HA 95.08@19\n",
      "31m02s Epoch: 541/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.62% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "31m06s Epoch: 542/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.93% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "31m09s Epoch: 543/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "31m13s Epoch: 544/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 53.55% | HA 95.08@19\n",
      "31m16s Epoch: 545/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 46.45% | HA 95.08@19\n",
      "31m20s Epoch: 546/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 51.37% | HA 95.08@19\n",
      "31m23s Epoch: 547/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 75.41% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc93.98906707763672_tracc87.92613636363636_epoch_547_20240723104845.pt\n",
      "31m27s Epoch: 548/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.93% | Val: Loss nan  Acc(top1) 93.99% | HA 95.08@19\n",
      "31m30s Epoch: 549/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 80.87% | HA 95.08@19\n",
      "31m34s Epoch: 550/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "31m37s Epoch: 551/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "31m40s Epoch: 552/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.91% | Val: Loss nan  Acc(top1) 63.39% | HA 95.08@19\n",
      "31m44s Epoch: 553/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.35% | Val: Loss nan  Acc(top1) 46.99% | HA 95.08@19\n",
      "31m47s Epoch: 554/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 45.36% | HA 95.08@19\n",
      "31m51s Epoch: 555/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 74.32% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc92.89617156982422_tracc87.07386363636364_epoch_555_20240723104912.pt\n",
      "31m54s Epoch: 556/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.07% | Val: Loss nan  Acc(top1) 92.90% | HA 95.08@19\n",
      "31m58s Epoch: 557/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 88.64% | Val: Loss nan  Acc(top1) 71.58% | HA 95.08@19\n",
      "32m01s Epoch: 558/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.65% | Val: Loss nan  Acc(top1) 72.13% | HA 95.08@19\n",
      "32m05s Epoch: 559/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.07% | Val: Loss nan  Acc(top1) 49.18% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc88.52458953857422_tracc88.63636363636364_epoch_559_20240723104926.pt\n",
      "32m08s Epoch: 560/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 88.52% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc92.34972381591797_tracc87.35795454545455_epoch_560_20240723104930.pt\n",
      "32m12s Epoch: 561/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.36% | Val: Loss nan  Acc(top1) 92.35% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc89.07103729248047_tracc87.2159090909091_epoch_561_20240723104933.pt\n",
      "32m15s Epoch: 562/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.22% | Val: Loss nan  Acc(top1) 89.07% | HA 95.08@19\n",
      "32m19s Epoch: 563/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "32m22s Epoch: 564/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 71.04% | HA 95.08@19\n",
      "32m25s Epoch: 565/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "32m29s Epoch: 566/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "32m32s Epoch: 567/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.36% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "32m36s Epoch: 568/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.64% | Val: Loss nan  Acc(top1) 68.31% | HA 95.08@19\n",
      "32m39s Epoch: 569/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "32m43s Epoch: 570/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 65.57% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc86.33879089355469_tracc89.3465909090909_epoch_570_20240723105004.pt\n",
      "32m46s Epoch: 571/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 86.34% | HA 95.08@19\n",
      "32m50s Epoch: 572/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "32m53s Epoch: 573/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.07% | Val: Loss nan  Acc(top1) 61.75% | HA 95.08@19\n",
      "32m57s Epoch: 574/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.08% | Val: Loss nan  Acc(top1) 49.73% | HA 95.08@19\n",
      "33m00s Epoch: 575/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 29.51% | HA 95.08@19\n",
      "33m03s Epoch: 576/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 31.15% | HA 95.08@19\n",
      "33m07s Epoch: 577/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.65% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "33m10s Epoch: 578/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.34% | Val: Loss nan  Acc(top1) 33.88% | HA 95.08@19\n",
      "33m14s Epoch: 579/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 50.27% | HA 95.08@19\n",
      "33m17s Epoch: 580/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 85.80% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "33m21s Epoch: 581/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.49% | Val: Loss nan  Acc(top1) 45.90% | HA 95.08@19\n",
      "33m24s Epoch: 582/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.49% | Val: Loss nan  Acc(top1) 50.82% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.08197021484375_valacc88.52458953857422_tracc85.79545454545455_epoch_582_20240723105046.pt\n",
      "33m28s Epoch: 583/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 85.80% | Val: Loss nan  Acc(top1) 88.52% | HA 95.08@19\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc95.62841033935547_tracc87.07386363636364_epoch_584_20240723105049.pt\n",
      "33m31s Epoch: 584/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.07% | Val: Loss nan  Acc(top1) 95.63% | HA 95.63@584\n",
      "33m35s Epoch: 585/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 42.62% | HA 95.63@584\n",
      "33m38s Epoch: 586/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 74.32% | HA 95.63@584\n",
      "33m41s Epoch: 587/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.93% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "33m45s Epoch: 588/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.48% | Val: Loss nan  Acc(top1) 77.60% | HA 95.63@584\n",
      "33m48s Epoch: 589/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "33m52s Epoch: 590/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "33m55s Epoch: 591/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 90.20% | Val: Loss nan  Acc(top1) 80.33% | HA 95.63@584\n",
      "33m59s Epoch: 592/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 86.65% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "34m02s Epoch: 593/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.64% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "34m06s Epoch: 594/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 85.37% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "34m09s Epoch: 595/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 40.98% | HA 95.63@584\n",
      "34m13s Epoch: 596/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.20% | Val: Loss nan  Acc(top1) 39.34% | HA 95.63@584\n",
      "34m16s Epoch: 597/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 56.28% | HA 95.63@584\n",
      "34m20s Epoch: 598/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 43.17% | HA 95.63@584\n",
      "34m23s Epoch: 599/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 59.02% | HA 95.63@584\n",
      "34m27s Epoch: 600/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 90.34% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "34m30s Epoch: 601/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.91% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "34m34s Epoch: 602/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 81.42% | HA 95.63@584\n",
      "34m37s Epoch: 603/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "34m41s Epoch: 604/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 59.02% | HA 95.63@584\n",
      "34m44s Epoch: 605/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.49% | Val: Loss nan  Acc(top1) 54.64% | HA 95.63@584\n",
      "34m47s Epoch: 606/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.22% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "34m51s Epoch: 607/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 55.74% | HA 95.63@584\n",
      "34m54s Epoch: 608/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "34m58s Epoch: 609/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 60.11% | HA 95.63@584\n",
      "35m01s Epoch: 610/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "35m05s Epoch: 611/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.93% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "35m08s Epoch: 612/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "35m12s Epoch: 613/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.36% | Val: Loss nan  Acc(top1) 57.92% | HA 95.63@584\n",
      "35m15s Epoch: 614/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 65.57% | HA 95.63@584\n",
      "35m18s Epoch: 615/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 69.95% | HA 95.63@584\n",
      "35m22s Epoch: 616/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.65% | Val: Loss nan  Acc(top1) 51.37% | HA 95.63@584\n",
      "35m25s Epoch: 617/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "35m29s Epoch: 618/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.92% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "35m32s Epoch: 619/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.63% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "35m36s Epoch: 620/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.77% | Val: Loss nan  Acc(top1) 59.56% | HA 95.63@584\n",
      "35m39s Epoch: 621/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.64% | Val: Loss nan  Acc(top1) 54.64% | HA 95.63@584\n",
      "35m43s Epoch: 622/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "35m46s Epoch: 623/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 81.97% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc86.33879089355469_tracc89.3465909090909_epoch_623_20240723105308.pt\n",
      "35m50s Epoch: 624/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 86.34% | HA 95.63@584\n",
      "35m53s Epoch: 625/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 72.13% | HA 95.63@584\n",
      "35m57s Epoch: 626/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 86.93% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "36m00s Epoch: 627/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "36m03s Epoch: 628/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.77% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "36m07s Epoch: 629/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.36% | Val: Loss nan  Acc(top1) 56.83% | HA 95.63@584\n",
      "36m10s Epoch: 630/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 51.37% | HA 95.63@584\n",
      "36m14s Epoch: 631/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.93% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc85.24590301513672_tracc87.7840909090909_epoch_631_20240723105335.pt\n",
      "36m17s Epoch: 632/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 85.25% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.61748504638672_tracc89.20454545454545_epoch_632_20240723105339.pt\n",
      "36m21s Epoch: 633/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.20% | Val: Loss nan  Acc(top1) 89.62% | HA 95.63@584\n",
      "36m24s Epoch: 634/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 55.19% | HA 95.63@584\n",
      "36m28s Epoch: 635/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc93.98906707763672_tracc89.63068181818183_epoch_635_20240723105349.pt\n",
      "36m31s Epoch: 636/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.63% | Val: Loss nan  Acc(top1) 93.99% | HA 95.63@584\n",
      "36m35s Epoch: 637/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.65% | Val: Loss nan  Acc(top1) 59.02% | HA 95.63@584\n",
      "36m38s Epoch: 638/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 51.91% | HA 95.63@584\n",
      "36m41s Epoch: 639/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 33.88% | HA 95.63@584\n",
      "36m45s Epoch: 640/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "36m48s Epoch: 641/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.22% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "36m52s Epoch: 642/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.07% | Val: Loss nan  Acc(top1) 44.26% | HA 95.63@584\n",
      "36m55s Epoch: 643/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.49% | Val: Loss nan  Acc(top1) 69.40% | HA 95.63@584\n",
      "36m59s Epoch: 644/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.20% | Val: Loss nan  Acc(top1) 67.21% | HA 95.63@584\n",
      "37m02s Epoch: 645/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 71.04% | HA 95.63@584\n",
      "37m06s Epoch: 646/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.61748504638672_tracc90.05681818181817_epoch_646_20240723105427.pt\n",
      "37m09s Epoch: 647/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 89.62% | HA 95.63@584\n",
      "37m13s Epoch: 648/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "37m16s Epoch: 649/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.65% | Val: Loss nan  Acc(top1) 47.54% | HA 95.63@584\n",
      "37m19s Epoch: 650/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 55.19% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.07103729248047_tracc87.35795454545455_epoch_650_20240723105441.pt\n",
      "37m23s Epoch: 651/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.36% | Val: Loss nan  Acc(top1) 89.07% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc93.44261932373047_tracc88.21022727272727_epoch_651_20240723105444.pt\n",
      "37m26s Epoch: 652/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 93.44% | HA 95.63@584\n",
      "37m30s Epoch: 653/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 57.38% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc85.79234313964844_tracc88.49431818181817_epoch_653_20240723105451.pt\n",
      "37m33s Epoch: 654/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 85.79% | HA 95.63@584\n",
      "37m37s Epoch: 655/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.91% | Val: Loss nan  Acc(top1) 38.80% | HA 95.63@584\n",
      "37m40s Epoch: 656/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.07% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "37m43s Epoch: 657/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "37m47s Epoch: 658/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 61.20% | HA 95.63@584\n",
      "37m50s Epoch: 659/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "37m54s Epoch: 660/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.07% | Val: Loss nan  Acc(top1) 63.93% | HA 95.63@584\n",
      "37m57s Epoch: 661/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.07% | Val: Loss nan  Acc(top1) 56.28% | HA 95.63@584\n",
      "38m01s Epoch: 662/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.64% | Val: Loss nan  Acc(top1) 51.37% | HA 95.63@584\n",
      "38m04s Epoch: 663/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 44.81% | HA 95.63@584\n",
      "38m08s Epoch: 664/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 80.87% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc86.88524627685547_tracc91.33522727272727_epoch_664_20240723105529.pt\n",
      "38m11s Epoch: 665/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 91.34% | Val: Loss nan  Acc(top1) 86.89% | HA 95.63@584\n",
      "38m14s Epoch: 666/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 91.48% | Val: Loss nan  Acc(top1) 56.28% | HA 95.63@584\n",
      "38m18s Epoch: 667/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.49% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "38m21s Epoch: 668/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 65.57% | HA 95.63@584\n",
      "38m25s Epoch: 669/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.06% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "38m28s Epoch: 670/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 70.49% | HA 95.63@584\n",
      "38m32s Epoch: 671/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.35% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "38m35s Epoch: 672/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "38m38s Epoch: 673/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.20% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "38m42s Epoch: 674/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "38m45s Epoch: 675/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 36.07% | HA 95.63@584\n",
      "38m49s Epoch: 676/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 34.97% | HA 95.63@584\n",
      "38m52s Epoch: 677/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 33.33% | HA 95.63@584\n",
      "38m55s Epoch: 678/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.91% | Val: Loss nan  Acc(top1) 41.53% | HA 95.63@584\n",
      "38m59s Epoch: 679/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.91% | Val: Loss nan  Acc(top1) 42.08% | HA 95.63@584\n",
      "39m02s Epoch: 680/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 55.19% | HA 95.63@584\n",
      "39m06s Epoch: 681/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.49% | Val: Loss nan  Acc(top1) 66.12% | HA 95.63@584\n",
      "39m09s Epoch: 682/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 31.15% | HA 95.63@584\n",
      "39m13s Epoch: 683/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 28.42% | HA 95.63@584\n",
      "39m16s Epoch: 684/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 34.43% | HA 95.63@584\n",
      "39m19s Epoch: 685/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.48% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "39m23s Epoch: 686/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 27.87% | HA 95.63@584\n",
      "39m26s Epoch: 687/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 34.97% | HA 95.63@584\n",
      "39m30s Epoch: 688/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.07% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "39m33s Epoch: 689/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.49% | Val: Loss nan  Acc(top1) 43.72% | HA 95.63@584\n",
      "39m36s Epoch: 690/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "39m40s Epoch: 691/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 37.70% | HA 95.63@584\n",
      "39m43s Epoch: 692/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 35.52% | HA 95.63@584\n",
      "39m47s Epoch: 693/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "39m50s Epoch: 694/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.49% | Val: Loss nan  Acc(top1) 83.61% | HA 95.63@584\n",
      "39m54s Epoch: 695/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "39m57s Epoch: 696/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 47.54% | HA 95.63@584\n",
      "40m00s Epoch: 697/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 63.93% | HA 95.63@584\n",
      "40m04s Epoch: 698/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 56.83% | HA 95.63@584\n",
      "40m07s Epoch: 699/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 87.78% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "40m11s Epoch: 700/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 29.51% | HA 95.63@584\n",
      "40m14s Epoch: 701/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.77% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "40m18s Epoch: 702/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 49.18% | HA 95.63@584\n",
      "40m21s Epoch: 703/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "40m24s Epoch: 704/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.91% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "40m28s Epoch: 705/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.35% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "40m31s Epoch: 706/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.49% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "40m35s Epoch: 707/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 53.55% | HA 95.63@584\n",
      "40m38s Epoch: 708/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.36% | Val: Loss nan  Acc(top1) 36.07% | HA 95.63@584\n",
      "40m42s Epoch: 709/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 90.77% | Val: Loss nan  Acc(top1) 44.26% | HA 95.63@584\n",
      "40m45s Epoch: 710/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "40m48s Epoch: 711/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "40m52s Epoch: 712/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.35% | Val: Loss nan  Acc(top1) 56.28% | HA 95.63@584\n",
      "40m55s Epoch: 713/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 72.13% | HA 95.63@584\n",
      "40m59s Epoch: 714/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.20% | Val: Loss nan  Acc(top1) 57.92% | HA 95.63@584\n",
      "41m02s Epoch: 715/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "41m06s Epoch: 716/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.35% | Val: Loss nan  Acc(top1) 34.97% | HA 95.63@584\n",
      "41m09s Epoch: 717/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.49% | Val: Loss nan  Acc(top1) 36.61% | HA 95.63@584\n",
      "41m13s Epoch: 718/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 91.62% | Val: Loss nan  Acc(top1) 45.36% | HA 95.63@584\n",
      "41m16s Epoch: 719/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.34% | Val: Loss nan  Acc(top1) 48.09% | HA 95.63@584\n",
      "41m19s Epoch: 720/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.49% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "41m23s Epoch: 721/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.36% | Val: Loss nan  Acc(top1) 79.23% | HA 95.63@584\n",
      "41m26s Epoch: 722/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.78% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "41m30s Epoch: 723/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 66.12% | HA 95.63@584\n",
      "41m33s Epoch: 724/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc90.16393280029297_tracc89.20454545454545_epoch_724_20240723105855.pt\n",
      "41m37s Epoch: 725/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 90.16% | HA 95.63@584\n",
      "41m40s Epoch: 726/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 82.51% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc85.24590301513672_tracc90.05681818181817_epoch_726_20240723105902.pt\n",
      "41m43s Epoch: 727/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 90.06% | Val: Loss nan  Acc(top1) 85.25% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc90.16393280029297_tracc87.35795454545455_epoch_727_20240723105905.pt\n",
      "41m47s Epoch: 728/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 87.36% | Val: Loss nan  Acc(top1) 90.16% | HA 95.63@584\n",
      "41m50s Epoch: 729/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.77% | Val: Loss nan  Acc(top1) 80.33% | HA 95.63@584\n",
      "41m54s Epoch: 730/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "41m57s Epoch: 731/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 79.78% | HA 95.63@584\n",
      "42m01s Epoch: 732/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 45.36% | HA 95.63@584\n",
      "42m04s Epoch: 733/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 37.70% | HA 95.63@584\n",
      "42m07s Epoch: 734/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "42m11s Epoch: 735/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.49% | Val: Loss nan  Acc(top1) 30.05% | HA 95.63@584\n",
      "42m14s Epoch: 736/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 37.70% | HA 95.63@584\n",
      "42m18s Epoch: 737/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.35% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "42m21s Epoch: 738/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.36% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "42m24s Epoch: 739/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "42m28s Epoch: 740/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc87.97814178466797_tracc86.93181818181817_epoch_740_20240723105949.pt\n",
      "42m31s Epoch: 741/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.93% | Val: Loss nan  Acc(top1) 87.98% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc87.97814178466797_tracc90.625_epoch_741_20240723105953.pt\n",
      "42m35s Epoch: 742/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.62% | Val: Loss nan  Acc(top1) 87.98% | HA 95.63@584\n",
      "42m38s Epoch: 743/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 61.20% | HA 95.63@584\n",
      "42m42s Epoch: 744/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 38.25% | HA 95.63@584\n",
      "42m45s Epoch: 745/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.22% | Val: Loss nan  Acc(top1) 38.80% | HA 95.63@584\n",
      "42m48s Epoch: 746/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.07% | Val: Loss nan  Acc(top1) 65.57% | HA 95.63@584\n",
      "42m52s Epoch: 747/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 87.64% | Val: Loss nan  Acc(top1) 80.87% | HA 95.63@584\n",
      "42m55s Epoch: 748/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "42m59s Epoch: 749/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "43m02s Epoch: 750/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 74.86% | HA 95.63@584\n",
      "43m06s Epoch: 751/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 34.97% | HA 95.63@584\n",
      "43m09s Epoch: 752/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.06% | Val: Loss nan  Acc(top1) 54.10% | HA 95.63@584\n",
      "43m12s Epoch: 753/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 42.62% | HA 95.63@584\n",
      "43m16s Epoch: 754/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "43m19s Epoch: 755/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.35% | Val: Loss nan  Acc(top1) 48.09% | HA 95.63@584\n",
      "43m23s Epoch: 756/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "43m26s Epoch: 757/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 59.56% | HA 95.63@584\n",
      "43m30s Epoch: 758/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.78% | Val: Loss nan  Acc(top1) 74.32% | HA 95.63@584\n",
      "43m33s Epoch: 759/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.91% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "43m36s Epoch: 760/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "43m40s Epoch: 761/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "43m43s Epoch: 762/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.62% | Val: Loss nan  Acc(top1) 61.20% | HA 95.63@584\n",
      "43m47s Epoch: 763/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 54.10% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc87.97814178466797_tracc86.36363636363636_epoch_763_20240723110108.pt\n",
      "43m50s Epoch: 764/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.36% | Val: Loss nan  Acc(top1) 87.98% | HA 95.63@584\n",
      "43m53s Epoch: 765/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "43m57s Epoch: 766/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 80.33% | HA 95.63@584\n",
      "44m00s Epoch: 767/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 51.37% | HA 95.63@584\n",
      "44m04s Epoch: 768/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.91% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "44m07s Epoch: 769/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "44m11s Epoch: 770/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.07% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "44m14s Epoch: 771/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc93.44261932373047_tracc89.0625_epoch_771_20240723110136.pt\n",
      "44m17s Epoch: 772/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 93.44% | HA 95.63@584\n",
      "44m21s Epoch: 773/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 91.34% | Val: Loss nan  Acc(top1) 78.69% | HA 95.63@584\n",
      "44m24s Epoch: 774/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "44m28s Epoch: 775/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 62.30% | HA 95.63@584\n",
      "44m31s Epoch: 776/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 45.36% | HA 95.63@584\n",
      "44m34s Epoch: 777/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "44m38s Epoch: 778/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "44m41s Epoch: 779/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "44m45s Epoch: 780/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.07% | Val: Loss nan  Acc(top1) 57.92% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc92.89617156982422_tracc89.20454545454545_epoch_780_20240723110206.pt\n",
      "44m48s Epoch: 781/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 92.90% | HA 95.63@584\n",
      "44m52s Epoch: 782/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.77% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "44m55s Epoch: 783/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.05% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "44m58s Epoch: 784/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 78.14% | HA 95.63@584\n",
      "45m02s Epoch: 785/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "45m05s Epoch: 786/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 54.64% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc88.52458953857422_tracc90.19886363636364_epoch_786_20240723110227.pt\n",
      "45m09s Epoch: 787/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 90.20% | Val: Loss nan  Acc(top1) 88.52% | HA 95.63@584\n",
      "45m12s Epoch: 788/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 62.30% | HA 95.63@584\n",
      "45m16s Epoch: 789/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 87.07% | Val: Loss nan  Acc(top1) 54.64% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc87.43169403076172_tracc88.63636363636364_epoch_789_20240723110237.pt\n",
      "45m19s Epoch: 790/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 87.43% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc94.53551483154297_tracc86.2215909090909_epoch_790_20240723110241.pt\n",
      "45m22s Epoch: 791/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 86.22% | Val: Loss nan  Acc(top1) 94.54% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc93.98906707763672_tracc89.63068181818183_epoch_791_20240723110244.pt\n",
      "45m26s Epoch: 792/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 93.99% | HA 95.63@584\n",
      "45m29s Epoch: 793/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 31.15% | HA 95.63@584\n",
      "45m33s Epoch: 794/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "45m36s Epoch: 795/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.64% | Val: Loss nan  Acc(top1) 80.87% | HA 95.63@584\n",
      "45m40s Epoch: 796/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.20% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "45m43s Epoch: 797/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.51% | Val: Loss nan  Acc(top1) 72.13% | HA 95.63@584\n",
      "45m46s Epoch: 798/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 73.77% | HA 95.63@584\n",
      "45m50s Epoch: 799/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 81.42% | HA 95.63@584\n",
      "45m53s Epoch: 800/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.06% | Val: Loss nan  Acc(top1) 82.51% | HA 95.63@584\n",
      "45m57s Epoch: 801/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.63% | Val: Loss nan  Acc(top1) 75.96% | HA 95.63@584\n",
      "46m00s Epoch: 802/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 74.32% | HA 95.63@584\n",
      "46m03s Epoch: 803/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "46m07s Epoch: 804/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 31.15% | HA 95.63@584\n",
      "46m10s Epoch: 805/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "46m14s Epoch: 806/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 51.91% | HA 95.63@584\n",
      "46m17s Epoch: 807/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "46m20s Epoch: 808/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.63% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "46m24s Epoch: 809/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.91% | Val: Loss nan  Acc(top1) 40.98% | HA 95.63@584\n",
      "46m27s Epoch: 810/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.64% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "46m30s Epoch: 811/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 65.03% | HA 95.63@584\n",
      "46m34s Epoch: 812/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.22% | Val: Loss nan  Acc(top1) 53.01% | HA 95.63@584\n",
      "46m37s Epoch: 813/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.21% | Val: Loss nan  Acc(top1) 75.41% | HA 95.63@584\n",
      "46m41s Epoch: 814/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "46m44s Epoch: 815/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 55.19% | HA 95.63@584\n",
      "46m48s Epoch: 816/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.36% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "46m51s Epoch: 817/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 70.49% | HA 95.63@584\n",
      "46m54s Epoch: 818/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 56.83% | HA 95.63@584\n",
      "46m58s Epoch: 819/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.20% | Val: Loss nan  Acc(top1) 47.54% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc88.52458953857422_tracc90.3409090909091_epoch_819_20240723110419.pt\n",
      "47m01s Epoch: 820/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 88.52% | HA 95.63@584\n",
      "47m05s Epoch: 821/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "47m08s Epoch: 822/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 44.26% | HA 95.63@584\n",
      "47m11s Epoch: 823/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 51.37% | HA 95.63@584\n",
      "47m15s Epoch: 824/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.36% | Val: Loss nan  Acc(top1) 59.02% | HA 95.63@584\n",
      "47m18s Epoch: 825/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.51% | Val: Loss nan  Acc(top1) 83.06% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc85.24590301513672_tracc89.63068181818183_epoch_825_20240723110440.pt\n",
      "47m22s Epoch: 826/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.63% | Val: Loss nan  Acc(top1) 85.25% | HA 95.63@584\n",
      "47m25s Epoch: 827/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.92% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "47m29s Epoch: 828/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.63% | Val: Loss nan  Acc(top1) 65.03% | HA 95.63@584\n",
      "47m32s Epoch: 829/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.64% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "47m35s Epoch: 830/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 45.36% | HA 95.63@584\n",
      "47m39s Epoch: 831/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 37.16% | HA 95.63@584\n",
      "47m42s Epoch: 832/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 88.21% | Val: Loss nan  Acc(top1) 56.28% | HA 95.63@584\n",
      "47m46s Epoch: 833/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.35% | Val: Loss nan  Acc(top1) 34.97% | HA 95.63@584\n",
      "47m49s Epoch: 834/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.35% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "47m52s Epoch: 835/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 48.09% | HA 95.63@584\n",
      "47m56s Epoch: 836/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "47m59s Epoch: 837/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 67.21% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.61748504638672_tracc89.3465909090909_epoch_837_20240723110521.pt\n",
      "48m03s Epoch: 838/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 89.62% | HA 95.63@584\n",
      "48m06s Epoch: 839/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 82.51% | HA 95.63@584\n",
      "48m10s Epoch: 840/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.49% | Val: Loss nan  Acc(top1) 56.28% | HA 95.63@584\n",
      "48m13s Epoch: 841/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 85.94% | Val: Loss nan  Acc(top1) 56.28% | HA 95.63@584\n",
      "48m16s Epoch: 842/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.07% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "48m20s Epoch: 843/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 69.95% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc91.25682830810547_tracc88.35227272727273_epoch_843_20240723110541.pt\n",
      "48m23s Epoch: 844/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.35% | Val: Loss nan  Acc(top1) 91.26% | HA 95.63@584\n",
      "48m27s Epoch: 845/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.36% | Val: Loss nan  Acc(top1) 47.54% | HA 95.63@584\n",
      "48m30s Epoch: 846/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 91.76% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "48m33s Epoch: 847/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 79.23% | HA 95.63@584\n",
      "48m37s Epoch: 848/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.77% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "48m40s Epoch: 849/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 55.19% | HA 95.63@584\n",
      "48m44s Epoch: 850/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "48m47s Epoch: 851/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 54.10% | HA 95.63@584\n",
      "48m51s Epoch: 852/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 39.34% | HA 95.63@584\n",
      "48m54s Epoch: 853/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.36% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "48m57s Epoch: 854/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "49m01s Epoch: 855/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "49m04s Epoch: 856/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 49.18% | HA 95.63@584\n",
      "49m08s Epoch: 857/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 61.20% | HA 95.63@584\n",
      "49m11s Epoch: 858/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 65.03% | HA 95.63@584\n",
      "49m15s Epoch: 859/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 86.65% | Val: Loss nan  Acc(top1) 74.86% | HA 95.63@584\n",
      "49m18s Epoch: 860/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 83.61% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.61748504638672_tracc90.625_epoch_860_20240723110640.pt\n",
      "49m21s Epoch: 861/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.62% | Val: Loss nan  Acc(top1) 89.62% | HA 95.63@584\n",
      "49m25s Epoch: 862/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 42.62% | HA 95.63@584\n",
      "49m28s Epoch: 863/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "49m32s Epoch: 864/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 57.92% | HA 95.63@584\n",
      "49m35s Epoch: 865/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 81.42% | HA 95.63@584\n",
      "49m39s Epoch: 866/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "49m42s Epoch: 867/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.49% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "49m46s Epoch: 868/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 55.19% | HA 95.63@584\n",
      "49m49s Epoch: 869/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.93% | Val: Loss nan  Acc(top1) 60.66% | HA 95.63@584\n",
      "49m52s Epoch: 870/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 87.93% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "49m56s Epoch: 871/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 86.79% | Val: Loss nan  Acc(top1) 59.56% | HA 95.63@584\n",
      "49m59s Epoch: 872/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 62.30% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.07103729248047_tracc88.92045454545455_epoch_872_20240723110721.pt\n",
      "50m03s Epoch: 873/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 89.07% | HA 95.63@584\n",
      "50m06s Epoch: 874/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 49.18% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc95.08197021484375_tracc89.3465909090909_epoch_874_20240723110728.pt\n",
      "50m10s Epoch: 875/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 95.08% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc92.34972381591797_tracc87.35795454545455_epoch_875_20240723110731.pt\n",
      "50m13s Epoch: 876/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.36% | Val: Loss nan  Acc(top1) 92.35% | HA 95.63@584\n",
      "50m16s Epoch: 877/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 63.39% | HA 95.63@584\n",
      "50m20s Epoch: 878/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.06% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "50m23s Epoch: 879/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 56.28% | HA 95.63@584\n",
      "50m27s Epoch: 880/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 90.20% | Val: Loss nan  Acc(top1) 72.68% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc93.98906707763672_tracc87.64204545454545_epoch_880_20240723110748.pt\n",
      "50m30s Epoch: 881/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.64% | Val: Loss nan  Acc(top1) 93.99% | HA 95.63@584\n",
      "50m34s Epoch: 882/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.06% | Val: Loss nan  Acc(top1) 71.58% | HA 95.63@584\n",
      "50m37s Epoch: 883/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 83.06% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc87.97814178466797_tracc88.63636363636364_epoch_883_20240723110759.pt\n",
      "50m41s Epoch: 884/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 87.98% | HA 95.63@584\n",
      "50m44s Epoch: 885/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "50m47s Epoch: 886/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 59.02% | HA 95.63@584\n",
      "50m51s Epoch: 887/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 86.65% | Val: Loss nan  Acc(top1) 78.69% | HA 95.63@584\n",
      "50m54s Epoch: 888/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 68.31% | HA 95.63@584\n",
      "50m58s Epoch: 889/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 88.35% | Val: Loss nan  Acc(top1) 55.19% | HA 95.63@584\n",
      "51m01s Epoch: 890/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.91% | Val: Loss nan  Acc(top1) 54.64% | HA 95.63@584\n",
      "51m04s Epoch: 891/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.51% | Val: Loss nan  Acc(top1) 56.83% | HA 95.63@584\n",
      "51m08s Epoch: 892/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 71.04% | HA 95.63@584\n",
      "51m11s Epoch: 893/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 53.01% | HA 95.63@584\n",
      "51m15s Epoch: 894/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.62% | Val: Loss nan  Acc(top1) 65.57% | HA 95.63@584\n",
      "51m18s Epoch: 895/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "51m22s Epoch: 896/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 40.44% | HA 95.63@584\n",
      "51m25s Epoch: 897/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.20% | Val: Loss nan  Acc(top1) 54.10% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc88.52458953857422_tracc89.48863636363636_epoch_897_20240723110847.pt\n",
      "51m29s Epoch: 898/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 88.52% | HA 95.63@584\n",
      "51m32s Epoch: 899/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "51m35s Epoch: 900/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.91% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "51m39s Epoch: 901/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "51m42s Epoch: 902/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "51m46s Epoch: 903/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 90.06% | Val: Loss nan  Acc(top1) 64.48% | HA 95.63@584\n",
      "51m49s Epoch: 904/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.48% | Val: Loss nan  Acc(top1) 56.28% | HA 95.63@584\n",
      "51m53s Epoch: 905/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 81.97% | HA 95.63@584\n",
      "51m56s Epoch: 906/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 82.51% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc93.98906707763672_tracc89.48863636363636_epoch_906_20240723110917.pt\n",
      "51m59s Epoch: 907/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 93.99% | HA 95.63@584\n",
      "52m03s Epoch: 908/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 55.19% | HA 95.63@584\n",
      "52m06s Epoch: 909/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.77% | Val: Loss nan  Acc(top1) 31.15% | HA 95.63@584\n",
      "52m10s Epoch: 910/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 31.69% | HA 95.63@584\n",
      "52m13s Epoch: 911/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.93% | Val: Loss nan  Acc(top1) 48.09% | HA 95.63@584\n",
      "52m17s Epoch: 912/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.06% | Val: Loss nan  Acc(top1) 56.83% | HA 95.63@584\n",
      "52m20s Epoch: 913/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.22% | Val: Loss nan  Acc(top1) 61.20% | HA 95.63@584\n",
      "52m23s Epoch: 914/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.06% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "52m27s Epoch: 915/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.34% | Val: Loss nan  Acc(top1) 48.09% | HA 95.63@584\n",
      "52m30s Epoch: 916/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 31.15% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc88.52458953857422_tracc90.9090909090909_epoch_916_20240723110952.pt\n",
      "52m34s Epoch: 917/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.91% | Val: Loss nan  Acc(top1) 88.52% | HA 95.63@584\n",
      "52m37s Epoch: 918/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.48% | Val: Loss nan  Acc(top1) 61.20% | HA 95.63@584\n",
      "52m40s Epoch: 919/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 85.23% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "52m44s Epoch: 920/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 89.49% | Val: Loss nan  Acc(top1) 60.11% | HA 95.63@584\n",
      "52m47s Epoch: 921/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "52m51s Epoch: 922/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.20% | Val: Loss nan  Acc(top1) 63.93% | HA 95.63@584\n",
      "52m54s Epoch: 923/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "52m58s Epoch: 924/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 38.25% | HA 95.63@584\n",
      "53m01s Epoch: 925/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.07% | Val: Loss nan  Acc(top1) 28.96% | HA 95.63@584\n",
      "53m04s Epoch: 926/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.77% | Val: Loss nan  Acc(top1) 45.36% | HA 95.63@584\n",
      "53m08s Epoch: 927/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.77% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "53m11s Epoch: 928/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 57.92% | HA 95.63@584\n",
      "53m15s Epoch: 929/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "53m18s Epoch: 930/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 48.09% | HA 95.63@584\n",
      "53m22s Epoch: 931/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 86.93% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "53m25s Epoch: 932/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.91% | Val: Loss nan  Acc(top1) 28.42% | HA 95.63@584\n",
      "53m28s Epoch: 933/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.64% | Val: Loss nan  Acc(top1) 48.63% | HA 95.63@584\n",
      "53m32s Epoch: 934/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.20% | Val: Loss nan  Acc(top1) 51.37% | HA 95.63@584\n",
      "53m35s Epoch: 935/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 45.36% | HA 95.63@584\n",
      "53m39s Epoch: 936/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "53m42s Epoch: 937/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "53m46s Epoch: 938/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 31.69% | HA 95.63@584\n",
      "53m49s Epoch: 939/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.22% | Val: Loss nan  Acc(top1) 31.69% | HA 95.63@584\n",
      "53m52s Epoch: 940/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 45.36% | HA 95.63@584\n",
      "53m56s Epoch: 941/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.91% | Val: Loss nan  Acc(top1) 37.70% | HA 95.63@584\n",
      "53m59s Epoch: 942/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "54m03s Epoch: 943/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "54m06s Epoch: 944/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "54m09s Epoch: 945/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 59.56% | HA 95.63@584\n",
      "54m13s Epoch: 946/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.20% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "54m16s Epoch: 947/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.62% | Val: Loss nan  Acc(top1) 73.77% | HA 95.63@584\n",
      "54m20s Epoch: 948/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 58.47% | HA 95.63@584\n",
      "54m23s Epoch: 949/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.78% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "54m26s Epoch: 950/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 87.36% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "54m30s Epoch: 951/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.49% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "54m33s Epoch: 952/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 51.91% | HA 95.63@584\n",
      "54m37s Epoch: 953/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.78% | Val: Loss nan  Acc(top1) 78.14% | HA 95.63@584\n",
      "54m40s Epoch: 954/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 70.49% | HA 95.63@584\n",
      "54m44s Epoch: 955/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 80.87% | HA 95.63@584\n",
      "54m47s Epoch: 956/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 63.93% | HA 95.63@584\n",
      "54m50s Epoch: 957/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.79% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "54m54s Epoch: 958/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.34% | Val: Loss nan  Acc(top1) 78.69% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.07103729248047_tracc88.92045454545455_epoch_958_20240723111215.pt\n",
      "54m57s Epoch: 959/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 89.07% | HA 95.63@584\n",
      "55m01s Epoch: 960/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 45.36% | HA 95.63@584\n",
      "55m04s Epoch: 961/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.63% | Val: Loss nan  Acc(top1) 48.63% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc88.52458953857422_tracc88.06818181818183_epoch_961_20240723111226.pt\n",
      "55m08s Epoch: 962/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.07% | Val: Loss nan  Acc(top1) 88.52% | HA 95.63@584\n",
      "55m11s Epoch: 963/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "55m14s Epoch: 964/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 42.62% | HA 95.63@584\n",
      "55m18s Epoch: 965/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 65.03% | HA 95.63@584\n",
      "55m21s Epoch: 966/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "55m25s Epoch: 967/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.35% | Val: Loss nan  Acc(top1) 53.01% | HA 95.63@584\n",
      "55m28s Epoch: 968/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.35% | Val: Loss nan  Acc(top1) 37.70% | HA 95.63@584\n",
      "55m32s Epoch: 969/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "55m35s Epoch: 970/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.35% | Val: Loss nan  Acc(top1) 47.54% | HA 95.63@584\n",
      "55m38s Epoch: 971/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "55m42s Epoch: 972/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.34% | Val: Loss nan  Acc(top1) 61.75% | HA 95.63@584\n",
      "55m45s Epoch: 973/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 56.83% | HA 95.63@584\n",
      "55m49s Epoch: 974/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 49.18% | HA 95.63@584\n",
      "55m52s Epoch: 975/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.63% | Val: Loss nan  Acc(top1) 82.51% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc93.98906707763672_tracc88.35227272727273_epoch_975_20240723111314.pt\n",
      "55m55s Epoch: 976/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 93.99% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc90.16393280029297_tracc89.63068181818183_epoch_976_20240723111317.pt\n",
      "55m59s Epoch: 977/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 90.16% | HA 95.63@584\n",
      "56m02s Epoch: 978/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.77% | Val: Loss nan  Acc(top1) 51.37% | HA 95.63@584\n",
      "56m06s Epoch: 979/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 90.06% | Val: Loss nan  Acc(top1) 47.54% | HA 95.63@584\n",
      "56m09s Epoch: 980/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.07103729248047_tracc89.20454545454545_epoch_980_20240723111331.pt\n",
      "56m13s Epoch: 981/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 89.07% | HA 95.63@584\n",
      "56m16s Epoch: 982/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 42.62% | HA 95.63@584\n",
      "56m20s Epoch: 983/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.22% | Val: Loss nan  Acc(top1) 78.69% | HA 95.63@584\n",
      "56m23s Epoch: 984/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 61.20% | HA 95.63@584\n",
      "56m26s Epoch: 985/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 35.52% | HA 95.63@584\n",
      "56m30s Epoch: 986/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.35% | Val: Loss nan  Acc(top1) 34.97% | HA 95.63@584\n",
      "56m33s Epoch: 987/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "56m37s Epoch: 988/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "56m40s Epoch: 989/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 39.34% | HA 95.63@584\n",
      "56m43s Epoch: 990/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc87.97814178466797_tracc89.3465909090909_epoch_990_20240723111405.pt\n",
      "56m47s Epoch: 991/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 87.98% | HA 95.63@584\n",
      "56m50s Epoch: 992/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 91.19% | Val: Loss nan  Acc(top1) 71.58% | HA 95.63@584\n",
      "56m54s Epoch: 993/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "56m57s Epoch: 994/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.22% | Val: Loss nan  Acc(top1) 83.06% | HA 95.63@584\n",
      "57m01s Epoch: 995/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "57m04s Epoch: 996/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.20% | Val: Loss nan  Acc(top1) 83.06% | HA 95.63@584\n",
      "57m07s Epoch: 997/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.34% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "57m11s Epoch: 998/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.07% | Val: Loss nan  Acc(top1) 63.93% | HA 95.63@584\n",
      "57m14s Epoch: 999/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 44.26% | HA 95.63@584\n",
      "57m18s Epoch: 1000/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 51.91% | HA 95.63@584\n",
      "57m21s Epoch: 1001/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.20% | Val: Loss nan  Acc(top1) 63.39% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.61748504638672_tracc89.77272727272727_epoch_1001_20240723111443.pt\n",
      "57m25s Epoch: 1002/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 89.62% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc93.44261932373047_tracc90.48295454545455_epoch_1002_20240723111446.pt\n",
      "57m28s Epoch: 1003/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 90.48% | Val: Loss nan  Acc(top1) 93.44% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc94.53551483154297_tracc89.63068181818183_epoch_1003_20240723111450.pt\n",
      "57m32s Epoch: 1004/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 94.54% | HA 95.63@584\n",
      "57m35s Epoch: 1005/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.35% | Val: Loss nan  Acc(top1) 81.97% | HA 95.63@584\n",
      "57m38s Epoch: 1006/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 42.62% | HA 95.63@584\n",
      "57m42s Epoch: 1007/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.48% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "57m45s Epoch: 1008/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.08  Acc 88.21% | Val: Loss nan  Acc(top1) 81.97% | HA 95.63@584\n",
      "57m49s Epoch: 1009/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 48.63% | HA 95.63@584\n",
      "57m52s Epoch: 1010/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.77% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "57m55s Epoch: 1011/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.21% | Val: Loss nan  Acc(top1) 47.54% | HA 95.63@584\n",
      "57m59s Epoch: 1012/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 48.09% | HA 95.63@584\n",
      "58m02s Epoch: 1013/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "58m06s Epoch: 1014/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.77% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "58m09s Epoch: 1015/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "58m13s Epoch: 1016/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "58m16s Epoch: 1017/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "58m19s Epoch: 1018/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "58m23s Epoch: 1019/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "58m26s Epoch: 1020/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.93% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "58m30s Epoch: 1021/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 73.22% | HA 95.63@584\n",
      "58m33s Epoch: 1022/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 59.02% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc94.53551483154297_tracc88.7784090909091_epoch_1022_20240723111555.pt\n",
      "58m37s Epoch: 1023/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.78% | Val: Loss nan  Acc(top1) 94.54% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc88.52458953857422_tracc88.92045454545455_epoch_1023_20240723111558.pt\n",
      "58m40s Epoch: 1024/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 88.52% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc95.08197021484375_tracc90.3409090909091_epoch_1024_20240723111602.pt\n",
      "58m43s Epoch: 1025/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.34% | Val: Loss nan  Acc(top1) 95.08% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc88.52458953857422_tracc86.50568181818183_epoch_1025_20240723111605.pt\n",
      "58m47s Epoch: 1026/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.51% | Val: Loss nan  Acc(top1) 88.52% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc90.16393280029297_tracc87.35795454545455_epoch_1026_20240723111609.pt\n",
      "58m50s Epoch: 1027/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.36% | Val: Loss nan  Acc(top1) 90.16% | HA 95.63@584\n",
      "58m54s Epoch: 1028/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 56.28% | HA 95.63@584\n",
      "58m57s Epoch: 1029/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 34.97% | HA 95.63@584\n",
      "59m01s Epoch: 1030/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "59m04s Epoch: 1031/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.06% | Val: Loss nan  Acc(top1) 31.15% | HA 95.63@584\n",
      "59m07s Epoch: 1032/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.64% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "59m11s Epoch: 1033/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "59m14s Epoch: 1034/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.64% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "59m18s Epoch: 1035/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "59m21s Epoch: 1036/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.65% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "59m24s Epoch: 1037/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "59m28s Epoch: 1038/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 62.84% | HA 95.63@584\n",
      "59m31s Epoch: 1039/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.64% | Val: Loss nan  Acc(top1) 59.02% | HA 95.63@584\n",
      "59m35s Epoch: 1040/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 48.09% | HA 95.63@584\n",
      "59m38s Epoch: 1041/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 54.64% | HA 95.63@584\n",
      "59m41s Epoch: 1042/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.91% | Val: Loss nan  Acc(top1) 47.54% | HA 95.63@584\n",
      "59m45s Epoch: 1043/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.49% | Val: Loss nan  Acc(top1) 44.81% | HA 95.63@584\n",
      "59m48s Epoch: 1044/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.06% | Val: Loss nan  Acc(top1) 37.16% | HA 95.63@584\n",
      "59m51s Epoch: 1045/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.21% | Val: Loss nan  Acc(top1) 36.61% | HA 95.63@584\n",
      "59m55s Epoch: 1046/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.64% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "59m58s Epoch: 1047/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.22% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "1h00m Epoch: 1048/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 34.97% | HA 95.63@584\n",
      "1h00m Epoch: 1049/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 71.04% | HA 95.63@584\n",
      "1h00m Epoch: 1050/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.65% | Val: Loss nan  Acc(top1) 53.55% | HA 95.63@584\n",
      "1h00m Epoch: 1051/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.77% | Val: Loss nan  Acc(top1) 79.78% | HA 95.63@584\n",
      "1h00m Epoch: 1052/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "1h00m Epoch: 1053/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 73.22% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.07103729248047_tracc89.91477272727273_epoch_1053_20240723111740.pt\n",
      "1h00m Epoch: 1054/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.91% | Val: Loss nan  Acc(top1) 89.07% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc94.53551483154297_tracc88.06818181818183_epoch_1054_20240723111744.pt\n",
      "1h00m Epoch: 1055/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 94.54% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc95.08197021484375_tracc89.91477272727273_epoch_1055_20240723111747.pt\n",
      "1h00m Epoch: 1056/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.91% | Val: Loss nan  Acc(top1) 95.08% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc95.08197021484375_tracc89.0625_epoch_1056_20240723111750.pt\n",
      "1h00m Epoch: 1057/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 95.08% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc92.89617156982422_tracc87.7840909090909_epoch_1057_20240723111754.pt\n",
      "1h00m Epoch: 1058/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 92.90% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc93.98906707763672_tracc91.05113636363636_epoch_1058_20240723111757.pt\n",
      "1h00m Epoch: 1059/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 91.05% | Val: Loss nan  Acc(top1) 93.99% | HA 95.63@584\n",
      "1h00m Epoch: 1060/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.21% | Val: Loss nan  Acc(top1) 69.95% | HA 95.63@584\n",
      "1h00m Epoch: 1061/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "1h00m Epoch: 1062/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.36% | Val: Loss nan  Acc(top1) 78.14% | HA 95.63@584\n",
      "1h00m Epoch: 1063/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.36% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "1h00m Epoch: 1064/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.35% | Val: Loss nan  Acc(top1) 59.56% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.61748504638672_tracc89.3465909090909_epoch_1064_20240723111818.pt\n",
      "1h01m Epoch: 1065/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 89.62% | HA 95.63@584\n",
      "1h01m Epoch: 1066/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.93% | Val: Loss nan  Acc(top1) 83.61% | HA 95.63@584\n",
      "1h01m Epoch: 1067/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.07% | Val: Loss nan  Acc(top1) 70.49% | HA 95.63@584\n",
      "1h01m Epoch: 1068/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 87.78% | Val: Loss nan  Acc(top1) 71.58% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc94.53551483154297_tracc88.7784090909091_epoch_1068_20240723111831.pt\n",
      "1h01m Epoch: 1069/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 94.54% | HA 95.63@584\n",
      "1h01m Epoch: 1070/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.64% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "1h01m Epoch: 1071/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.20% | Val: Loss nan  Acc(top1) 61.20% | HA 95.63@584\n",
      "1h01m Epoch: 1072/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 73.77% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.07103729248047_tracc89.63068181818183_epoch_1072_20240723111845.pt\n",
      "1h01m Epoch: 1073/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 89.07% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc93.44261932373047_tracc86.50568181818183_epoch_1073_20240723111849.pt\n",
      "1h01m Epoch: 1074/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 86.51% | Val: Loss nan  Acc(top1) 93.44% | HA 95.63@584\n",
      "1h01m Epoch: 1075/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 84.70% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc90.16393280029297_tracc87.5_epoch_1075_20240723111855.pt\n",
      "1h01m Epoch: 1076/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 90.16% | HA 95.63@584\n",
      "1h01m Epoch: 1077/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.64% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc91.25682830810547_tracc90.3409090909091_epoch_1077_20240723111902.pt\n",
      "1h01m Epoch: 1078/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.34% | Val: Loss nan  Acc(top1) 91.26% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc94.53551483154297_tracc91.19318181818183_epoch_1078_20240723111906.pt\n",
      "1h01m Epoch: 1079/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.19% | Val: Loss nan  Acc(top1) 94.54% | HA 95.63@584\n",
      "1h01m Epoch: 1080/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.20% | Val: Loss nan  Acc(top1) 73.22% | HA 95.63@584\n",
      "1h01m Epoch: 1081/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.64% | Val: Loss nan  Acc(top1) 53.55% | HA 95.63@584\n",
      "1h01m Epoch: 1082/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.78% | Val: Loss nan  Acc(top1) 83.06% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.61748504638672_tracc89.20454545454545_epoch_1082_20240723111919.pt\n",
      "1h02m Epoch: 1083/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 89.62% | HA 95.63@584\n",
      "1h02m Epoch: 1084/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 51.37% | HA 95.63@584\n",
      "1h02m Epoch: 1085/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.91% | Val: Loss nan  Acc(top1) 59.56% | HA 95.63@584\n",
      "1h02m Epoch: 1086/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "1h02m Epoch: 1087/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.62% | Val: Loss nan  Acc(top1) 57.92% | HA 95.63@584\n",
      "1h02m Epoch: 1088/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.20% | Val: Loss nan  Acc(top1) 55.74% | HA 95.63@584\n",
      "1h02m Epoch: 1089/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 80.87% | HA 95.63@584\n",
      "1h02m Epoch: 1090/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.35% | Val: Loss nan  Acc(top1) 81.42% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.61748504638672_tracc87.7840909090909_epoch_1090_20240723111946.pt\n",
      "1h02m Epoch: 1091/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.78% | Val: Loss nan  Acc(top1) 89.62% | HA 95.63@584\n",
      "1h02m Epoch: 1092/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.93% | Val: Loss nan  Acc(top1) 68.31% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc93.44261932373047_tracc89.77272727272727_epoch_1092_20240723111953.pt\n",
      "1h02m Epoch: 1093/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 93.44% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.07103729248047_tracc87.5_epoch_1093_20240723111957.pt\n",
      "1h02m Epoch: 1094/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 87.50% | Val: Loss nan  Acc(top1) 89.07% | HA 95.63@584\n",
      "1h02m Epoch: 1095/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.93% | Val: Loss nan  Acc(top1) 51.37% | HA 95.63@584\n",
      "1h02m Epoch: 1096/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.06% | Val: Loss nan  Acc(top1) 59.02% | HA 95.63@584\n",
      "1h02m Epoch: 1097/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 90.91% | Val: Loss nan  Acc(top1) 73.77% | HA 95.63@584\n",
      "1h02m Epoch: 1098/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.77% | Val: Loss nan  Acc(top1) 59.02% | HA 95.63@584\n",
      "1h02m Epoch: 1099/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.07% | Val: Loss nan  Acc(top1) 57.38% | HA 95.63@584\n",
      "1h02m Epoch: 1100/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc91.80327606201172_tracc88.7784090909091_epoch_1100_20240723112021.pt\n",
      "1h03m Epoch: 1101/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 91.80% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc90.71038055419922_tracc89.77272727272727_epoch_1101_20240723112024.pt\n",
      "1h03m Epoch: 1102/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 90.71% | HA 95.63@584\n",
      "1h03m Epoch: 1103/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 47.54% | HA 95.63@584\n",
      "1h03m Epoch: 1104/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.35% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "1h03m Epoch: 1105/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 34.97% | HA 95.63@584\n",
      "1h03m Epoch: 1106/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.93% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "1h03m Epoch: 1107/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.93% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "1h03m Epoch: 1108/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.34% | Val: Loss nan  Acc(top1) 49.18% | HA 95.63@584\n",
      "1h03m Epoch: 1109/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.92% | Val: Loss nan  Acc(top1) 52.46% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc90.71038055419922_tracc88.7784090909091_epoch_1109_20240723112052.pt\n",
      "1h03m Epoch: 1110/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 90.71% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc92.89617156982422_tracc89.3465909090909_epoch_1110_20240723112055.pt\n",
      "1h03m Epoch: 1111/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 92.90% | HA 95.63@584\n",
      "1h03m Epoch: 1112/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.93% | Val: Loss nan  Acc(top1) 69.40% | HA 95.63@584\n",
      "1h03m Epoch: 1113/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "1h03m Epoch: 1114/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.05% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "1h03m Epoch: 1115/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.62% | Val: Loss nan  Acc(top1) 51.37% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc88.52458953857422_tracc90.48295454545455_epoch_1115_20240723112112.pt\n",
      "1h03m Epoch: 1116/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.48% | Val: Loss nan  Acc(top1) 88.52% | HA 95.63@584\n",
      "1h03m Epoch: 1117/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.49% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "1h04m Epoch: 1118/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.91% | Val: Loss nan  Acc(top1) 57.38% | HA 95.63@584\n",
      "1h04m Epoch: 1119/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 91.05% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "1h04m Epoch: 1120/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 85.94% | Val: Loss nan  Acc(top1) 75.41% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc93.44261932373047_tracc89.63068181818183_epoch_1120_20240723112129.pt\n",
      "1h04m Epoch: 1121/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.63% | Val: Loss nan  Acc(top1) 93.44% | HA 95.63@584\n",
      "1h04m Epoch: 1122/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "1h04m Epoch: 1123/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.78% | Val: Loss nan  Acc(top1) 78.69% | HA 95.63@584\n",
      "1h04m Epoch: 1124/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.78% | Val: Loss nan  Acc(top1) 37.16% | HA 95.63@584\n",
      "1h04m Epoch: 1125/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.91% | Val: Loss nan  Acc(top1) 53.55% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc86.33879089355469_tracc89.77272727272727_epoch_1125_20240723112146.pt\n",
      "1h04m Epoch: 1126/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 86.34% | HA 95.63@584\n",
      "1h04m Epoch: 1127/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.78% | Val: Loss nan  Acc(top1) 54.64% | HA 95.63@584\n",
      "1h04m Epoch: 1128/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.92% | Val: Loss nan  Acc(top1) 83.06% | HA 95.63@584\n",
      "1h04m Epoch: 1129/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.93% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "1h04m Epoch: 1130/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "1h04m Epoch: 1131/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.92% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "1h04m Epoch: 1132/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 42.08% | HA 95.63@584\n",
      "1h04m Epoch: 1133/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "1h04m Epoch: 1134/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "1h04m Epoch: 1135/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "1h05m Epoch: 1136/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.35% | Val: Loss nan  Acc(top1) 48.09% | HA 95.63@584\n",
      "1h05m Epoch: 1137/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 55.74% | HA 95.63@584\n",
      "1h05m Epoch: 1138/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.63% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "1h05m Epoch: 1139/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "1h05m Epoch: 1140/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 55.19% | HA 95.63@584\n",
      "1h05m Epoch: 1141/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 89.20% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "1h05m Epoch: 1142/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 55.74% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc86.88524627685547_tracc86.64772727272727_epoch_1142_20240723112244.pt\n",
      "1h05m Epoch: 1143/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.65% | Val: Loss nan  Acc(top1) 86.89% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc93.44261932373047_tracc88.06818181818183_epoch_1143_20240723112248.pt\n",
      "1h05m Epoch: 1144/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.07% | Val: Loss nan  Acc(top1) 93.44% | HA 95.63@584\n",
      "1h05m Epoch: 1145/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 87.64% | Val: Loss nan  Acc(top1) 71.04% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc94.53551483154297_tracc86.78977272727273_epoch_1145_20240723112254.pt\n",
      "1h05m Epoch: 1146/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.79% | Val: Loss nan  Acc(top1) 94.54% | HA 95.63@584\n",
      "1h05m Epoch: 1147/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "1h05m Epoch: 1148/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.78% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "1h05m Epoch: 1149/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.48% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "1h05m Epoch: 1150/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 64.48% | HA 95.63@584\n",
      "1h05m Epoch: 1151/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.50% | Val: Loss nan  Acc(top1) 34.97% | HA 95.63@584\n",
      "1h05m Epoch: 1152/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.77% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "1h06m Epoch: 1153/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 91.34% | Val: Loss nan  Acc(top1) 45.90% | HA 95.63@584\n",
      "1h06m Epoch: 1154/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.35% | Val: Loss nan  Acc(top1) 69.95% | HA 95.63@584\n",
      "1h06m Epoch: 1155/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 86.79% | Val: Loss nan  Acc(top1) 34.97% | HA 95.63@584\n",
      "1h06m Epoch: 1156/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.21% | Val: Loss nan  Acc(top1) 76.50% | HA 95.63@584\n",
      "1h06m Epoch: 1157/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.48% | Val: Loss nan  Acc(top1) 58.47% | HA 95.63@584\n",
      "1h06m Epoch: 1158/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "1h06m Epoch: 1159/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.79% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "1h06m Epoch: 1160/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc87.97814178466797_tracc88.49431818181817_epoch_1160_20240723112346.pt\n",
      "1h06m Epoch: 1161/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 87.98% | HA 95.63@584\n",
      "1h06m Epoch: 1162/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.35% | Val: Loss nan  Acc(top1) 82.51% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc85.24590301513672_tracc90.19886363636364_epoch_1162_20240723112353.pt\n",
      "1h06m Epoch: 1163/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 90.20% | Val: Loss nan  Acc(top1) 85.25% | HA 95.63@584\n",
      "1h06m Epoch: 1164/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.63% | Val: Loss nan  Acc(top1) 75.41% | HA 95.63@584\n",
      "1h06m Epoch: 1165/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.34% | Val: Loss nan  Acc(top1) 49.73% | HA 95.63@584\n",
      "1h06m Epoch: 1166/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.51% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "1h06m Epoch: 1167/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.49% | Val: Loss nan  Acc(top1) 62.30% | HA 95.63@584\n",
      "1h06m Epoch: 1168/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 87.22% | Val: Loss nan  Acc(top1) 59.02% | HA 95.63@584\n",
      "1h06m Epoch: 1169/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 82.51% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc92.89617156982422_tracc89.91477272727273_epoch_1169_20240723112416.pt\n",
      "1h06m Epoch: 1170/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.91% | Val: Loss nan  Acc(top1) 92.90% | HA 95.63@584\n",
      "1h07m Epoch: 1171/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.34% | Val: Loss nan  Acc(top1) 77.60% | HA 95.63@584\n",
      "1h07m Epoch: 1172/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.77% | Val: Loss nan  Acc(top1) 74.32% | HA 95.63@584\n",
      "1h07m Epoch: 1173/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.93% | Val: Loss nan  Acc(top1) 69.40% | HA 95.63@584\n",
      "1h07m Epoch: 1174/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.06% | Val: Loss nan  Acc(top1) 63.93% | HA 95.63@584\n",
      "1h07m Epoch: 1175/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "1h07m Epoch: 1176/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.78% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "1h07m Epoch: 1177/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.78% | Val: Loss nan  Acc(top1) 32.24% | HA 95.63@584\n",
      "1h07m Epoch: 1178/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "1h07m Epoch: 1179/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 50.27% | HA 95.63@584\n",
      "1h07m Epoch: 1180/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.21% | Val: Loss nan  Acc(top1) 53.55% | HA 95.63@584\n",
      "1h07m Epoch: 1181/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 88.92% | Val: Loss nan  Acc(top1) 55.19% | HA 95.63@584\n",
      "1h07m Epoch: 1182/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.64% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "1h07m Epoch: 1183/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 90.48% | Val: Loss nan  Acc(top1) 46.99% | HA 95.63@584\n",
      "1h07m Epoch: 1184/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 49.18% | HA 95.63@584\n",
      "1h07m Epoch: 1185/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.35% | Val: Loss nan  Acc(top1) 31.15% | HA 95.63@584\n",
      "1h07m Epoch: 1186/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.49% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "1h07m Epoch: 1187/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.22% | Val: Loss nan  Acc(top1) 37.70% | HA 95.63@584\n",
      "1h07m Epoch: 1188/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.91% | Val: Loss nan  Acc(top1) 46.45% | HA 95.63@584\n",
      "1h08m Epoch: 1189/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 87.93% | Val: Loss nan  Acc(top1) 42.62% | HA 95.63@584\n",
      "1h08m Epoch: 1190/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.92% | Val: Loss nan  Acc(top1) 57.38% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc85.24590301513672_tracc89.3465909090909_epoch_1190_20240723112527.pt\n",
      "1h08m Epoch: 1191/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 89.35% | Val: Loss nan  Acc(top1) 85.25% | HA 95.63@584\n",
      "1h08m Epoch: 1192/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 87.64% | Val: Loss nan  Acc(top1) 55.19% | HA 95.63@584\n",
      "1h08m Epoch: 1193/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.20% | Val: Loss nan  Acc(top1) 57.38% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.07103729248047_tracc88.06818181818183_epoch_1193_20240723112538.pt\n",
      "1h08m Epoch: 1194/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.09  Acc 88.07% | Val: Loss nan  Acc(top1) 89.07% | HA 95.63@584\n",
      "1h08m Epoch: 1195/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 89.06% | Val: Loss nan  Acc(top1) 55.19% | HA 95.63@584\n",
      "1h08m Epoch: 1196/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.11  Acc 89.91% | Val: Loss nan  Acc(top1) 42.08% | HA 95.63@584\n",
      "1h08m Epoch: 1197/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 86.79% | Val: Loss nan  Acc(top1) 50.82% | HA 95.63@584\n",
      "1h08m Epoch: 1198/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 88.64% | Val: Loss nan  Acc(top1) 59.02% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc89.07103729248047_tracc86.93181818181817_epoch_1198_20240723112554.pt\n",
      "1h08m Epoch: 1199/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.12  Acc 86.93% | Val: Loss nan  Acc(top1) 89.07% | HA 95.63@584\n",
      "save model to ../../../trained_models/step_2_first_stage_pruning/multifold/s2_wprun_4C_fold1_2024072310_prunratio85.0/uec_4C_weight_prun_fold1_haacc_95.62841033935547_valacc93.98906707763672_tracc90.3409090909091_epoch_1199_20240723112558.pt\n",
      "1h08m Epoch: 1200/1200 | Time: 0m03s (Train 0m03s  Val 0m00s) | Train: LR 1.0000000000000002e-07  Loss 0.10  Acc 90.34% | Val: Loss nan  Acc(top1) 93.99% | HA 95.63@584\n",
      "Execution finished in: 1h08m\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ae31fa-cf50-449c-88b3-fdca70c66228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee28e6-b711-4642-8ac9-7e506be8ad4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5acd0-ef85-4301-aedb-5cc561681ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
