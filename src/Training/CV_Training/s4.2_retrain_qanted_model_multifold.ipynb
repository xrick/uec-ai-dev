{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369d21d4-2c2f-4ebf-b987-d28aef031497",
   "metadata": {},
   "source": [
    "## QAT過程問題集\n",
    "- can't convert float NaN (actually 0.00000) to int:\n",
    "  - 與weight-decay設定值可能有關，設定太大倒導致錯誤。\n",
    "  - 可能是同時開啟三個訓練程式造成記憶體不足造成。\n",
    "- 在訓練的過程中，若出現labels values errors，把batch size降低，即可正常，詳細原因待查。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d582be4b-1b35-4dfc-a20d-b68077cb88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;\n",
    "import os;\n",
    "import glob;\n",
    "import math;\n",
    "import random;\n",
    "import torch;\n",
    "import torch.optim as optim;\n",
    "import torch.nn as nn;\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56d3cae9-1208-4e00-bbcf-b7b5c2f91304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu121\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737226ae-820b-4739-ab00-755bfe323ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../../\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b701027a-c7e0-4908-8d3b-6c8cdf91746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import common.opts as opts;\n",
    "import common.utils as U;\n",
    "# import th.resources.models as models;\n",
    "import th.resources.no_softmax_quant_model as models;\n",
    "import th.resources.calculator as calc;\n",
    "from SharedLibs.datestring import getDateStr, genDataTimeStr;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55848bf9-571e-4995-bba6-c85fd888664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d9d813-5a9d-4595-a25b-b0fcef8f351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.quantization import QuantStub, DeQuantStub\n",
    "# from tinynn.converter import TFLiteConverter\n",
    "# from tinynn.graph.quantization.quantizer import PostQuantizer\n",
    "# from tinynn.graph.tracer import model_tracer\n",
    "# from tinynn.util.train_util import DLContext, get_device\n",
    "# from tinynn.graph.quantization.algorithm.cross_layer_equalization import cross_layer_equalize\n",
    "# from tinynn.converter import TFLiteConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cce7e38-1de4-4782-9a3e-6a1fc3cad987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref site:https://discuss.pytorch.org/t/how-to-generate-a-fully-quantized-model/175185\n",
    "# from torch.ao.quantization.backend_config import BackendConfig, BackendPatternConfig, DTypeConfig, ObservationType\n",
    "# from torch.quantization import quantize_fx\n",
    "# from torch.ao.quantization import QConfigMapping\n",
    "# from torch.ao.quantization.fx.custom_config import PrepareCustomConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aa078a1-193d-419e-a154-eb69dd598cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42;\n",
    "random.seed(seed);\n",
    "np.random.seed(seed);\n",
    "torch.manual_seed(seed);\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed);\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(seed)\n",
    "# torch.backends.cudnn.deterministic = True;\n",
    "# torch.backends.cudnn.benchmark = False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aeb4481-8d39-4ed5-b432-d04802e4ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask8 = 0x4000 # >> 8 : 16384\n",
    "mask7 = 0x2000 # >> 7 :  8192\n",
    "mask6 = 0x1000 # >> 6 :  4096\n",
    "mask5 = 0x0800 # >> 5 :  2048\n",
    "mask4 = 0x0400 # >> 4 :  1024\n",
    "mask3 = 0x0200 # >> 3 :   512\n",
    "mask2 = 0x0100 # >> 2 :   256\n",
    "mask1 = 0x0080 # >> 1 :   128\n",
    "mask0 = 0x0040 # >> 0 :    64 below the value, drop the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbce076-92c6-4b5e-97cc-a76f874c6539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70785c76-3485-48fd-b052-f33ea13876c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskOP(x):\n",
    "    x = np.int16(x)\n",
    "    # print(f\"begin:x:{x}\")\n",
    "    if (mask8&x):\n",
    "        return x >> 8\n",
    "    elif (mask7&x):\n",
    "        return x >> 7\n",
    "    elif (mask6&x):\n",
    "        return x >> 6\n",
    "    elif (mask5&x):\n",
    "        return x >> 5\n",
    "    elif (mask4&x):\n",
    "        return x >> 4\n",
    "    elif (mask3&x):\n",
    "        return x >> 3\n",
    "    elif (mask2&x):\n",
    "        return x >> 2\n",
    "    elif (mask1&x):\n",
    "        return x >> 1\n",
    "    elif (mask0&x):\n",
    "        return x\n",
    "    else:\n",
    "        return 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aa9d71f-76e1-43c0-bf2f-a278115274db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_int8(x, axis):\n",
    "    len_of_x = len(x[0][0][0])\n",
    "    print(f\"len_of_x:{len_of_x}\")\n",
    "    for i in range(len_of_x):\n",
    "        nflag = 2; #positive\n",
    "        print(\"{}:{}\".format(i,x[0][0][0][i]))\n",
    "        tmp_x = x[0][0][0][i]\n",
    "        if tmp_x < 0:\n",
    "            tmp_x = np.abs(tmp_x)\n",
    "            nflag = 1\n",
    "        tmp_x = maskOP(tmp_x)\n",
    "        if(nflag==1):\n",
    "            tmp_x = -1 * (tmp_x)\n",
    "        print(\"{}:{}\".format(i,x[0][0][0][i]))\n",
    "        print(\"*********************************\")\n",
    "        x[0][0][0][i] = tmp_x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e85b1c0f-4596-4d69-bc5d-94af3243dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLGenerator():\n",
    "    #Generates data for Keras\n",
    "    def __init__(self, samples, labels, options):\n",
    "        random.seed(42);\n",
    "        #Initialization\n",
    "        print(f\"length of samples:{len(samples)}\")\n",
    "        self.data = [(samples[i], labels[i]) for i in range (0, len(samples))];\n",
    "        self.opt = options;\n",
    "        self.batch_size = options.batchSize;\n",
    "        self.preprocess_funcs = self.preprocess_setup();\n",
    "        self.mapdict = dict([('52',1),('56',2),('71',3),('99',4)])\n",
    "\n",
    "    def __len__(self):\n",
    "        #Denotes the number of batches per epoch\n",
    "        return int(np.floor(len(self.data) / self.batch_size));\n",
    "\n",
    "    def __getitem__(self, batchIndex):\n",
    "        #Generate one batch of data\n",
    "        batchX, batchY = self.generate_batch_select_fixed_class(batchIndex);#self.generate_batch(batchIndex);\n",
    "        batchX = np.expand_dims(batchX, axis=1);\n",
    "        batchX = np.expand_dims(batchX, axis=3);\n",
    "        return batchX, batchY\n",
    "\n",
    "    def generate_batch(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                if label1 != label2:\n",
    "                    break\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random())\n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            # print(f\"sound length after U.mix is {len(sound)}\")\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[str(label1)]- 1\n",
    "            idx2 = self.mapdict[str(label2)] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            # label = (eye[label1] * r + eye[label2] * (1 - r)).astype(np.float32)\n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        # print(f\"labels in generate_batch is:\\n{labels}\")\n",
    "\n",
    "        return sounds, labels;\n",
    "\n",
    "    def generate_batch_select_fixed_class(self, batchIndex):\n",
    "        #Generates data containing batch_size samples\n",
    "        sounds = [];\n",
    "        labels = [];\n",
    "        indexes = None;\n",
    "        #two variables recording alarm and moaning sounds count\n",
    "        alarm_selected = 0;\n",
    "        moaning_selected = 0;\n",
    "        help_eng_selected = 0;\n",
    "        for i in range(self.batch_size):\n",
    "            # Training phase of BC learning\n",
    "            # Select two training examples\n",
    "            while True:\n",
    "                # print(\"enter while true\")\n",
    "                sound1, label1 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                sound2, label2 = self.data[random.randint(0, len(self.data) - 1)]\n",
    "                lbl1_int = np.int16(label1);\n",
    "                lbl2_int = np.int16(label2);\n",
    "                if (lbl1_int == 52 and lbl2_int == 99) or (lbl1_int == 99 and lbl2_int ==52):\n",
    "                    if (alarm_selected == moaning_selected) and (alarm_selected == help_eng_selected):\n",
    "                        alarm_selected += 1;\n",
    "                        break;\n",
    "                if (lbl1_int == 56 and lbl2_int == 99) or (lbl1_int == 99 and lbl2_int == 56):\n",
    "                    if (moaning_selected < alarm_selected) and (moaning_selected == help_eng_selected):\n",
    "                        moaning_selected += 1;\n",
    "                        break;\n",
    "                if (lbl1_int == 71 and lbl2_int == 99) or (lbl1_int == 99 and lbl2_int == 71):\n",
    "                    if (help_eng_selected < alarm_selected) and (help_eng_selected < moaning_selected):\n",
    "                        help_eng_selected += 1;\n",
    "                        break;\n",
    "            # print(f\"escape for loop\");\n",
    "            sound1 = self.preprocess(sound1)\n",
    "            sound2 = self.preprocess(sound2)\n",
    "\n",
    "            # Mix two examples\n",
    "            r = np.array(random.random());\n",
    "           \n",
    "            # print(f\"r:{r}, lbl1:{label1}, lbl2:{label2}\")  \n",
    "            sound = U.mix(sound1, sound2, r, self.opt.sr).astype(np.float32)\n",
    "            eye = np.eye(self.opt.nClasses)\n",
    "            idx1 = self.mapdict[str(label1)]- 1\n",
    "            idx2 = self.mapdict[str(label2)] - 1\n",
    "            label = (eye[idx1] * r + eye[idx2] * (1 - r)).astype(np.float32)\n",
    "            \n",
    "\n",
    "            #For stronger augmentation\n",
    "            sound = U.random_gain(6)(sound).astype(np.float32)\n",
    "            # print(f\"sound length after U.random_gain is {len(sound)}\")\n",
    "            sounds.append(sound);\n",
    "            labels.append(label);\n",
    "\n",
    "        sounds = np.asarray(sounds);\n",
    "        labels = np.asarray(labels);\n",
    "        \n",
    "        return sounds, labels; \n",
    "\n",
    "    def preprocess_setup(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength),\n",
    "                  U.normalize(32768.0)]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess_setup_without_normalization(self):\n",
    "        funcs = []\n",
    "        if self.opt.strongAugment:\n",
    "            funcs += [U.random_scale(1.25)]\n",
    "\n",
    "        funcs += [U.padding(self.opt.inputLength // 2),\n",
    "                  U.random_crop(self.opt.inputLength)\n",
    "                  ]\n",
    "        return funcs\n",
    "\n",
    "    def preprocess(self, sound):\n",
    "        for f in self.preprocess_funcs:\n",
    "            sound = f(sound)\n",
    "\n",
    "        return sound;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82f89d02-367f-4058-addf-3b9befa7988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainGen(opt=None, split=None):\n",
    "    # dataset = np.load(\"../../../../uec_iot_ai_models_datasets/generated_datasets/train/version11/single_fold_train_20240603063535.npz\", allow_pickle=True);#home\n",
    "    # dataset = np.load(\"../../../../uec_iot_models_datasets/version11/single_fold_train_20240603063535.npz\", allow_pickle=True);#office\n",
    "    # dataset = np.load(\"../../../../uec_iot_dev_training_datasets/generated_datasets/multifold/train/version15_multifold_home_fold5/fold5_train_20240721013721.npz\", allow_pickle=True);#home\n",
    "    dataset = np.load(opt.trainData, allow_pickle=True)\n",
    "    train_sounds = []\n",
    "    train_labels = []\n",
    "    # train_sounds = [dataset['x'][i][0] for i in range(len(dataset['x']))]\n",
    "    # train_labels = [dataset['y'][i][0] for i in range(len(dataset['y']))]\n",
    "    train_sounds = dataset['{}'.format(opt.current_fold)].item()['sounds']\n",
    "    train_labels = dataset['{}'.format(opt.current_fold)].item()['labels']\n",
    "    trainGen = TLGenerator(train_sounds, train_labels, opt);\n",
    "    return trainGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76de21f9-4fcd-4f11-8110-a78aedfeea20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc6ab99b-1937-49d1-a1e9-7cf45adfa155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOpts():\n",
    "    parser = argparse.ArgumentParser(description='Transfer Learning for ACDNet');\n",
    "    parser.add_argument('--netType', default='TLACDNet',  required=False);\n",
    "    parser.add_argument('--data', default='./datasets/processed/',  required=False);\n",
    "    parser.add_argument('--dataset', required=False, default='uec_iot', choices=['10']);\n",
    "    parser.add_argument('--BC', default=True, action='store_true', help='BC learning');\n",
    "    parser.add_argument('--strongAugment', default=True,  action='store_true', help='Add scale and gain augmentation');\n",
    "    #在ipynb中，不能使用parser.parse，要改用parser.parse_known_args()\n",
    "    opt, unknown = parser.parse_known_args()\n",
    "    \"\"\"\n",
    "   \n",
    "    \"\"\"\n",
    "    #Leqarning settings\n",
    "    opt.batchSize = 64;\n",
    "    opt.weightDecay = 5e-4;\n",
    "    opt.LR = 0.1;\n",
    "    opt.momentum = 0.9;\n",
    "    \n",
    "    opt.schedule = [0.3, 0.6, 0.9];\n",
    "    opt.warmup = 10;\n",
    "\n",
    "    #Basic Net Settings\n",
    "    opt.nClasses = 4#50;\n",
    "    opt.nFolds = 1;#5;\n",
    "    opt.splits = [i for i in range(1, opt.nFolds + 1)];\n",
    "    opt.sr = 20000;\n",
    "    opt.inputLength = 20150;\n",
    "    #Test data\n",
    "    opt.nCrops = 2;\n",
    "    # opt.ch_config = [8,64,32,64,64,128,128,256,256,512,512,2];\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "207352a5-4f18-4809-9215-7eb4fbd16c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_info(opt):\n",
    "    print('+------------------------------+');\n",
    "    print('| {} Sound classification'.format(opt.netType));\n",
    "    print('+------------------------------+');\n",
    "    print('| dataset  : {}'.format(opt.dataset));\n",
    "    print('| nEpochs  : {}'.format(opt.nEpochs));\n",
    "    print('| LRInit   : {}'.format(opt.LR));\n",
    "    print('| schedule : {}'.format(opt.schedule));\n",
    "    print('| warmup   : {}'.format(opt.warmup));\n",
    "    print('| batchSize: {}'.format(opt.batchSize));\n",
    "    print('| nFolds: {}'.format(opt.nFolds));\n",
    "    print('| Splits: {}'.format(opt.splits));\n",
    "    print('| Device: {}'.format(opt.device));\n",
    "    print('| Model Path: {}'.format(opt.model_path));\n",
    "    print('| Model Name: {}'.format(opt.model_name));\n",
    "    print('+------------------------------+');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce696bd8-42c8-4408-9d39-8fe42665011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IfQuanted = False;\n",
    "class QATTrainer:\n",
    "    def __init__(self, opt=None, split=0):\n",
    "        self.opt = opt;\n",
    "        self.testX = None;\n",
    "        self.testY = None;\n",
    "        self.trainX = None;\n",
    "        self.trainY = None;\n",
    "        self.opt.device = opt.device;\n",
    "        self.trainGen = getTrainGen(self.opt)#train_generator.setup(self.opt, self.opt.split);\n",
    "        self.qunt_nClass = opt.nClasses;\n",
    "        self.bestAcc = 0.0;\n",
    "        self.bestAccEpoch = 0;\n",
    "\n",
    "    def load_train_data(self):\n",
    "        print('Preparing calibration dataset..');\n",
    "        x,y = self.trainGen.__getitem__(0);\n",
    "        self.trainX = torch.tensor(np.moveaxis(x, 3, 1)).to(self.opt.device);\n",
    "        \"\"\"\n",
    "        trainX size:torch.Size([1, 1, 20150]), but must be [1,1,1,20150]\n",
    "        Due to the reason: raise ValueError(\"Input shape must be `(N, C, H, W)`!\")\n",
    "        \"\"\"\n",
    "        # print(f\"trainX[0] shape:{self.trainX[0].shape}\")\n",
    "        self.trainY = torch.tensor(y).to(self.opt.device);\n",
    "        print('Calibration dataset is ready');\n",
    "        # self.opt.batchSize = 32;\n",
    "\n",
    "    # def load_test_data(self):\n",
    "    #     if(self.testX is None):\n",
    "    #         data = np.load('../../datasets/CurrentUse/forOneClassModel_alarm/test_val/final_val_test_npz/final_valSet_20240119004614.npz', allow_pickle=True);\n",
    "    #         dataX = np.moveaxis(data['x'], 3, 1).astype(np.float32);\n",
    "    #         self.testX = torch.tensor(dataX).to(self.opt.device);\n",
    "    #         self.testY = torch.tensor(data['y']).to(self.opt.device);\n",
    "\n",
    "    def load_test_data(self):\n",
    "        # testData = '../../../../uec_iot_ai_models_datasets/generated_datasets/val/version11/final_single_val_20240603063755.npz'#home\n",
    "        # testData = '../../../../uec_iot_models_datasets/version11/final_single_val_20240603063755.npz' #office\n",
    "        testData = self.opt.valData;\n",
    "        data = np.load(testData, allow_pickle=True);\n",
    "        print(f\"device is :{self.opt.device}\")\n",
    "        print(f\"len of Y:{len(data['y'])}\")\n",
    "        # self.testX = torch.tensor(np.moveaxis(data['x'], 3, 1)).to(self.opt.device);\n",
    "        dataX = np.moveaxis(data['x'], 3, 1).astype(np.float32);\n",
    "        self.testX = torch.tensor(dataX).to(self.opt.device);\n",
    "        self.testY = torch.tensor(data['y']).type(torch.float32).to(self.opt.device);\n",
    "\n",
    "    def __validate_test(self, net, qat_done, testX, testY):\n",
    "        net.eval();\n",
    "        if qat_done:\n",
    "            self.testX.to('cpu');\n",
    "            self.testY.to('cpu');\n",
    "        else:\n",
    "            self.testX.to(self.opt.device);\n",
    "            self.testY.to(self.opt.device);\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = len(self.testX);\n",
    "            x = self.testX[:];\n",
    "            scores = net(x);\n",
    "            y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "            acc = self.__compute_accuracy_2(y_pred, self.testY);\n",
    "        return acc;\n",
    "\n",
    "    \n",
    "    def __validate(self, net, lossFunc):\n",
    "        if self.testX is None:\n",
    "            self.load_test_data();\n",
    "        net.eval();\n",
    "        acc=0.0; \n",
    "        loss = 0.0;\n",
    "        with torch.no_grad():\n",
    "            y_pred = None;\n",
    "            batch_size = len(self.testX);#(self.opt.batchSize//self.opt.nCrops)*self.opt.nCrops;\n",
    "            x = self.testX[:];\n",
    "            try:\n",
    "                scores = net(x);\n",
    "                y_pred = scores.data if y_pred is None else torch.cat((y_pred, scores.data));\n",
    "                acc, loss = self.__compute_accuracy(y_pred, self.testY, lossFunc);\n",
    "            except ValueError:\n",
    "                print(f\"error data:{x}\")\n",
    "        net.train();\n",
    "        return acc, loss;\n",
    "\n",
    "    #Calculating average prediction (10 crops) and final accuracy\n",
    "    def __compute_accuracy_2(self, y_pred, y_target):\n",
    "        print(y_pred.shape);\n",
    "        with torch.no_grad():\n",
    "            y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1);\n",
    "            y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1);\n",
    "\n",
    "            y_pred = y_pred.argmax(dim=1);\n",
    "            y_target = y_target.argmax(dim=1);\n",
    "\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "        return acc;\n",
    "        \n",
    "\n",
    "    def __compute_accuracy(self, y_pred, y_target, lossFunc):\n",
    "        print(f\"shape of y_pred:{y_pred.shape}\");\n",
    "        print(f\"shape of y_target:{y_target.shape}\");\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            #Reshape to shape theme like each sample comtains 10 samples, calculate mean and find theindices that has highest average value for each sample\n",
    "            if self.opt.nCrops == 1:\n",
    "                y_pred = y_pred.argmax(dim=1);\n",
    "                y_target = y_target.argmax(dim=1);\n",
    "            else:\n",
    "                y_pred = (y_pred.reshape(y_pred.shape[0]//self.opt.nCrops, self.opt.nCrops, y_pred.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "                y_target = (y_target.reshape(y_target.shape[0]//self.opt.nCrops, self.opt.nCrops, y_target.shape[1])).mean(dim=1).argmax(dim=1);\n",
    "                print(f\"after: len of y_pred:{len(y_pred)}, len of y_target:{len(y_target)}\")\n",
    "            acc = (((y_pred==y_target)*1).float().mean()*100).item();\n",
    "            # valLossFunc = torch.nn.KLDivLoss();\n",
    "            loss = lossFunc(y_pred.float().log(), y_target.float()).item();\n",
    "            # loss = 0.0;\n",
    "        return acc, loss;\n",
    "        \n",
    "\n",
    "    def __load_model(self, quant=True):\n",
    "        state = torch.load(self.opt.model_path, map_location=self.opt.device);\n",
    "        print(state['config']);\n",
    "        net = None;\n",
    "        net = models.GetACDNetQuantModel(input_len=self.opt.inputLength, nclass=self.qunt_nClass, sr=self.opt.sr, channel_config=state['config']).to(self.opt.device);\n",
    "        calc.summary(net, (1,1,self.opt.inputLength));\n",
    "        net.load_state_dict(state['weight']);\n",
    "        return net;\n",
    "\n",
    "    \n",
    "    def __train(self, net):\n",
    "        self.load_train_data();\n",
    "        # net.eval();\n",
    "        # calc.summary(net, (1,1,self.opt.inputLength));\n",
    "        lossFunc = torch.nn.KLDivLoss(reduction='batchmean');\n",
    "        optimizer = optim.SGD(net.parameters(), lr=self.opt.LR, weight_decay=self.opt.weightDecay, momentum=self.opt.momentum, nesterov=True);\n",
    "        train_start_time = time.time();\n",
    "        for epochIdx in range(self.opt.nEpochs):\n",
    "            epoch_start_time = time.time();\n",
    "            optimizer.param_groups[0]['lr'] = self.__get_lr(epochIdx+1);\n",
    "            cur_lr = optimizer.param_groups[0]['lr'];\n",
    "            running_loss = 0.0;\n",
    "            running_acc = 0.0;\n",
    "            n_batches = math.ceil(len(self.trainGen.data)/self.opt.batchSize);\n",
    "            for batchIdx in range(n_batches):\n",
    "                # with torch.no_grad():\n",
    "                x,y = self.trainGen.__getitem__(batchIdx)\n",
    "                x = torch.tensor(np.moveaxis(x, 3, 1)).to(self.opt.device);\n",
    "                y = torch.tensor(y).to(self.opt.device);\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad();\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                try:\n",
    "                    outputs = torch.softmax(input=net(x),dim=1); #need to check float NaN value?\n",
    "                    running_acc += (((outputs.data.argmax(dim=1) == y.argmax(dim=1))*1).float().mean()).item();\n",
    "                    loss = lossFunc(outputs.log(), y);\n",
    "                    loss.backward();\n",
    "                    optimizer.step();\n",
    "                    running_loss += loss.item();\n",
    "                except ValueError:\n",
    "                    print(f\"error label:{y}\")\n",
    "                    print(f\"error data:{x}\")\n",
    "                    continue\n",
    "\n",
    "            tr_acc = (running_acc / n_batches)*100;\n",
    "            tr_loss = running_loss / n_batches;\n",
    "\n",
    "            #Epoch wise validation Validation\n",
    "            epoch_train_time = time.time() - epoch_start_time;\n",
    "\n",
    "            net.eval();\n",
    "            val_acc, val_loss = self.__validate(net, lossFunc);\n",
    "            #Save best model\n",
    "            self.__chk_bestAcc(val_acc, epochIdx, net);\n",
    "            self.__on_epoch_end(epoch_start_time, epoch_train_time, epochIdx, cur_lr, tr_loss, tr_acc, val_loss, val_acc);\n",
    "\n",
    "            running_loss = 0;\n",
    "            running_acc = 0;\n",
    "            net.train();\n",
    "\n",
    "        total_time_taken = time.time() - train_start_time;\n",
    "        print(\"Execution finished in: {}\".format(U.to_hms(total_time_taken)));\n",
    "\n",
    "\n",
    "    def __chk_bestAcc(self, acc, epochIdx, net):\n",
    "        print(\"__chk_bestAcc is called\")\n",
    "        print(f\"current best Acc is {self.bestAcc}\")\n",
    "        print(f\"pass in acc is {acc}\")\n",
    "        if acc > self.bestAcc:\n",
    "            self.bestAcc = acc;\n",
    "            self.bestAccEpoch = epochIdx +1;\n",
    "            print(f\"model saved....., acc: {acc}\")\n",
    "            \n",
    "    def __on_epoch_end(self, start_time, train_time, epochIdx, lr, tr_loss, tr_acc, val_loss, val_acc):\n",
    "        epoch_time = time.time() - start_time;\n",
    "        val_time = epoch_time - train_time;\n",
    "        line = 'SP-{} Epoch: {}/{} | Time: {} (Train {}  Val {}) | Train: LR {}  Loss {:.2f}  Acc {:.2f}% | Val: Loss {:.2f}  Acc(top1) {:.2f}% | HA {:.2f}@{}\\n'.format(\n",
    "            self.opt.splits, epochIdx+1, self.opt.nEpochs, U.to_hms(epoch_time), U.to_hms(train_time), U.to_hms(val_time),\n",
    "            lr, tr_loss, tr_acc, val_loss, val_acc, self.bestAcc, self.bestAccEpoch);\n",
    "        # print(line)\n",
    "        sys.stdout.write(line);\n",
    "        sys.stdout.flush();\n",
    "        \n",
    "    \n",
    "    def __get_lr(self, epoch):\n",
    "        divide_epoch = np.array([self.opt.nEpochs * i for i in self.opt.schedule]);\n",
    "        decay = sum(epoch > divide_epoch);\n",
    "        if epoch <= self.opt.warmup:\n",
    "            decay = 1;\n",
    "        return self.opt.LR * np.power(0.1, decay);\n",
    "\n",
    "    def __get_batch(self, index):\n",
    "        x = self.trainX[index*self.opt.batchSize : (index+1)*self.opt.batchSize];\n",
    "        y = self.trainY[index*self.opt.batchSize : (index+1)*self.opt.batchSize];\n",
    "        return x.to(self.opt.device), y.to(self.opt.device);\n",
    "    \n",
    "    \n",
    "    def __calibrate(self, net):\n",
    "        self.load_train_data();\n",
    "        net.eval();\n",
    "        with torch.no_grad():\n",
    "            for i in range(1,2):\n",
    "                x_pred = None;\n",
    "                for idx in range(math.ceil(len(self.trainX)/self.opt.batchSize)):\n",
    "                    x = self.trainX[idx*self.opt.batchSize : (idx+1)*self.opt.batchSize];\n",
    "                    scores = net(x);\n",
    "                    x_pred = scores.data if x_pred is None else torch.cat((x_pred, scores.data));\n",
    "                x_pred = x_pred.argmax(dim=1);\n",
    "                x_target = self.trainY.argmax(dim=1);\n",
    "                acc = (((x_pred==x_target)*1).float().mean()*100).item();\n",
    "                print('calibrate accuracy is: {:.2f}'.format(acc));\n",
    "        return acc;\n",
    "\n",
    "    def QuantizeModel(self):\n",
    "        net = self.__load_model(True);\n",
    "        # net = self.__load_model(False);\n",
    "        config = net.ch_config;\n",
    "        net.eval();\n",
    "        \n",
    "        #Fuse modules to\n",
    "        torch.quantization.fuse_modules(net.sfeb, ['0','1','2'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.sfeb, ['3','4','5'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['0','1','2'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['4','5','6'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['7','8','9'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['11','12','13'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['14','15','16'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['18','19','20'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['21','22','23'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['25','26','27'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['28','29','30'], inplace=True);\n",
    "        torch.quantization.fuse_modules(net.tfeb, ['33','34','35'], inplace=True);\n",
    "        net.train();\n",
    "        # Calibrate with the training data\n",
    "        # self.__calibrate(net);\n",
    "        net.to(self.opt.device);\n",
    "        self.__train(net);\n",
    "        self.load_test_data();\n",
    "        val_acc = self.__validate_test(net, True, self.testX, self.testY);\n",
    "        print('Testing: Acc(top1) {:.2f}%'.format(val_acc));\n",
    "\n",
    "        #place trained model to cpu\n",
    "        \n",
    "        net.train();\n",
    "        net.to('cpu');\n",
    "        net.qconfig = torch.quantization.get_default_qconfig('qnnpack')\n",
    "        torch.backends.quantized.engine = 'qnnpack';\n",
    "        print(f\"net.qconfig : {net.qconfig}\");\n",
    "        torch.quantization.prepare_qat(net, inplace=True);\n",
    "        # Convert to quantized model\n",
    "        torch.quantization.convert(net, inplace=True);\n",
    "        print('Post Training Quantization: Convert done');\n",
    "\n",
    "        print(\"Size of model after quantization\");\n",
    "        torch.save(net.state_dict(), \"temp.p\")\n",
    "        print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "        os.remove('temp.p')\n",
    "        # net.to('cpu');\n",
    "        # torch.jit.save(torch.jit.script(net), '{}/th/quantized_models/{}.pt'.format(os.getcwd(), self.opt.model_name.format()));\n",
    "        torch.jit.save(torch.jit.script(net), '../../../trained_models/step_6_QAT_and_Convert2TFLite/multifold/{}.pt'.format(self.opt.model_name));\n",
    "        torch.save({'weight':net.state_dict(), 'config':net.ch_config}, '../../../trained_models/step_6_QAT_and_Convert2TFLite/multifold/uncompressed_qat_models/uncompress_{}.pt'.format(self.opt.model_name));\n",
    "        \n",
    "        # **************convert to tflite**********\n",
    "        with torch.no_grad():\n",
    "            # dummy_input = torch.randn(1, 1, 30225, 1); wrong: RuntimeError: quantized::conv2d (qnnpack): each dimension of output tensor should be greater than 0.\n",
    "            dummy_input = torch.FloatTensor(quantize_int8(torch.randn(1, 1, 1, self.opt.inputLength).numpy(),3)); #correct,workable\n",
    "    \n",
    "            converter = TFLiteConverter(net,\n",
    "                                        dummy_input,\n",
    "                                        quantize_input_output_type='int8',#設定此欄，輸入會強制為int8\n",
    "                                        fuse_quant_dequant=True,\n",
    "                                        quantize_target_type='int8',\n",
    "                                        hybrid_conv=False,\n",
    "                                        float16_quantization=True,\n",
    "                                        optimize=5,\n",
    "                                        tflite_path=\"../../../trained_models/step_6_QAT_and_Convert2TFLite/multifold/{}.tflite\".format(self.opt.model_name))\n",
    "            converter.convert()\n",
    "\n",
    "\n",
    "    def ReTrainModel(self):\n",
    "        # quanted_net_path = \"../../../trained_models/step_6_QAT_and_Convert2TFLite/multifold/uncompressed_qat_models/uncompress_qat_model_uec4C_val93_tr_87_prunInfo_85.0_85.0_DSver15_fold2_20240730131440.pt\";\n",
    "        quanted_net_path = self.opt.model_path\n",
    "        state = torch.load(quanted_net_path);\n",
    "        quanted_net = models.GetACDNetQuantModel(input_len=20150, nclass=4, sr=20000, channel_config=state['config']);\n",
    "        print(state['weight']);\n",
    "        print(\"********************************************************************************************\");\n",
    "        print(quanted_net)\n",
    "        return\n",
    "        quanted_net.load_state_dict(state['weight']);\n",
    "        quanted_net.to(self.opt.device);\n",
    "\n",
    "        # Calibrate with the training data\n",
    "        self.__calibrate(quanted_net);\n",
    "        quanted_net.train();\n",
    "        self.__train(quanted_net);\n",
    "        self.load_test_data();\n",
    "        val_acc = self.__validate_test(quanted_net, True, self.testX, self.testY);\n",
    "        print('Testing: Acc(top1) {:.2f}%'.format(val_acc));\n",
    "        # **************save the retrain net and preserve the config**********\n",
    "        quanted.train();\n",
    "        torch.jit.save(torch.jit.script(net), '../../../trained_models/step_6_QAT_and_Convert2TFLite/multifold/{}.pt'.format(self.opt.model_name));\n",
    "        torch.save({'weight':net.state_dict(), 'config':net.ch_config}, '../../../trained_models/step_6_QAT_and_Convert2TFLite/multifold/uncompressed_qat_models/uncompress_{}.pt'.format(self.opt.model_name));\n",
    "        # **************convert to tflite**********\n",
    "        with torch.no_grad():\n",
    "            # dummy_input = torch.randn(1, 1, 30225, 1); wrong: RuntimeError: quantized::conv2d (qnnpack): each dimension of output tensor should be greater than 0.\n",
    "            dummy_input = torch.FloatTensor(quantize_int8(torch.randn(1, 1, 1, self.opt.inputLength).numpy(),3)); #correct,workable\n",
    "    \n",
    "            converter = TFLiteConverter(quanted_net,\n",
    "                                        dummy_input,\n",
    "                                        quantize_input_output_type='int8',#設定此欄，輸入會強制為int8\n",
    "                                        fuse_quant_dequant=True,\n",
    "                                        quantize_target_type='int8',\n",
    "                                        hybrid_conv=False,\n",
    "                                        float16_quantization=True,\n",
    "                                        optimize=5,\n",
    "                                        tflite_path=\"../../../trained_models/step_6_QAT_and_Convert2TFLite/multifold/{}.tflite\".format(self.opt.model_name))\n",
    "            converter.convert()\n",
    "    \n",
    "    def TestModel(self, quant=False):\n",
    "        if quant:\n",
    "            IfQuanted = True\n",
    "            print(f\"the model name:{self.opt.model_name}\");\n",
    "            net = torch.jit.load('../../../trained_models/step_6_QAT_and_Convert2TFLite/multifold/{}.pt'.format(self.opt.model_name))\n",
    "            net.to('cpu')\n",
    "        else:\n",
    "            IfQuanted = False;\n",
    "            print(\"has not quanted, load unquanted model...\");\n",
    "            net = self.__load_model();\n",
    "            net.to(self.opt.device);\n",
    "            # net.to('cuda:0');\n",
    "            # calc.summary(net, (1,1,self.opt.inputLength));\n",
    "        self.load_test_data();\n",
    "        # if IfuseCudaInVal:\n",
    "        #     print(f\"not quanted yet val use cuda and IfuseCudaInVal={IfuseCudaInVal}\");\n",
    "        #     net.to('cuda:0');\n",
    "        # else:\n",
    "        #     print(f\"quanted finished val use cpu and IfuseCudaInVal={IfuseCudaInVal}\");\n",
    "        #     net.to('cpu');\n",
    "        net.eval();\n",
    "        # val_acc = self.__validate_test(net, False, self.testX, self.testY);\n",
    "        val_acc = self.__validate_test(net, IfQuanted, self.testX, self.testY)\n",
    "        print('Testing: Acc(top1) {:.2f}%'.format(val_acc));\n",
    "\n",
    "    def GetModelSize(self):\n",
    "        orig_net_path = self.opt.model_path;\n",
    "        print('Full precision model size (KB):', os.path.getsize(orig_net_path)/(1024));\n",
    "        save_onnx_name = \"../../../trained_models/step_6_QAT_and_Convert2TFLite/multifold/{}.onnx\".format(self.opt.model_name);\n",
    "        quant_net_path = \"../has_qat_models/onnx_models/\"+save_onnx_name;\n",
    "        print('Quantized model size (KB):', os.path.getsize(quant_net_path)/(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b391031-54bf-4c2f-8c3d-40c0e6790c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#quanted net config: [4, 32, 7, 17, 22, 26, 17, 27, 27, 35, 88, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98118a3-2019-4cf6-9a38-55c0784dbe5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b75e728-df29-4468-ae40-06defe6836b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    opt = getOpts();#opts.parse();\n",
    "    \n",
    "    # if torch.backends.mps.is_available():\n",
    "    #     opt.device=\"mps\"; #for apple m2 gpu\n",
    "    # elif torch.cuda.is_available():\n",
    "    #     opt.device=\"cuda:0\"; #for nVidia gpu\n",
    "    # else:\n",
    "    #     opt.device=\"cpu\"\n",
    "    #quantization pack only support cpu\n",
    "    opt.device=\"cpu\"\n",
    "    print(f\"***Use device:{opt.device}\");\n",
    "    opt.nEpochs = 1200;#2000;\n",
    "    first_pruning_ratio=0.85*100;\n",
    "    second_pruning_ratio=0.85*100;\n",
    "    src_model_valacc=93;\n",
    "    src_model_tracc=87\n",
    "    ds_ver=\"15\"\n",
    "    opt.current_fold = \"fold1\"\n",
    "    opt.saveInfo = \"uec4C_val{}_tr_{}_prunInfo_{}_{}_DSver{}_{}\".format(src_model_valacc,src_model_tracc,first_pruning_ratio,second_pruning_ratio,ds_ver,opt.current_fold);\n",
    "    opt.model_path = \"../../../trained_models/step_6_QAT_and_Convert2TFLite/multifold/uncompressed_qat_models/uncompress_qat_model_uec4C_val93_tr_87_prunInfo_85.0_85.0_DSver15_fold2_20240730131440.pt\"\n",
    "    timeStr = genDataTimeStr();\n",
    "    opt.model_name = \"retrained_qat_model_{}_{}\".format(opt.saveInfo,timeStr);\n",
    "    #home\n",
    "    # opt.trainData=\"../../../../uec_iot_dev_training_datasets/generated_datasets/multifold/train/version15_multifold_home_fold5/fold5_train_20240721013721.npz\";\n",
    "    # opt.valData=\"../../../../uec_iot_dev_training_datasets/generated_datasets/multifold/val/version15_multifold_home_fold5/final_fold5_val_version15_multifold_home_20240721024402.npz\";\n",
    "    #office\n",
    "    opt.trainData=\"../../../../uec_iot_models_datasets/multifold/train/version15_multifold_home_fold1/fold1_train_20240721013720.npz\";\n",
    "    opt.valData=\"../../../../uec_iot_models_datasets/multifold/val/version15_multifold_home_fold1/final_fold1_val_version15_multifold_home.npz\";\n",
    "    \n",
    "    opt.split = 1;\n",
    "    opt.hasQuated = False;\n",
    "    display_info(opt);\n",
    "    trainer = QATTrainer(opt);\n",
    "\n",
    "    print(f'Retrain Quanted Model:{opt.model_path}');\n",
    "    trainer.ReTrainModel();\n",
    "    # print('Testing performance of the provided model.....');\n",
    "    # trainer.TestModel();\n",
    "\n",
    "    # print('Quantization process is started.....');\n",
    "    # trainer.QuantizeModel();\n",
    "    # print('Quantization done');\n",
    "\n",
    "    # print('Testing quantized model.');\n",
    "    # trainer.TestModel(True);\n",
    "    print('Finished');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1d22c1a-75b7-49c1-855f-7021a5ad68b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npruning algo: tylor-pruning\\npruning ration : 0.85\\nfinal accuracy : 87.46\\nepoch: \\nself.opt.LR = 0.1;\\nopt.momentum = 0.09;\\nself.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\\nself.opt.warmup = 0;\\nself.opt.prune_algo = 'tylor-pruning';\\nself.opt.prune_interval = 1;\\nself.opt.nEpochs = 1000;\\n===================================\\n\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "pruning algo: tylor-pruning\n",
    "pruning ration : 0.85\n",
    "final accuracy : 87.46\n",
    "epoch: \n",
    "self.opt.LR = 0.1;\n",
    "opt.momentum = 0.09;\n",
    "self.opt.schedule = [0.15, 0.30, 0.45, 0.60, 0.75];\n",
    "self.opt.warmup = 0;\n",
    "self.opt.prune_algo = 'tylor-pruning';\n",
    "self.opt.prune_interval = 1;\n",
    "self.opt.nEpochs = 1000;\n",
    "===================================\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c96f0ef-7dfc-4bd9-a951-72ea15e96c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Use device:cpu\n",
      "+------------------------------+\n",
      "| TLACDNet Sound classification\n",
      "+------------------------------+\n",
      "| dataset  : uec_iot\n",
      "| nEpochs  : 1200\n",
      "| LRInit   : 0.1\n",
      "| schedule : [0.3, 0.6, 0.9]\n",
      "| warmup   : 10\n",
      "| batchSize: 64\n",
      "| nFolds: 1\n",
      "| Splits: [1]\n",
      "| Device: cpu\n",
      "| Model Path: ../../../trained_models/step_6_QAT_and_Convert2TFLite/multifold/uncompressed_qat_models/uncompress_qat_model_uec4C_val93_tr_87_prunInfo_85.0_85.0_DSver15_fold2_20240730131440.pt\n",
      "| Model Name: retrained_qat_model_uec4C_val93_tr_87_prunInfo_85.0_85.0_DSver15_fold1_20240730161934\n",
      "+------------------------------+\n",
      "length of samples:711\n",
      "Retrain Quanted Model:../../../trained_models/step_6_QAT_and_Convert2TFLite/multifold/uncompressed_qat_models/uncompress_qat_model_uec4C_val93_tr_87_prunInfo_85.0_85.0_DSver15_fold2_20240730131440.pt\n",
      "OrderedDict([('sfeb.0.weight', tensor([[[[-1.1195, -1.5193,  1.0395, -4.0780, -3.5983, -1.5193,  0.5597,\n",
      "            3.1185,  9.2755]]],\n",
      "\n",
      "\n",
      "        [[[10.1551,  3.1985, -5.4374, -2.3988,  0.6397, -3.3584, -0.9595,\n",
      "           -0.3998, -0.0800]]],\n",
      "\n",
      "\n",
      "        [[[-8.2360, -4.7177,  2.5588,  4.0780,  0.5597,  2.9586, -0.3998,\n",
      "            1.1994, -1.9191]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6397,  3.1185, -2.4788,  1.3593,  8.1561,  3.0385, -1.0395,\n",
      "           -4.1580, -8.4759]]]], size=(4, 1, 1, 9), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.07996133714914322,\n",
      "       zero_point=0)), ('sfeb.0.bias', Parameter containing:\n",
      "tensor([0.2469, 0.9927, 0.2803, 0.1344])), ('sfeb.0.scale', tensor(1.)), ('sfeb.0.zero_point', tensor(0)), ('sfeb.3.weight', tensor([[[[-0.0348, -0.0418, -0.1044,  0.0696,  0.1880]],\n",
      "\n",
      "         [[ 0.0557, -0.0070, -0.0139,  0.0000, -0.0766]],\n",
      "\n",
      "         [[-0.0139,  0.0557,  0.0627, -0.0418,  0.0418]],\n",
      "\n",
      "         [[-0.0975,  0.0000, -0.0766,  0.1183, -0.0209]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0418,  0.0557,  0.1253, -0.2297,  0.1044]],\n",
      "\n",
      "         [[-0.1462,  0.2088, -0.4664,  0.7101, -0.4386]],\n",
      "\n",
      "         [[ 0.0696, -0.3133,  0.4177, -0.4734,  0.2715]],\n",
      "\n",
      "         [[-0.0766, -0.0209, -0.2785,  0.3202, -0.0627]]],\n",
      "\n",
      "\n",
      "        [[[-0.0557,  0.0000, -0.0835, -0.2088,  0.1183]],\n",
      "\n",
      "         [[-0.0278,  0.1183, -0.2506,  0.3411, -0.1810]],\n",
      "\n",
      "         [[ 0.0070, -0.1532,  0.1323, -0.1114,  0.0348]],\n",
      "\n",
      "         [[-0.0418,  0.2297, -0.2506,  0.2228,  0.2158]]],\n",
      "\n",
      "\n",
      "        [[[-0.0835, -0.1671, -0.2645,  0.1044, -0.0418]],\n",
      "\n",
      "         [[-0.1183, -0.1880,  0.3620, -0.4943,  0.3202]],\n",
      "\n",
      "         [[-0.2506,  0.1044, -0.3063,  0.3411, -0.2436]],\n",
      "\n",
      "         [[-0.1253, -0.1462,  0.3411, -0.4038,  0.0905]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2506, -0.3341,  0.0975,  0.2297, -0.1392]],\n",
      "\n",
      "         [[ 0.0975,  0.0835, -0.3063,  0.1532,  0.0209]],\n",
      "\n",
      "         [[-0.2645,  0.1462,  0.3481, -0.2993,  0.1183]],\n",
      "\n",
      "         [[-0.2924,  0.5360, -0.1392, -0.2785,  0.2158]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.1462,  0.0766,  0.0557, -0.3063,  0.3550]],\n",
      "\n",
      "         [[-0.1183, -0.0418,  0.5151, -0.5082,  0.0278]],\n",
      "\n",
      "         [[ 0.1183,  0.1949, -0.5569,  0.1810, -0.0627]],\n",
      "\n",
      "         [[ 0.2088, -0.6753,  0.3550,  0.4246, -0.0975]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0070,  0.0278, -0.2645, -0.1671,  0.1114]],\n",
      "\n",
      "         [[-0.2019,  0.3481, -0.2576, -0.2019,  0.2088]],\n",
      "\n",
      "         [[ 0.3272, -0.5569,  0.0278,  0.2297, -0.2645]],\n",
      "\n",
      "         [[-0.3411, -0.0627,  0.3133, -0.0209, -0.3063]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.4316, -0.0905, -0.2993, -0.0278]],\n",
      "\n",
      "         [[ 0.0070,  0.0905, -0.2228, -0.1740,  0.1810]],\n",
      "\n",
      "         [[-0.3968, -0.3063,  0.3550,  0.2993, -0.2436]],\n",
      "\n",
      "         [[-0.2436, -0.1740,  0.1949,  0.3063, -0.3341]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.2019,  0.2297,  0.2645, -0.1532, -0.4038]],\n",
      "\n",
      "         [[-0.5082,  0.0348,  0.5708,  0.0557, -0.1601]],\n",
      "\n",
      "         [[ 0.6544, -0.1740, -0.4803,  0.0627, -0.0418]],\n",
      "\n",
      "         [[ 0.1671, -0.1949, -0.2297, -0.0696,  0.4177]]],\n",
      "\n",
      "\n",
      "        [[[-0.1810,  0.1253, -0.2506, -0.1462,  0.5012]],\n",
      "\n",
      "         [[ 0.1671, -0.0627, -0.1323, -0.0487, -0.0487]],\n",
      "\n",
      "         [[-0.1880, -0.0766,  0.1114,  0.0418, -0.1810]],\n",
      "\n",
      "         [[-0.1532, -0.2506,  0.2854,  0.0696, -0.2436]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.2436,  0.2158, -0.0278, -0.5708]],\n",
      "\n",
      "         [[ 0.1532, -0.0278, -0.2228, -0.2993, -0.2367]],\n",
      "\n",
      "         [[-0.4246, -0.3481,  0.0975,  0.3202,  0.2088]],\n",
      "\n",
      "         [[-0.1183, -0.5221, -0.2088, -0.1949,  0.2158]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000, -0.0070, -0.0070,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000, -0.0070, -0.0070]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1740,  0.4038, -0.0905, -0.3202, -0.3968]],\n",
      "\n",
      "         [[ 0.1601,  0.3620,  0.0696, -0.3063, -0.1601]],\n",
      "\n",
      "         [[-0.2715, -0.2645,  0.1671,  0.2088, -0.0487]],\n",
      "\n",
      "         [[ 0.3133, -0.0696, -0.3898,  0.1880,  0.2645]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0696,  0.0696, -0.0627, -0.3202, -0.1392]],\n",
      "\n",
      "         [[ 0.1323, -0.1880, -0.1740, -0.2645,  0.1323]],\n",
      "\n",
      "         [[-0.3898,  0.0696,  0.2506,  0.1532, -0.0139]],\n",
      "\n",
      "         [[-0.0209, -0.1880, -0.0975,  0.1114,  0.3202]]],\n",
      "\n",
      "\n",
      "        [[[-0.4316, -0.2367,  0.0278,  0.3411,  0.2506]],\n",
      "\n",
      "         [[ 0.3411,  0.2506, -0.0766, -0.3690, -0.3272]],\n",
      "\n",
      "         [[-0.3620, -0.3550, -0.0278,  0.0905,  0.1880]],\n",
      "\n",
      "         [[ 0.0557, -0.1323, -0.3690, -0.5848, -0.3133]]],\n",
      "\n",
      "\n",
      "        [[[-0.0070,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0070,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0070,  0.0000, -0.0070,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.2367, -0.1810, -0.1392,  0.0905,  0.3411]],\n",
      "\n",
      "         [[-0.4943,  0.1671,  0.1044,  0.1949,  0.0418]],\n",
      "\n",
      "         [[ 0.0627, -0.3690, -0.0835, -0.0070, -0.0557]],\n",
      "\n",
      "         [[ 0.0835,  0.3759,  0.2088, -0.1671, -0.4873]]],\n",
      "\n",
      "\n",
      "        [[[-0.1880, -0.1044, -0.0835,  0.1532,  0.0348]],\n",
      "\n",
      "         [[ 0.1114,  0.0835,  0.1114,  0.0835, -0.0975]],\n",
      "\n",
      "         [[ 0.0209, -0.2506, -0.0418, -0.0348,  0.1114]],\n",
      "\n",
      "         [[ 0.2158,  0.1532, -0.1044, -0.1114, -0.2297]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2158,  0.3341,  0.4525,  0.2436,  0.2854]],\n",
      "\n",
      "         [[-0.3133, -0.3272, -0.1810, -0.2158, -0.2019]],\n",
      "\n",
      "         [[ 0.0139,  0.2924,  0.1183,  0.2158,  0.0696]],\n",
      "\n",
      "         [[-0.3133, -0.4107, -0.5012, -0.4525, -0.3202]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0209,  0.2367, -0.0627, -0.1949, -0.4107]],\n",
      "\n",
      "         [[-0.3898, -0.2576,  0.0835,  0.2576, -0.0209]],\n",
      "\n",
      "         [[ 0.2924,  0.1880, -0.1044, -0.1323, -0.1532]],\n",
      "\n",
      "         [[-0.2854, -0.0905, -0.2088,  0.2088,  0.2924]]],\n",
      "\n",
      "\n",
      "        [[[-0.1880, -0.0835, -0.2297, -0.1880, -0.3341]],\n",
      "\n",
      "         [[-0.2158,  0.1462,  0.1114,  0.1392,  0.0139]],\n",
      "\n",
      "         [[ 0.1114, -0.1253, -0.0766, -0.0835, -0.1253]],\n",
      "\n",
      "         [[ 0.1183,  0.1044,  0.1392,  0.2576,  0.1949]]],\n",
      "\n",
      "\n",
      "        [[[-0.3202, -0.3968, -0.4803, -0.1740, -0.3690]],\n",
      "\n",
      "         [[ 0.2576,  0.2436,  0.0835,  0.0975,  0.1462]],\n",
      "\n",
      "         [[-0.4386, -0.3341, -0.2924, -0.2854, -0.0557]],\n",
      "\n",
      "         [[ 0.0557, -0.0835,  0.2854,  0.2993,  0.3341]]],\n",
      "\n",
      "\n",
      "        [[[-0.1044, -0.2576, -0.2506, -0.2158, -0.2854]],\n",
      "\n",
      "         [[ 0.2506,  0.1601, -0.0139, -0.0975,  0.0070]],\n",
      "\n",
      "         [[-0.0627, -0.0766, -0.0975, -0.1392, -0.0209]],\n",
      "\n",
      "         [[ 0.0348, -0.2019, -0.0696,  0.0000, -0.0070]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0975,  0.0139,  0.1114,  0.0905,  0.1949]],\n",
      "\n",
      "         [[-0.1323, -0.0557,  0.0070, -0.0278,  0.0070]],\n",
      "\n",
      "         [[ 0.1601,  0.0835, -0.0348, -0.1532, -0.0696]],\n",
      "\n",
      "         [[-0.1392, -0.2228, -0.2785, -0.1532, -0.2297]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1323,  0.0418,  0.1392,  0.1740,  0.2228]],\n",
      "\n",
      "         [[-0.1532, -0.1392, -0.1183, -0.1323, -0.1323]],\n",
      "\n",
      "         [[ 0.2297,  0.1044, -0.0139, -0.1323, -0.0627]],\n",
      "\n",
      "         [[-0.1740, -0.2367, -0.1601, -0.2158, -0.2576]]],\n",
      "\n",
      "\n",
      "        [[[-0.0070, -0.0070, -0.0070,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000, -0.0070, -0.0070, -0.0139, -0.0070]],\n",
      "\n",
      "         [[-0.0139, -0.0070, -0.0070, -0.0070, -0.0070]],\n",
      "\n",
      "         [[ 0.0000, -0.0070, -0.0070,  0.0000,  0.0070]]],\n",
      "\n",
      "\n",
      "        [[[-0.5012, -0.7170, -0.8911, -0.5082, -0.4177]],\n",
      "\n",
      "         [[ 0.2576, -0.0696, -0.2158, -0.4177, -0.1880]],\n",
      "\n",
      "         [[ 0.4664,  0.3690,  0.5987,  0.6892,  0.7518]],\n",
      "\n",
      "         [[-0.0348, -0.1323, -0.2576, -0.2019, -0.0975]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]], size=(32, 4, 1, 5),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.006961383391171694, zero_point=0)), ('sfeb.3.bias', Parameter containing:\n",
      "tensor([ 0.2257, -0.0402,  0.3497,  0.1639,  0.5392,  0.1226, -0.0230,  0.3324,\n",
      "         0.4018,  0.4191, -0.0103,  0.1505,  0.4584,  0.9564, -0.0749,  0.1652,\n",
      "         0.5367,  0.6590, -0.1096,  0.3220, -0.0532,  1.1342, -0.0054,  0.5563,\n",
      "         0.2202, -0.0410,  0.1965,  0.3841,  0.7822, -0.0416,  0.8936, -0.0111])), ('sfeb.3.scale', tensor(1.)), ('sfeb.3.zero_point', tensor(0)), ('tfeb.0.weight', tensor([[[[ 0.1106, -0.1352,  0.2213],\n",
      "          [-0.2090, -0.4672, -0.1106],\n",
      "          [-0.1844, -0.4549, -0.0123]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.6147, -0.6147, -0.3073],\n",
      "          [-0.5163, -0.2951,  0.0246],\n",
      "          [ 0.1106,  0.3565,  0.4180]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1475,  0.0984,  0.0984],\n",
      "          [ 0.0984, -0.1229, -0.2582],\n",
      "          [-0.3442, -0.4795, -0.4918]]],\n",
      "\n",
      "\n",
      "        [[[-0.1844, -0.2951, -0.3442],\n",
      "          [ 0.0984, -0.0738,  0.0615],\n",
      "          [ 0.4303,  0.4303,  0.4180]]],\n",
      "\n",
      "\n",
      "        [[[-0.2213, -1.2663,  1.2909],\n",
      "          [ 1.5613, -1.3523, -0.4672],\n",
      "          [ 0.2951, -0.5778,  0.1475]]]], size=(7, 1, 3, 3), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.012293931096792221,\n",
      "       zero_point=0)), ('tfeb.0.bias', Parameter containing:\n",
      "tensor([ 1.4754, -0.0022, -0.0046,  0.6752,  0.9717,  0.8723,  0.3191])), ('tfeb.0.scale', tensor(1.)), ('tfeb.0.zero_point', tensor(0)), ('tfeb.4.weight', tensor([[[[-0.0360,  0.1260, -0.0960],\n",
      "          [-0.0120,  0.1140,  0.0240],\n",
      "          [-0.1319,  0.1619,  0.0480]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0960,  0.1260, -0.0720],\n",
      "          [-0.0480,  0.0120, -0.1080],\n",
      "          [ 0.0240,  0.1619,  0.0120]],\n",
      "\n",
      "         [[-0.1140, -0.2339, -0.0900],\n",
      "          [-0.3119, -0.2999, -0.3179],\n",
      "          [-0.1260, -0.2699, -0.2699]],\n",
      "\n",
      "         [[-0.0240, -0.1679, -0.0300],\n",
      "          [-0.0540,  0.0420, -0.0240],\n",
      "          [-0.0060, -0.0060,  0.0300]]],\n",
      "\n",
      "\n",
      "        [[[-0.0120, -0.2219, -0.0540],\n",
      "          [-0.0360, -0.2039, -0.0420],\n",
      "          [ 0.0300, -0.0540,  0.0540]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0600, -0.0600,  0.0540],\n",
      "          [-0.1439, -0.1799, -0.0480],\n",
      "          [ 0.0780,  0.0900,  0.1619]],\n",
      "\n",
      "         [[ 0.0540,  0.2999,  0.2039],\n",
      "          [-0.0960,  0.0600,  0.0060],\n",
      "          [-0.0600, -0.0900, -0.0060]],\n",
      "\n",
      "         [[ 0.0480, -0.2879, -0.0360],\n",
      "          [-0.2219, -0.4978, -0.2279],\n",
      "          [ 0.0300, -0.1619, -0.0780]]],\n",
      "\n",
      "\n",
      "        [[[-0.1679, -0.1919, -0.0060],\n",
      "          [-0.0660,  0.0420,  0.1379],\n",
      "          [ 0.0120,  0.0420,  0.0720]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1499, -0.0840,  0.0360],\n",
      "          [-0.0060,  0.1140,  0.1499],\n",
      "          [ 0.0000,  0.0900,  0.0600]],\n",
      "\n",
      "         [[ 0.2339,  0.3239,  0.1439],\n",
      "          [-0.0900, -0.1799, -0.2699],\n",
      "          [-0.2039, -0.1799, -0.0540]],\n",
      "\n",
      "         [[ 0.0360, -0.1559, -0.0240],\n",
      "          [ 0.0000, -0.1559, -0.1919],\n",
      "          [ 0.0480, -0.2519, -0.0960]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.2939,  0.1319,  0.2579],\n",
      "          [ 0.0240,  0.1499, -0.0420],\n",
      "          [-0.2279,  0.0360,  0.1619]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1619,  0.1260,  0.1020],\n",
      "          [ 0.0960,  0.1679,  0.0540],\n",
      "          [-0.0900,  0.0660,  0.0300]],\n",
      "\n",
      "         [[ 0.5818, -0.0660, -0.4798],\n",
      "          [-0.0120, -0.0180, -0.0900],\n",
      "          [ 0.2639, -0.0300, -0.3059]],\n",
      "\n",
      "         [[-0.2339,  0.0780, -0.2339],\n",
      "          [-0.7017, -0.0540, -0.2159],\n",
      "          [-0.5338,  0.1559, -0.0900]]],\n",
      "\n",
      "\n",
      "        [[[-0.1619,  0.1559,  0.1080],\n",
      "          [-0.2099, -0.0420,  0.0300],\n",
      "          [-0.1559,  0.1739,  0.1619]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1619,  0.2939,  0.1200],\n",
      "          [-0.1919, -0.1379, -0.0780],\n",
      "          [-0.0120,  0.1799,  0.0660]],\n",
      "\n",
      "         [[ 0.2699, -0.1200, -0.0360],\n",
      "          [ 0.0840, -0.0120, -0.0780],\n",
      "          [ 0.0480, -0.1260, -0.1140]],\n",
      "\n",
      "         [[ 0.1140,  0.0360, -0.0420],\n",
      "          [-0.0120, -0.0480,  0.0060],\n",
      "          [-0.1080,  0.0420, -0.0360]]],\n",
      "\n",
      "\n",
      "        [[[-0.3898, -0.1080,  0.4498],\n",
      "          [-0.4198, -0.1859,  0.3659],\n",
      "          [-0.2519, -0.0540,  0.4738]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2579,  0.0300,  0.2699],\n",
      "          [-0.3239, -0.0480,  0.2039],\n",
      "          [-0.1859,  0.0300,  0.3119]],\n",
      "\n",
      "         [[ 0.6058, -0.0480, -0.5878],\n",
      "          [ 0.4138,  0.1140, -0.2519],\n",
      "          [ 0.2759,  0.0000, -0.2579]],\n",
      "\n",
      "         [[-0.1319, -0.0360, -0.0840],\n",
      "          [-0.1919, -0.1439, -0.0300],\n",
      "          [-0.1559, -0.1679, -0.1200]]]], size=(17, 7, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.005997653119266033, zero_point=0)), ('tfeb.4.bias', Parameter containing:\n",
      "tensor([-0.3367,  1.6521,  1.2000,  1.0372,  0.3474, -0.0355, -0.2732,  0.6707,\n",
      "        -0.4748,  0.1025, -0.0506,  1.0594,  0.9423,  0.8085,  0.3148,  0.4114,\n",
      "         0.6515])), ('tfeb.4.scale', tensor(1.)), ('tfeb.4.zero_point', tensor(0)), ('tfeb.7.weight', tensor([[[[-0.0668, -0.0239,  0.0477],\n",
      "          [-0.0716, -0.0382,  0.0239],\n",
      "          [-0.0286,  0.0048,  0.0525]],\n",
      "\n",
      "         [[ 0.0143,  0.0716, -0.0143],\n",
      "          [-0.1193, -0.0811, -0.0239],\n",
      "          [-0.0477, -0.0143, -0.0620]],\n",
      "\n",
      "         [[-0.0906, -0.0525, -0.0239],\n",
      "          [-0.0286, -0.0048,  0.0382],\n",
      "          [ 0.0143,  0.0477,  0.0143]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0763, -0.0477, -0.0191],\n",
      "          [-0.0239, -0.1097, -0.0239],\n",
      "          [-0.0334, -0.1002, -0.0143]],\n",
      "\n",
      "         [[-0.0191,  0.0000,  0.0048],\n",
      "          [-0.0668, -0.0382, -0.0286],\n",
      "          [ 0.1145,  0.0811,  0.0525]],\n",
      "\n",
      "         [[ 0.0095, -0.0239,  0.0906],\n",
      "          [-0.0239, -0.0668,  0.0525],\n",
      "          [ 0.0716, -0.0573,  0.0239]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0334, -0.0906,  0.0143],\n",
      "          [ 0.1097, -0.0239, -0.0239],\n",
      "          [ 0.1193, -0.0477,  0.0143]],\n",
      "\n",
      "         [[ 0.0668, -0.1670,  0.1479],\n",
      "          [ 0.0334, -0.3101,  0.1050],\n",
      "          [ 0.0811, -0.2815,  0.0811]],\n",
      "\n",
      "         [[-0.0429, -0.2195,  0.0382],\n",
      "          [ 0.0095, -0.1193,  0.0620],\n",
      "          [ 0.0811, -0.0382,  0.2338]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0668,  0.1431, -0.2242],\n",
      "          [-0.1908,  0.1861, -0.4342],\n",
      "          [-0.1670,  0.2195, -0.2624]],\n",
      "\n",
      "         [[-0.0906, -0.0525, -0.0620],\n",
      "          [-0.0573,  0.0000,  0.0000],\n",
      "          [-0.0048,  0.0525,  0.1193]],\n",
      "\n",
      "         [[-0.0429,  0.0573,  0.0954],\n",
      "          [-0.0477, -0.0334, -0.0334],\n",
      "          [-0.0429, -0.0048, -0.0334]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4771,  0.6059,  0.4198],\n",
      "          [ 0.3721,  0.5153,  0.2481],\n",
      "          [ 0.0382,  0.3387,  0.3483]],\n",
      "\n",
      "         [[-0.0429, -0.1097, -0.0429],\n",
      "          [-0.0286, -0.0191,  0.0239],\n",
      "          [ 0.0477,  0.0095,  0.0620]],\n",
      "\n",
      "         [[-0.0525, -0.0573,  0.0095],\n",
      "          [ 0.0048,  0.0429,  0.0477],\n",
      "          [-0.0429, -0.1097, -0.0763]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0429,  0.0859, -0.0668],\n",
      "          [ 0.0286,  0.0191,  0.0286],\n",
      "          [ 0.0239,  0.0620,  0.0477]],\n",
      "\n",
      "         [[ 0.0382,  0.0859,  0.0954],\n",
      "          [ 0.0191,  0.0095, -0.0286],\n",
      "          [-0.0859, -0.0286,  0.0191]],\n",
      "\n",
      "         [[-0.0811, -0.0716,  0.0095],\n",
      "          [ 0.0000, -0.0573, -0.0763],\n",
      "          [ 0.0906,  0.0334,  0.0095]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0286,  0.0286,  0.0000],\n",
      "          [-0.1145, -0.0906,  0.0286],\n",
      "          [ 0.1240,  0.1527,  0.0859]],\n",
      "\n",
      "         [[ 0.1908,  0.0954, -0.0239],\n",
      "          [-0.0477, -0.1002, -0.1574],\n",
      "          [-0.1288, -0.1431, -0.1574]],\n",
      "\n",
      "         [[ 0.0143,  0.0191, -0.0286],\n",
      "          [ 0.1431,  0.0525, -0.0525],\n",
      "          [-0.0239, -0.0429, -0.0668]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0239,  0.0095,  0.0048],\n",
      "          [-0.0477,  0.1193,  0.0716],\n",
      "          [ 0.0906, -0.0334, -0.0477]],\n",
      "\n",
      "         [[ 0.1622,  0.0573, -0.0525],\n",
      "          [-0.1097, -0.0573,  0.0048],\n",
      "          [ 0.2385,  0.1240,  0.0048]],\n",
      "\n",
      "         [[ 0.1240,  0.0763, -0.0334],\n",
      "          [ 0.0954,  0.0334, -0.0763],\n",
      "          [ 0.0573, -0.1002, -0.1050]]],\n",
      "\n",
      "\n",
      "        [[[-0.0716,  0.0716,  0.2385],\n",
      "          [-0.0191, -0.0143,  0.0143],\n",
      "          [-0.1574,  0.0191,  0.2719]],\n",
      "\n",
      "         [[-0.1813, -0.0525,  0.1574],\n",
      "          [-0.1145, -0.1002,  0.1002],\n",
      "          [ 0.0191, -0.0573,  0.0477]],\n",
      "\n",
      "         [[-0.0620, -0.1097,  0.1145],\n",
      "          [-0.2624, -0.0906,  0.1527],\n",
      "          [ 0.1097, -0.0429,  0.0239]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2147,  0.1050, -0.0477],\n",
      "          [-0.0239, -0.0620, -0.2004],\n",
      "          [ 0.2863,  0.2958,  0.0620]],\n",
      "\n",
      "         [[-0.1908,  0.0000,  0.0859],\n",
      "          [-0.0095, -0.0382, -0.1765],\n",
      "          [-0.1002,  0.2242,  0.2481]],\n",
      "\n",
      "         [[ 0.0239, -0.1813, -0.1384],\n",
      "          [ 0.1050, -0.0811, -0.1050],\n",
      "          [ 0.0716,  0.1908,  0.0668]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0763,  0.0334, -0.0429],\n",
      "          [-0.0048, -0.0048, -0.0143],\n",
      "          [ 0.0429,  0.0716,  0.0239]],\n",
      "\n",
      "         [[ 0.0048, -0.0239, -0.0334],\n",
      "          [ 0.0811, -0.0477, -0.0191],\n",
      "          [ 0.0334, -0.0668,  0.0382]],\n",
      "\n",
      "         [[-0.0429,  0.0000,  0.0000],\n",
      "          [ 0.0382, -0.0239, -0.0334],\n",
      "          [-0.0429, -0.0954, -0.0095]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0382, -0.0668, -0.0620],\n",
      "          [-0.0239, -0.0143, -0.0191],\n",
      "          [ 0.0668,  0.0048, -0.0048]],\n",
      "\n",
      "         [[ 0.1240,  0.1050,  0.0191],\n",
      "          [-0.0954, -0.0954, -0.1193],\n",
      "          [ 0.0668,  0.0859,  0.0095]],\n",
      "\n",
      "         [[-0.0239, -0.0429,  0.0334],\n",
      "          [-0.0429, -0.0573,  0.0334],\n",
      "          [-0.0573, -0.1193,  0.0000]]]], size=(22, 17, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.0047709341160953045, zero_point=0)), ('tfeb.7.bias', Parameter containing:\n",
      "tensor([ 0.4024,  1.1692, -2.3165,  1.5581,  0.0431,  0.0921, -0.5000,  0.7607,\n",
      "         0.2444,  0.8626,  0.5954,  0.2358,  0.1750,  0.4465, -1.3794,  0.4974,\n",
      "         1.1351, -0.2054,  0.7818,  0.4179,  0.0982,  0.9377])), ('tfeb.7.scale', tensor(1.)), ('tfeb.7.zero_point', tensor(0)), ('tfeb.11.weight', tensor([[[[-0.0204, -0.0263, -0.0029],\n",
      "          [-0.0905, -0.0847, -0.1168],\n",
      "          [ 0.0350,  0.0058, -0.1401]],\n",
      "\n",
      "         [[-0.0058, -0.0847, -0.0088],\n",
      "          [-0.0438, -0.0642,  0.0029],\n",
      "          [-0.0876, -0.0555, -0.0088]],\n",
      "\n",
      "         [[-0.0467, -0.0730, -0.0438],\n",
      "          [-0.0438, -0.0584, -0.0613],\n",
      "          [-0.0350, -0.0555, -0.0613]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0409, -0.0321,  0.0117],\n",
      "          [-0.0350, -0.0613, -0.0496],\n",
      "          [-0.0584, -0.0380, -0.0292]],\n",
      "\n",
      "         [[-0.0642, -0.0788,  0.1109],\n",
      "          [ 0.0058,  0.0671,  0.0613],\n",
      "          [-0.0204,  0.0526, -0.0058]],\n",
      "\n",
      "         [[ 0.0321, -0.0788, -0.1168],\n",
      "          [ 0.0876, -0.0117, -0.0701],\n",
      "          [ 0.0467,  0.0263, -0.0526]]],\n",
      "\n",
      "\n",
      "        [[[-0.0730,  0.0467,  0.0000],\n",
      "          [ 0.0000, -0.0146, -0.0642],\n",
      "          [ 0.0584,  0.0467, -0.0321]],\n",
      "\n",
      "         [[ 0.0350,  0.0934, -0.0175],\n",
      "          [-0.0350,  0.0263, -0.0876],\n",
      "          [-0.0058,  0.0701, -0.0963]],\n",
      "\n",
      "         [[-0.0788, -0.0788,  0.0000],\n",
      "          [-0.0350, -0.0029,  0.0146],\n",
      "          [ 0.0000,  0.0496,  0.0526]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0380,  0.1460, -0.0146],\n",
      "          [ 0.0934,  0.3007,  0.0671],\n",
      "          [ 0.0788,  0.1839, -0.0029]],\n",
      "\n",
      "         [[-0.0847,  0.1168,  0.0613],\n",
      "          [ 0.0146,  0.1577,  0.0380],\n",
      "          [-0.0204,  0.2014, -0.0292]],\n",
      "\n",
      "         [[-0.1460, -0.0876, -0.0788],\n",
      "          [-0.0321,  0.0905,  0.0000],\n",
      "          [ 0.0759,  0.1927,  0.0467]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0701, -0.0350, -0.0029],\n",
      "          [-0.1226, -0.1168, -0.0701],\n",
      "          [ 0.0350,  0.0934,  0.0117]],\n",
      "\n",
      "         [[-0.0321, -0.1781, -0.0204],\n",
      "          [-0.0234, -0.2073, -0.1139],\n",
      "          [-0.0321, -0.1401, -0.0117]],\n",
      "\n",
      "         [[ 0.0817, -0.0788, -0.1226],\n",
      "          [ 0.2978,  0.0438, -0.2044],\n",
      "          [ 0.2102,  0.1226, -0.0817]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1956, -0.0117, -0.0438],\n",
      "          [ 0.1168,  0.0000, -0.0350],\n",
      "          [-0.1752, -0.0438,  0.0263]],\n",
      "\n",
      "         [[-0.0234, -0.1255,  0.1168],\n",
      "          [-0.0438, -0.0701,  0.0175],\n",
      "          [ 0.0117,  0.0642, -0.0350]],\n",
      "\n",
      "         [[ 0.0555,  0.0146,  0.0292],\n",
      "          [-0.0088, -0.0642, -0.0963],\n",
      "          [-0.1139, -0.0350, -0.0350]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0467, -0.0409, -0.0847],\n",
      "          [ 0.0613,  0.0000, -0.0467],\n",
      "          [ 0.1197,  0.0438,  0.0058]],\n",
      "\n",
      "         [[-0.1168, -0.0876,  0.0467],\n",
      "          [-0.0321, -0.0847,  0.0409],\n",
      "          [ 0.0730, -0.0146,  0.0058]],\n",
      "\n",
      "         [[ 0.0584,  0.0350, -0.0730],\n",
      "          [ 0.0117, -0.0058, -0.1051],\n",
      "          [ 0.0380, -0.0029, -0.0905]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0234,  0.0234, -0.0438],\n",
      "          [-0.0029, -0.0817, -0.1197],\n",
      "          [ 0.0263, -0.0526, -0.0642]],\n",
      "\n",
      "         [[-0.0204,  0.0088, -0.0204],\n",
      "          [-0.0409,  0.0117,  0.0380],\n",
      "          [-0.0526, -0.0438, -0.0117]],\n",
      "\n",
      "         [[-0.0088,  0.0234,  0.0146],\n",
      "          [-0.0058, -0.0146,  0.0000],\n",
      "          [ 0.0759,  0.0380, -0.0234]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0321,  0.0730,  0.0993],\n",
      "          [-0.0409,  0.0175,  0.0526],\n",
      "          [-0.0847, -0.0409,  0.0146]],\n",
      "\n",
      "         [[ 0.0438,  0.0000,  0.0292],\n",
      "          [ 0.0380,  0.0613,  0.0555],\n",
      "          [-0.0380,  0.0292,  0.0000]],\n",
      "\n",
      "         [[ 0.0175,  0.0292,  0.0088],\n",
      "          [ 0.0058,  0.0088, -0.0029],\n",
      "          [ 0.0175,  0.0029, -0.0058]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0058,  0.0146,  0.0000],\n",
      "          [ 0.0058,  0.0175, -0.0146],\n",
      "          [-0.1139, -0.1226, -0.1577]],\n",
      "\n",
      "         [[-0.0584, -0.0555, -0.0496],\n",
      "          [ 0.0204, -0.0117, -0.0204],\n",
      "          [ 0.0263, -0.0204, -0.0350]],\n",
      "\n",
      "         [[ 0.0759,  0.0438,  0.0058],\n",
      "          [ 0.0730,  0.0350, -0.0088],\n",
      "          [-0.0058, -0.0380, -0.0788]]],\n",
      "\n",
      "\n",
      "        [[[-0.1401, -0.0671, -0.0321],\n",
      "          [-0.0876, -0.0526, -0.0029],\n",
      "          [-0.0526, -0.0321, -0.0058]],\n",
      "\n",
      "         [[-0.0642, -0.2219,  0.3007],\n",
      "          [ 0.0000, -0.1898,  0.2890],\n",
      "          [ 0.1022, -0.1401,  0.2978]],\n",
      "\n",
      "         [[-0.0029,  0.0058,  0.0029],\n",
      "          [-0.0117, -0.0204,  0.0058],\n",
      "          [-0.0409, -0.0671,  0.0234]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0146,  0.0584],\n",
      "          [-0.0234, -0.0175,  0.0234],\n",
      "          [-0.0058, -0.0671,  0.0555]],\n",
      "\n",
      "         [[ 0.0350, -0.0934,  0.1285],\n",
      "          [ 0.0380, -0.0496,  0.1956],\n",
      "          [-0.0029, -0.0993,  0.0175]],\n",
      "\n",
      "         [[ 0.0993,  0.0321,  0.1197],\n",
      "          [ 0.0029, -0.0642,  0.0175],\n",
      "          [-0.0029, -0.0613, -0.0058]]]], size=(26, 22, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.002919454127550125, zero_point=0)), ('tfeb.11.bias', Parameter containing:\n",
      "tensor([-0.0825, -1.2826,  1.2048, -0.0095,  0.8619,  0.8392, -0.7786,  0.1670,\n",
      "         1.1614,  1.1046,  0.6548,  0.8558,  0.1470, -0.5245,  1.0783, -0.1640,\n",
      "         0.7417,  0.2397,  0.9036,  1.3370, -0.3012,  0.3835,  0.0322,  1.2321,\n",
      "        -0.3059, -0.1686])), ('tfeb.11.scale', tensor(1.)), ('tfeb.11.zero_point', tensor(0)), ('tfeb.14.weight', tensor([[[[-0.2058, -0.0096, -0.0161],\n",
      "          [-0.1383,  0.0868,  0.0225],\n",
      "          [-0.0804, -0.0611,  0.0579]],\n",
      "\n",
      "         [[-0.0514, -0.0450, -0.0129],\n",
      "          [-0.0225, -0.0032, -0.0611],\n",
      "          [-0.1061,  0.0354, -0.0129]],\n",
      "\n",
      "         [[-0.0322,  0.0643, -0.0161],\n",
      "          [ 0.0064,  0.1351,  0.0289],\n",
      "          [ 0.0386,  0.0965,  0.1029]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0129,  0.0740,  0.0450],\n",
      "          [ 0.0225, -0.0129, -0.1318],\n",
      "          [ 0.0225, -0.0547,  0.0193]],\n",
      "\n",
      "         [[-0.0322, -0.0032,  0.0900],\n",
      "          [-0.0322, -0.0096,  0.0997],\n",
      "          [-0.0354, -0.0482, -0.0032]],\n",
      "\n",
      "         [[ 0.0386,  0.0161,  0.0418],\n",
      "          [-0.0482,  0.0096,  0.0965],\n",
      "          [ 0.0289, -0.0257, -0.0322]]],\n",
      "\n",
      "\n",
      "        [[[-0.1447, -0.0289, -0.0643],\n",
      "          [ 0.0129,  0.0675, -0.0450],\n",
      "          [-0.0740,  0.0096, -0.0482]],\n",
      "\n",
      "         [[ 0.0932, -0.0096,  0.0289],\n",
      "          [ 0.0418,  0.0289, -0.0675],\n",
      "          [-0.0579,  0.0064, -0.0675]],\n",
      "\n",
      "         [[ 0.0064,  0.0129, -0.0868],\n",
      "          [ 0.1415,  0.0707, -0.0257],\n",
      "          [ 0.0514,  0.0193, -0.1158]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0161, -0.1190, -0.0740],\n",
      "          [ 0.0161, -0.0257,  0.0225],\n",
      "          [-0.0418,  0.1415,  0.2669]],\n",
      "\n",
      "         [[ 0.0193,  0.0675,  0.1479],\n",
      "          [-0.1318, -0.0707, -0.0064],\n",
      "          [-0.1640, -0.1576, -0.1029]],\n",
      "\n",
      "         [[ 0.0868,  0.1286,  0.0322],\n",
      "          [ 0.0354,  0.1222,  0.0611],\n",
      "          [ 0.1190,  0.0740, -0.0193]]],\n",
      "\n",
      "\n",
      "        [[[-0.1222,  0.1029,  0.0386],\n",
      "          [-0.0482, -0.0257,  0.0514],\n",
      "          [-0.0804,  0.0643,  0.0386]],\n",
      "\n",
      "         [[ 0.0032, -0.0772,  0.0289],\n",
      "          [-0.0900, -0.0804, -0.0740],\n",
      "          [ 0.0000, -0.0257, -0.1158]],\n",
      "\n",
      "         [[-0.2412, -0.1061, -0.1415],\n",
      "          [ 0.0386,  0.0386, -0.0932],\n",
      "          [ 0.1254,  0.2058,  0.0997]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0868,  0.1479,  0.0450],\n",
      "          [ 0.0193, -0.0965, -0.0932],\n",
      "          [-0.1383, -0.0450, -0.0675]],\n",
      "\n",
      "         [[-0.0161,  0.0289, -0.0836],\n",
      "          [-0.0450,  0.0772,  0.0643],\n",
      "          [-0.0579, -0.0354,  0.0804]],\n",
      "\n",
      "         [[-0.0482, -0.1254,  0.0161],\n",
      "          [-0.0547, -0.0129,  0.0322],\n",
      "          [ 0.0354, -0.0611, -0.1158]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0579,  0.0675, -0.0064],\n",
      "          [ 0.1125,  0.0579, -0.0225],\n",
      "          [ 0.0772, -0.0579, -0.1383]],\n",
      "\n",
      "         [[ 0.0129,  0.0257, -0.0643],\n",
      "          [ 0.0289,  0.0129, -0.0514],\n",
      "          [-0.0129, -0.0257, -0.0579]],\n",
      "\n",
      "         [[ 0.1254,  0.1190,  0.0482],\n",
      "          [ 0.0225,  0.0032, -0.0418],\n",
      "          [ 0.1511,  0.1286,  0.0064]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2219,  0.0740,  0.0482],\n",
      "          [ 0.3119,  0.1865,  0.1029],\n",
      "          [ 0.1158, -0.0225, -0.1576]],\n",
      "\n",
      "         [[-0.0354,  0.0064,  0.0257],\n",
      "          [ 0.0225,  0.0418, -0.0289],\n",
      "          [-0.0932,  0.0193, -0.0675]],\n",
      "\n",
      "         [[-0.0772, -0.0514, -0.0772],\n",
      "          [-0.1093, -0.0675, -0.0096],\n",
      "          [-0.1865, -0.0643,  0.0611]]],\n",
      "\n",
      "\n",
      "        [[[-0.0579,  0.1125, -0.0289],\n",
      "          [-0.0547,  0.0900, -0.0514],\n",
      "          [ 0.0354, -0.0064, -0.1286]],\n",
      "\n",
      "         [[-0.0418, -0.1125, -0.0354],\n",
      "          [-0.0450, -0.1158,  0.0064],\n",
      "          [ 0.0096, -0.0772, -0.0514]],\n",
      "\n",
      "         [[ 0.0514,  0.2219,  0.1029],\n",
      "          [-0.0836,  0.0096,  0.0579],\n",
      "          [-0.0547, -0.1736, -0.0965]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0322, -0.0193, -0.1093],\n",
      "          [-0.1254, -0.0161, -0.0322],\n",
      "          [-0.0547,  0.0064,  0.0772]],\n",
      "\n",
      "         [[ 0.0032,  0.0289,  0.0547],\n",
      "          [-0.0257, -0.0514, -0.0096],\n",
      "          [-0.0032, -0.1061, -0.1093]],\n",
      "\n",
      "         [[-0.1576, -0.1061, -0.0289],\n",
      "          [ 0.0482, -0.0611, -0.0322],\n",
      "          [-0.0900, -0.0129, -0.1029]]],\n",
      "\n",
      "\n",
      "        [[[-0.1158,  0.0772,  0.1254],\n",
      "          [-0.1640, -0.0932,  0.1608],\n",
      "          [-0.0193, -0.1254,  0.0257]],\n",
      "\n",
      "         [[ 0.2508,  0.0225,  0.0579],\n",
      "          [ 0.1961,  0.0225, -0.1125],\n",
      "          [ 0.0611,  0.0643,  0.0000]],\n",
      "\n",
      "         [[ 0.0032, -0.0129, -0.1093],\n",
      "          [ 0.0096, -0.0257, -0.0740],\n",
      "          [ 0.1704, -0.0064, -0.0482]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0096,  0.0032, -0.0997],\n",
      "          [ 0.0450, -0.0032, -0.1736],\n",
      "          [ 0.1543,  0.0514, -0.1833]],\n",
      "\n",
      "         [[ 0.0000, -0.0740, -0.0707],\n",
      "          [-0.0514, -0.0096, -0.0096],\n",
      "          [ 0.0000, -0.0032, -0.0932]],\n",
      "\n",
      "         [[ 0.0868,  0.3569,  0.0193],\n",
      "          [ 0.0161, -0.0322,  0.0129],\n",
      "          [-0.0225, -0.0611, -0.0418]]]], size=(17, 26, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.0032154954969882965, zero_point=0)), ('tfeb.14.bias', Parameter containing:\n",
      "tensor([ 0.3644, -0.8566, -0.0918, -0.0952, -0.0378, -0.4376, -0.5453,  0.2206,\n",
      "         1.1059,  0.3885, -0.6367,  0.4188,  0.4928, -0.3795,  0.0206,  0.1581,\n",
      "         0.4149])), ('tfeb.14.scale', tensor(1.)), ('tfeb.14.zero_point', tensor(0)), ('tfeb.18.weight', tensor([[[[-0.1321, -0.0660, -0.0193],\n",
      "          [-0.0908,  0.1431,  0.2339],\n",
      "          [ 0.0138, -0.0743, -0.1293]],\n",
      "\n",
      "         [[-0.0193, -0.0028, -0.0028],\n",
      "          [-0.2284,  0.0000,  0.0358],\n",
      "          [-0.0358, -0.0083,  0.0110]],\n",
      "\n",
      "         [[-0.1733,  0.0633,  0.1211],\n",
      "          [-0.2751, -0.1266,  0.2339],\n",
      "          [ 0.0880, -0.1156, -0.0523]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0110,  0.0853, -0.1321],\n",
      "          [ 0.2861, -0.0138, -0.1128],\n",
      "          [ 0.1156, -0.0908, -0.1431]],\n",
      "\n",
      "         [[-0.0303, -0.0770,  0.0908],\n",
      "          [-0.0248,  0.2174,  0.0385],\n",
      "          [-0.0413,  0.0275,  0.0660]],\n",
      "\n",
      "         [[-0.0138, -0.0908, -0.1156],\n",
      "          [-0.0605, -0.0220, -0.2284],\n",
      "          [-0.0550,  0.1156, -0.0605]]],\n",
      "\n",
      "\n",
      "        [[[-0.1238, -0.1266, -0.0385],\n",
      "          [-0.0220, -0.1321, -0.1073],\n",
      "          [ 0.0358,  0.1898,  0.0880]],\n",
      "\n",
      "         [[ 0.0220,  0.0385,  0.0083],\n",
      "          [ 0.0495, -0.0138, -0.0633],\n",
      "          [-0.0193, -0.0248,  0.0220]],\n",
      "\n",
      "         [[ 0.1018, -0.0440, -0.1293],\n",
      "          [ 0.0138, -0.0468, -0.1761],\n",
      "          [ 0.0275,  0.0660,  0.0880]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1376,  0.1431,  0.0715],\n",
      "          [-0.0165,  0.1623,  0.0523],\n",
      "          [-0.1156, -0.1046, -0.0138]],\n",
      "\n",
      "         [[-0.0633, -0.0385, -0.0248],\n",
      "          [ 0.0220, -0.0138, -0.0468],\n",
      "          [ 0.0330,  0.0853,  0.0715]],\n",
      "\n",
      "         [[ 0.0220, -0.0358, -0.0605],\n",
      "          [ 0.0633,  0.0000,  0.0248],\n",
      "          [ 0.0220,  0.1623,  0.1623]]],\n",
      "\n",
      "\n",
      "        [[[-0.0468,  0.0275,  0.0055],\n",
      "          [-0.0440,  0.0110, -0.0275],\n",
      "          [-0.0798, -0.1128, -0.1376]],\n",
      "\n",
      "         [[ 0.0110,  0.1926,  0.0743],\n",
      "          [-0.0990, -0.0110, -0.0743],\n",
      "          [-0.0165,  0.0083, -0.0248]],\n",
      "\n",
      "         [[-0.0825,  0.1513,  0.0358],\n",
      "          [-0.1431, -0.1376, -0.0550],\n",
      "          [-0.0578, -0.0990, -0.0468]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0880, -0.1513, -0.0578],\n",
      "          [-0.0468, -0.0220,  0.1486],\n",
      "          [-0.1018, -0.0330,  0.0853]],\n",
      "\n",
      "         [[-0.2091, -0.1431, -0.0275],\n",
      "          [-0.1073, -0.0248, -0.1101],\n",
      "          [ 0.0963,  0.0853, -0.0248]],\n",
      "\n",
      "         [[ 0.0770,  0.1761,  0.0660],\n",
      "          [-0.0275, -0.0193,  0.0193],\n",
      "          [-0.0523, -0.1073, -0.0550]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0990, -0.0633,  0.0440],\n",
      "          [ 0.0523, -0.1706, -0.0633],\n",
      "          [-0.1596, -0.0523, -0.1238]],\n",
      "\n",
      "         [[-0.2119, -0.1156, -0.1156],\n",
      "          [-0.0935, -0.0688, -0.1211],\n",
      "          [-0.1266, -0.0990, -0.1541]],\n",
      "\n",
      "         [[-0.0193,  0.0798,  0.0055],\n",
      "          [ 0.2806,  0.1788, -0.2834],\n",
      "          [-0.1596,  0.1403,  0.0083]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0990,  0.0248,  0.0633],\n",
      "          [-0.1431,  0.0193,  0.1266],\n",
      "          [-0.1706, -0.0110, -0.0660]],\n",
      "\n",
      "         [[ 0.0385,  0.1431, -0.1843],\n",
      "          [ 0.1183,  0.1953, -0.1348],\n",
      "          [ 0.0743,  0.0110, -0.0468]],\n",
      "\n",
      "         [[ 0.1101,  0.0413, -0.0605],\n",
      "          [-0.0358, -0.1926, -0.0110],\n",
      "          [-0.0715,  0.1596, -0.0193]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0743,  0.0165,  0.0578],\n",
      "          [-0.1046, -0.0660,  0.0468],\n",
      "          [-0.0990,  0.0688,  0.1046]],\n",
      "\n",
      "         [[-0.1211, -0.1073, -0.0853],\n",
      "          [-0.1046, -0.0715, -0.0330],\n",
      "          [-0.0578, -0.0578,  0.0330]],\n",
      "\n",
      "         [[ 0.0358,  0.0523,  0.1568],\n",
      "          [-0.0880, -0.0303, -0.0193],\n",
      "          [-0.0853, -0.0193,  0.1156]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0220,  0.0330, -0.1128],\n",
      "          [ 0.0853,  0.1018,  0.0083],\n",
      "          [-0.0193, -0.0028, -0.0193]],\n",
      "\n",
      "         [[-0.0688, -0.0358, -0.0138],\n",
      "          [ 0.0000,  0.0853,  0.1183],\n",
      "          [-0.0193, -0.0220,  0.0303]],\n",
      "\n",
      "         [[ 0.0275,  0.0000,  0.0330],\n",
      "          [-0.0055, -0.0468,  0.0358],\n",
      "          [ 0.0138,  0.0605,  0.0880]]]], size=(27, 17, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.002751352731138468, zero_point=0)), ('tfeb.18.bias', Parameter containing:\n",
      "tensor([ 0.3226,  0.2594,  1.1439,  0.5904,  0.0089,  0.9556, -0.4925,  0.4135,\n",
      "         1.1986,  1.0393, -0.3012, -0.5942, -0.6213, -0.0076, -0.7552, -0.0087,\n",
      "         0.1399,  0.4297,  0.5925,  0.4998,  1.1638,  1.3796, -0.0095,  0.2391,\n",
      "         0.2530, -0.0042,  0.2829])), ('tfeb.18.scale', tensor(1.)), ('tfeb.18.zero_point', tensor(0)), ('tfeb.21.weight', tensor([[[[-0.0367, -0.0449, -0.0122],\n",
      "          [-0.0776, -0.0408, -0.0163],\n",
      "          [ 0.0204,  0.0612,  0.0041]],\n",
      "\n",
      "         [[-0.0531, -0.0694, -0.1510],\n",
      "          [-0.0327, -0.0122, -0.0653],\n",
      "          [ 0.1061, -0.0163, -0.0245]],\n",
      "\n",
      "         [[-0.0735,  0.0408, -0.0367],\n",
      "          [-0.0449,  0.0000, -0.0122],\n",
      "          [ 0.0163,  0.1510,  0.0286]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0531,  0.0286,  0.0327],\n",
      "          [ 0.0204,  0.0694,  0.1551],\n",
      "          [ 0.0000,  0.0245, -0.0286]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0694, -0.0163,  0.0041],\n",
      "          [ 0.0408,  0.0694,  0.0571],\n",
      "          [ 0.0286, -0.0327, -0.0204]]],\n",
      "\n",
      "\n",
      "        [[[-0.1388, -0.1225, -0.0122],\n",
      "          [-0.1796, -0.1225, -0.0653],\n",
      "          [ 0.0490,  0.0490,  0.0980]],\n",
      "\n",
      "         [[-0.1021,  0.0612,  0.0367],\n",
      "          [ 0.0245, -0.0980, -0.0449],\n",
      "          [ 0.0000, -0.1143, -0.0571]],\n",
      "\n",
      "         [[-0.0612,  0.0163,  0.0449],\n",
      "          [-0.1061,  0.1225,  0.1225],\n",
      "          [-0.0816,  0.0939,  0.1061]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0857, -0.1265, -0.0571],\n",
      "          [-0.0816, -0.0857, -0.1184],\n",
      "          [-0.1265, -0.0531, -0.0408]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0939, -0.0980, -0.1592],\n",
      "          [-0.0245, -0.0041, -0.0122],\n",
      "          [-0.0204,  0.0367,  0.0449]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0612,  0.0327, -0.0939],\n",
      "          [ 0.0939,  0.0204,  0.0204],\n",
      "          [-0.0163, -0.0082, -0.1102]],\n",
      "\n",
      "         [[ 0.0367, -0.0449, -0.0327],\n",
      "          [-0.1021, -0.0082,  0.0082],\n",
      "          [-0.0571, -0.0367, -0.0204]],\n",
      "\n",
      "         [[ 0.0327,  0.0245,  0.0653],\n",
      "          [-0.0327, -0.1347, -0.0490],\n",
      "          [ 0.0735, -0.0286,  0.0612]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0163,  0.0449,  0.0408],\n",
      "          [-0.0857,  0.0041,  0.1714],\n",
      "          [-0.0816, -0.1225,  0.0612]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0367, -0.0082, -0.0571],\n",
      "          [ 0.1102,  0.0694, -0.0041],\n",
      "          [-0.0327, -0.0571, -0.0571]]],\n",
      "\n",
      "\n",
      "        [[[-0.0204,  0.0816,  0.0000],\n",
      "          [ 0.1021,  0.0490, -0.0327],\n",
      "          [ 0.2449,  0.0286, -0.0163]],\n",
      "\n",
      "         [[ 0.0408,  0.0735,  0.1102],\n",
      "          [-0.0980, -0.1347, -0.0939],\n",
      "          [-0.0816, -0.1306, -0.1021]],\n",
      "\n",
      "         [[-0.0653, -0.0612, -0.0204],\n",
      "          [ 0.0286,  0.1429, -0.0122],\n",
      "          [-0.1388,  0.4449,  0.1021]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1551, -0.0408, -0.0082],\n",
      "          [-0.0082, -0.1755,  0.1265],\n",
      "          [ 0.1143, -0.0857,  0.0245]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0612,  0.0367,  0.0327],\n",
      "          [ 0.0122, -0.0041, -0.0082],\n",
      "          [ 0.1143,  0.0286,  0.0041]]],\n",
      "\n",
      "\n",
      "        [[[-0.1674,  0.0939,  0.0857],\n",
      "          [-0.1878, -0.0490, -0.0490],\n",
      "          [ 0.0163, -0.0571, -0.0163]],\n",
      "\n",
      "         [[ 0.0939,  0.1796,  0.0000],\n",
      "          [ 0.0204, -0.0122,  0.1102],\n",
      "          [-0.0898, -0.0776, -0.0163]],\n",
      "\n",
      "         [[-0.0367,  0.0000,  0.0612],\n",
      "          [ 0.0816,  0.1225, -0.0449],\n",
      "          [-0.0490,  0.0204, -0.1388]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0408, -0.0612, -0.0490],\n",
      "          [-0.0776, -0.0408,  0.0082],\n",
      "          [-0.0041,  0.1796,  0.1470]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0449, -0.0245, -0.0122],\n",
      "          [ 0.0122, -0.0163,  0.0204],\n",
      "          [-0.0776, -0.0490,  0.0286]]]], size=(27, 27, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.004082061350345612, zero_point=0)), ('tfeb.21.bias', Parameter containing:\n",
      "tensor([-6.1607e-01,  1.0682e+00, -3.0599e-03, -5.3301e-01, -1.7516e-01,\n",
      "        -1.5282e-01, -2.6838e-01, -1.5190e+00,  2.9676e-01, -2.2797e-01,\n",
      "         1.9249e-01, -1.2584e-03,  3.6042e-01, -1.0861e-01,  9.2968e-01,\n",
      "        -1.4917e-01, -9.1603e-03,  1.5510e-02, -6.1356e-01,  1.6319e-01,\n",
      "         2.8710e-02, -2.5037e-01, -3.9737e-03, -6.9642e-01,  5.7839e-01,\n",
      "         5.6772e-01, -2.5142e-02])), ('tfeb.21.scale', tensor(1.)), ('tfeb.21.zero_point', tensor(0)), ('tfeb.25.weight', tensor([[[[-0.0191, -0.0421, -0.0230],\n",
      "          [-0.0498, -0.0651, -0.0268],\n",
      "          [-0.0077, -0.0498, -0.0574]],\n",
      "\n",
      "         [[-0.0038,  0.0268, -0.0727],\n",
      "          [ 0.0536,  0.0536, -0.0919],\n",
      "          [-0.1761, -0.0804,  0.0153]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0306,  0.1110, -0.0306],\n",
      "          [-0.2143, -0.0498,  0.0919],\n",
      "          [-0.0957, -0.0383, -0.0153]],\n",
      "\n",
      "         [[-0.0038, -0.0191, -0.0689],\n",
      "          [ 0.0000,  0.0536, -0.0804],\n",
      "          [-0.0191,  0.0421,  0.0383]],\n",
      "\n",
      "         [[-0.0191,  0.0498, -0.0077],\n",
      "          [-0.0421, -0.0612,  0.0651],\n",
      "          [-0.0574, -0.0842, -0.1148]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0421,  0.0230,  0.0038],\n",
      "          [ 0.1148,  0.0421,  0.0689],\n",
      "          [ 0.0574,  0.1033,  0.1569]],\n",
      "\n",
      "         [[ 0.2067,  0.1340,  0.1110],\n",
      "          [-0.1493, -0.1263, -0.0077],\n",
      "          [-0.0880, -0.0536, -0.0842]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1416, -0.0612, -0.0880],\n",
      "          [-0.1301, -0.1033, -0.1531],\n",
      "          [-0.0459, -0.0574, -0.0612]],\n",
      "\n",
      "         [[ 0.0727, -0.0077,  0.1110],\n",
      "          [ 0.0957,  0.0306,  0.1569],\n",
      "          [-0.0727, -0.1340, -0.1301]],\n",
      "\n",
      "         [[-0.0880, -0.1148, -0.0995],\n",
      "          [ 0.0000,  0.0536,  0.0115],\n",
      "          [-0.0153, -0.0459, -0.0268]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0880,  0.0919,  0.0689],\n",
      "          [ 0.0498,  0.0765,  0.0000],\n",
      "          [-0.0153, -0.0230, -0.0191]],\n",
      "\n",
      "         [[ 0.0038, -0.0153, -0.0459],\n",
      "          [-0.1454,  0.0268,  0.0727],\n",
      "          [-0.0651,  0.0268,  0.2220]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1378, -0.0383,  0.0765],\n",
      "          [-0.0842, -0.0957,  0.0038],\n",
      "          [ 0.0804, -0.0459,  0.0344]],\n",
      "\n",
      "         [[-0.0689, -0.0153, -0.0765],\n",
      "          [-0.0651, -0.0191,  0.0077],\n",
      "          [ 0.0038,  0.0038, -0.0230]],\n",
      "\n",
      "         [[ 0.0727,  0.0191, -0.0804],\n",
      "          [ 0.1186,  0.0268,  0.0077],\n",
      "          [ 0.0383,  0.0421,  0.0268]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0536,  0.0804,  0.0804],\n",
      "          [ 0.0153,  0.0191,  0.0612],\n",
      "          [-0.0306, -0.0153, -0.0612]],\n",
      "\n",
      "         [[-0.0115,  0.0804,  0.0268],\n",
      "          [ 0.0880,  0.0957, -0.0957],\n",
      "          [ 0.0536, -0.0498, -0.0919]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0880, -0.1186, -0.0459],\n",
      "          [-0.0612, -0.0268, -0.0574],\n",
      "          [-0.0230, -0.0153, -0.0344]],\n",
      "\n",
      "         [[-0.0153,  0.0153, -0.0651],\n",
      "          [ 0.1799,  0.1722, -0.1033],\n",
      "          [ 0.0344, -0.0077, -0.1186]],\n",
      "\n",
      "         [[-0.0230, -0.0115,  0.0344],\n",
      "          [-0.0230, -0.0230, -0.0995],\n",
      "          [ 0.0038,  0.0268,  0.0038]]]], size=(35, 27, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.003827291075140238, zero_point=0)), ('tfeb.25.bias', Parameter containing:\n",
      "tensor([ 2.2458e-02,  1.8145e-01, -1.4483e-01,  3.3989e-01, -3.3505e-02,\n",
      "        -2.0607e-01, -5.8126e-01, -1.8462e-03,  5.8665e-01, -4.0273e-01,\n",
      "         3.1772e-01,  2.0584e-01, -7.4699e-05, -1.1020e-03, -6.3282e-04,\n",
      "         1.0249e+00,  4.3434e-01,  3.1130e-01,  4.5189e-01, -9.1418e-04,\n",
      "         4.6637e-01,  3.9823e-01, -1.0622e-03,  2.7867e-01,  6.1685e-01,\n",
      "         3.3725e-01, -2.9385e-01, -2.5947e-01, -2.2250e-03, -3.3507e-03,\n",
      "        -1.1068e-03,  2.3356e-01, -1.0140e-03, -1.6031e-03, -8.5330e-01])), ('tfeb.25.scale', tensor(1.)), ('tfeb.25.zero_point', tensor(0)), ('tfeb.28.weight', tensor([[[[ 0.0053, -0.0131, -0.1367],\n",
      "          [-0.0894, -0.0131, -0.0105],\n",
      "          [-0.0973, -0.0210, -0.0473]],\n",
      "\n",
      "         [[-0.0210,  0.0053,  0.0263],\n",
      "          [-0.0421, -0.0289, -0.0342],\n",
      "          [ 0.0394,  0.0605,  0.0578]],\n",
      "\n",
      "         [[-0.0289,  0.0789,  0.0631],\n",
      "          [ 0.0815,  0.0158,  0.0683],\n",
      "          [ 0.0578, -0.0053,  0.0026]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0683, -0.0394, -0.0999],\n",
      "          [-0.0289, -0.1341, -0.0289],\n",
      "          [-0.0289, -0.0657,  0.0158]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0315,  0.0026, -0.0053],\n",
      "          [ 0.0237, -0.0079, -0.0315],\n",
      "          [ 0.0158, -0.0105, -0.0263]],\n",
      "\n",
      "         [[-0.0289, -0.0210,  0.0000],\n",
      "          [ 0.0237, -0.0578,  0.0447],\n",
      "          [-0.0026,  0.0158, -0.0079]],\n",
      "\n",
      "         [[-0.0394, -0.0184, -0.0237],\n",
      "          [-0.0368, -0.0315, -0.0289],\n",
      "          [ 0.0105,  0.0473, -0.0105]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0421,  0.0026, -0.0184],\n",
      "          [-0.0867, -0.0105, -0.0131],\n",
      "          [ 0.0000, -0.0552, -0.1025]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0053, -0.0315, -0.0315],\n",
      "          [-0.0237, -0.0105, -0.0578],\n",
      "          [ 0.0158, -0.0079, -0.0578]],\n",
      "\n",
      "         [[ 0.0079,  0.0053, -0.0421],\n",
      "          [ 0.0289, -0.0184, -0.0053],\n",
      "          [-0.0131, -0.0342, -0.0210]],\n",
      "\n",
      "         [[-0.0184,  0.0815, -0.0736],\n",
      "          [-0.0263,  0.0053,  0.1551],\n",
      "          [-0.0289, -0.0578, -0.0053]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.0552, -0.0289, -0.0631],\n",
      "          [-0.0158, -0.1367, -0.0815],\n",
      "          [-0.0657, -0.0736, -0.0526]]],\n",
      "\n",
      "\n",
      "        [[[-0.0289, -0.0578, -0.0368],\n",
      "          [-0.0578, -0.0394, -0.0131],\n",
      "          [-0.0184, -0.0237,  0.0105]],\n",
      "\n",
      "         [[-0.0237,  0.0184,  0.0158],\n",
      "          [ 0.0499,  0.0184,  0.0263],\n",
      "          [-0.0237, -0.0131, -0.0289]],\n",
      "\n",
      "         [[ 0.1052,  0.0867, -0.0026],\n",
      "          [ 0.0237,  0.0762, -0.0079],\n",
      "          [ 0.0105,  0.0000, -0.0053]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[-0.1025, -0.0473, -0.1078],\n",
      "          [-0.0421, -0.0342, -0.0578],\n",
      "          [-0.0315, -0.0736, -0.0342]]]], size=(88, 35, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.002628754824399948, zero_point=0)), ('tfeb.28.bias', Parameter containing:\n",
      "tensor([ 0.0443, -0.0076, -0.0007, -0.0182, -0.0017, -0.0052, -0.0008, -0.0230,\n",
      "        -0.0611, -0.1036, -0.1384,  0.0966, -0.0015, -0.2386, -0.0997, -0.1301,\n",
      "         0.1491,  0.1043,  0.0917, -0.1615, -0.0081, -0.0006, -0.0050,  0.0313,\n",
      "         0.0656,  0.0243, -0.0012, -0.0747, -0.1218,  0.2087,  0.0627, -0.0010,\n",
      "        -0.0021, -0.0039, -0.1882, -0.2107, -0.0006, -0.0669,  0.0959, -0.0058,\n",
      "         0.0090, -0.0005, -0.0013, -0.1540, -0.0006, -0.0414,  0.0355, -0.3461,\n",
      "        -0.1924, -0.1393, -0.0005, -0.0850, -0.0446, -0.0161, -0.1490,  0.1610,\n",
      "        -0.0935, -0.0058,  0.0014,  0.0420,  0.0444,  0.0388,  0.1375, -0.0930,\n",
      "        -0.0943, -0.0072, -0.3120,  0.2187,  0.0582, -0.3818, -0.3240,  0.1330,\n",
      "         0.0611, -0.0636, -0.2670, -0.1498, -0.0043, -0.3493, -0.0016, -0.0409,\n",
      "        -0.2878, -0.0014,  0.1072, -0.1726, -0.0774, -0.0384, -0.0460,  0.1777])), ('tfeb.28.scale', tensor(1.)), ('tfeb.28.zero_point', tensor(0)), ('tfeb.33.weight', tensor([[[[-0.1366]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.3167]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0745]],\n",
      "\n",
      "         [[-0.0714]],\n",
      "\n",
      "         [[ 0.0217]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.1987]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0993]],\n",
      "\n",
      "         [[ 0.0559]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[-0.1335]],\n",
      "\n",
      "         [[-0.1211]],\n",
      "\n",
      "         [[-0.0062]],\n",
      "\n",
      "         [[ 0.0900]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0342]],\n",
      "\n",
      "         [[ 0.0310]],\n",
      "\n",
      "         [[ 0.1521]],\n",
      "\n",
      "         [[ 0.0248]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.3229]],\n",
      "\n",
      "         [[-0.0590]],\n",
      "\n",
      "         [[-0.2204]],\n",
      "\n",
      "         [[ 0.3539]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[-0.2949]],\n",
      "\n",
      "         [[ 0.0745]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0186]],\n",
      "\n",
      "         [[-0.1335]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0248]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0869]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.2018]],\n",
      "\n",
      "         [[-0.0652]],\n",
      "\n",
      "         [[ 0.0559]],\n",
      "\n",
      "         [[ 0.0124]],\n",
      "\n",
      "         [[ 0.0497]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0652]],\n",
      "\n",
      "         [[ 0.0404]],\n",
      "\n",
      "         [[ 0.0217]],\n",
      "\n",
      "         [[ 0.0155]],\n",
      "\n",
      "         [[-0.2608]],\n",
      "\n",
      "         [[ 0.0497]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[-0.2359]],\n",
      "\n",
      "         [[ 0.0373]],\n",
      "\n",
      "         [[ 0.0373]],\n",
      "\n",
      "         [[ 0.0776]],\n",
      "\n",
      "         [[-0.0062]],\n",
      "\n",
      "         [[ 0.1118]],\n",
      "\n",
      "         [[ 0.0621]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0404]],\n",
      "\n",
      "         [[-0.2670]],\n",
      "\n",
      "         [[ 0.0435]],\n",
      "\n",
      "         [[-0.0310]],\n",
      "\n",
      "         [[ 0.0962]],\n",
      "\n",
      "         [[-0.0217]],\n",
      "\n",
      "         [[ 0.1304]],\n",
      "\n",
      "         [[-0.2297]],\n",
      "\n",
      "         [[ 0.0621]],\n",
      "\n",
      "         [[ 0.3943]],\n",
      "\n",
      "         [[-0.0093]],\n",
      "\n",
      "         [[ 0.1149]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.1428]],\n",
      "\n",
      "         [[ 0.1025]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0776]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0186]],\n",
      "\n",
      "         [[ 0.3415]],\n",
      "\n",
      "         [[-0.2484]],\n",
      "\n",
      "         [[-0.3322]]],\n",
      "\n",
      "\n",
      "        [[[-0.2018]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0310]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0217]],\n",
      "\n",
      "         [[ 0.1397]],\n",
      "\n",
      "         [[-0.3136]],\n",
      "\n",
      "         [[-0.0373]],\n",
      "\n",
      "         [[-0.0900]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.2142]],\n",
      "\n",
      "         [[-0.2018]],\n",
      "\n",
      "         [[ 0.3105]],\n",
      "\n",
      "         [[-0.0931]],\n",
      "\n",
      "         [[-0.2173]],\n",
      "\n",
      "         [[ 0.3042]],\n",
      "\n",
      "         [[ 0.3042]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.1801]],\n",
      "\n",
      "         [[ 0.1552]],\n",
      "\n",
      "         [[-0.0497]],\n",
      "\n",
      "         [[ 0.1428]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0931]],\n",
      "\n",
      "         [[ 0.1925]],\n",
      "\n",
      "         [[ 0.0466]],\n",
      "\n",
      "         [[ 0.0404]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0714]],\n",
      "\n",
      "         [[ 0.1770]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.1521]],\n",
      "\n",
      "         [[ 0.0652]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.2359]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.1056]],\n",
      "\n",
      "         [[ 0.2887]],\n",
      "\n",
      "         [[ 0.0652]],\n",
      "\n",
      "         [[ 0.2235]],\n",
      "\n",
      "         [[ 0.0590]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.3384]],\n",
      "\n",
      "         [[ 0.1025]],\n",
      "\n",
      "         [[ 0.0497]],\n",
      "\n",
      "         [[ 0.1211]],\n",
      "\n",
      "         [[-0.0993]],\n",
      "\n",
      "         [[-0.0993]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0621]],\n",
      "\n",
      "         [[ 0.0900]],\n",
      "\n",
      "         [[-0.2763]],\n",
      "\n",
      "         [[ 0.1708]],\n",
      "\n",
      "         [[ 0.0124]],\n",
      "\n",
      "         [[-0.1614]],\n",
      "\n",
      "         [[ 0.0435]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0310]],\n",
      "\n",
      "         [[-0.0559]],\n",
      "\n",
      "         [[-0.1894]],\n",
      "\n",
      "         [[ 0.1645]],\n",
      "\n",
      "         [[ 0.2359]],\n",
      "\n",
      "         [[-0.1770]],\n",
      "\n",
      "         [[ 0.1645]],\n",
      "\n",
      "         [[-0.0900]],\n",
      "\n",
      "         [[-0.2639]],\n",
      "\n",
      "         [[ 0.0217]],\n",
      "\n",
      "         [[ 0.1397]],\n",
      "\n",
      "         [[ 0.2049]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.2887]],\n",
      "\n",
      "         [[ 0.1925]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.1583]],\n",
      "\n",
      "         [[ 0.1428]],\n",
      "\n",
      "         [[ 0.0590]],\n",
      "\n",
      "         [[-0.0248]],\n",
      "\n",
      "         [[-0.1087]],\n",
      "\n",
      "         [[-0.0062]]],\n",
      "\n",
      "\n",
      "        [[[-0.2142]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0621]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.2546]],\n",
      "\n",
      "         [[ 0.3694]],\n",
      "\n",
      "         [[ 0.0683]],\n",
      "\n",
      "         [[ 0.0310]],\n",
      "\n",
      "         [[-0.0714]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.2515]],\n",
      "\n",
      "         [[ 0.1708]],\n",
      "\n",
      "         [[ 0.2359]],\n",
      "\n",
      "         [[ 0.1583]],\n",
      "\n",
      "         [[-0.0217]],\n",
      "\n",
      "         [[ 0.0373]],\n",
      "\n",
      "         [[-0.1490]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.2049]],\n",
      "\n",
      "         [[-0.2266]],\n",
      "\n",
      "         [[-0.0931]],\n",
      "\n",
      "         [[-0.2297]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0155]],\n",
      "\n",
      "         [[ 0.3415]],\n",
      "\n",
      "         [[-0.1832]],\n",
      "\n",
      "         [[-0.0124]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0310]],\n",
      "\n",
      "         [[-0.2546]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0621]],\n",
      "\n",
      "         [[ 0.2111]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.2949]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.2670]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0869]],\n",
      "\n",
      "         [[ 0.0466]],\n",
      "\n",
      "         [[-0.0683]],\n",
      "\n",
      "         [[ 0.0838]],\n",
      "\n",
      "         [[ 0.1801]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0466]],\n",
      "\n",
      "         [[ 0.2608]],\n",
      "\n",
      "         [[-0.2732]],\n",
      "\n",
      "         [[-0.2639]],\n",
      "\n",
      "         [[ 0.0528]],\n",
      "\n",
      "         [[ 0.3260]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0993]],\n",
      "\n",
      "         [[-0.2484]],\n",
      "\n",
      "         [[ 0.1614]],\n",
      "\n",
      "         [[ 0.0528]],\n",
      "\n",
      "         [[ 0.0807]],\n",
      "\n",
      "         [[-0.0714]],\n",
      "\n",
      "         [[ 0.0186]],\n",
      "\n",
      "         [[-0.0031]],\n",
      "\n",
      "         [[ 0.2328]],\n",
      "\n",
      "         [[ 0.0497]],\n",
      "\n",
      "         [[ 0.2080]],\n",
      "\n",
      "         [[ 0.1925]],\n",
      "\n",
      "         [[-0.1583]],\n",
      "\n",
      "         [[-0.1273]],\n",
      "\n",
      "         [[-0.0900]],\n",
      "\n",
      "         [[-0.1490]],\n",
      "\n",
      "         [[ 0.2142]],\n",
      "\n",
      "         [[-0.0342]],\n",
      "\n",
      "         [[-0.2608]],\n",
      "\n",
      "         [[-0.2049]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.1770]],\n",
      "\n",
      "         [[-0.1801]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.1118]],\n",
      "\n",
      "         [[ 0.1863]],\n",
      "\n",
      "         [[ 0.0745]],\n",
      "\n",
      "         [[-0.0186]],\n",
      "\n",
      "         [[ 0.1025]],\n",
      "\n",
      "         [[ 0.0155]]],\n",
      "\n",
      "\n",
      "        [[[-0.0807]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0186]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.2111]],\n",
      "\n",
      "         [[ 0.0993]],\n",
      "\n",
      "         [[ 0.0838]],\n",
      "\n",
      "         [[ 0.3105]],\n",
      "\n",
      "         [[ 0.0590]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.3074]],\n",
      "\n",
      "         [[ 0.2732]],\n",
      "\n",
      "         [[-0.0497]],\n",
      "\n",
      "         [[-0.0528]],\n",
      "\n",
      "         [[ 0.1304]],\n",
      "\n",
      "         [[-0.0652]],\n",
      "\n",
      "         [[ 0.0279]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.1583]],\n",
      "\n",
      "         [[ 0.1366]],\n",
      "\n",
      "         [[-0.0590]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0248]],\n",
      "\n",
      "         [[-0.1863]],\n",
      "\n",
      "         [[ 0.0466]],\n",
      "\n",
      "         [[-0.0310]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[-0.0373]],\n",
      "\n",
      "         [[ 0.2266]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.2142]],\n",
      "\n",
      "         [[-0.0497]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0217]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.2297]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0714]],\n",
      "\n",
      "         [[-0.0838]],\n",
      "\n",
      "         [[ 0.2856]],\n",
      "\n",
      "         [[-0.0838]],\n",
      "\n",
      "         [[ 0.3074]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.0155]],\n",
      "\n",
      "         [[-0.1583]],\n",
      "\n",
      "         [[ 0.1552]],\n",
      "\n",
      "         [[ 0.1056]],\n",
      "\n",
      "         [[ 0.0497]],\n",
      "\n",
      "         [[ 0.0559]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.1490]],\n",
      "\n",
      "         [[ 0.0559]],\n",
      "\n",
      "         [[ 0.0590]],\n",
      "\n",
      "         [[-0.1832]],\n",
      "\n",
      "         [[ 0.3229]],\n",
      "\n",
      "         [[-0.0093]],\n",
      "\n",
      "         [[ 0.3881]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[-0.1056]],\n",
      "\n",
      "         [[ 0.2049]],\n",
      "\n",
      "         [[ 0.1676]],\n",
      "\n",
      "         [[-0.2049]],\n",
      "\n",
      "         [[ 0.1459]],\n",
      "\n",
      "         [[-0.1149]],\n",
      "\n",
      "         [[ 0.1645]],\n",
      "\n",
      "         [[-0.2111]],\n",
      "\n",
      "         [[ 0.1925]],\n",
      "\n",
      "         [[ 0.0031]],\n",
      "\n",
      "         [[ 0.0683]],\n",
      "\n",
      "         [[ 0.2018]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.0528]],\n",
      "\n",
      "         [[ 0.1739]],\n",
      "\n",
      "         [[ 0.0000]],\n",
      "\n",
      "         [[ 0.2422]],\n",
      "\n",
      "         [[-0.2204]],\n",
      "\n",
      "         [[ 0.2701]],\n",
      "\n",
      "         [[-0.0124]],\n",
      "\n",
      "         [[ 0.0652]],\n",
      "\n",
      "         [[ 0.0590]]]], size=(4, 88, 1, 1), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.003104560077190399,\n",
      "       zero_point=0)), ('tfeb.33.bias', Parameter containing:\n",
      "tensor([1.0392, 0.5876, 0.6163, 0.2399])), ('tfeb.33.scale', tensor(1.)), ('tfeb.33.zero_point', tensor(0)), ('tfeb.38.scale', tensor(1.)), ('tfeb.38.zero_point', tensor(0)), ('tfeb.38._packed_params.dtype', torch.qint8), ('tfeb.38._packed_params._packed_params', (tensor([[-0.2888, -1.9454,  0.7143,  0.6231],\n",
      "        [-0.3952,  0.7903,  0.6839, -1.7782],\n",
      "        [-0.2432,  0.7903, -1.8542,  0.6231],\n",
      "        [ 0.9119,  0.3648,  0.4560,  0.5167]], size=(4, 4), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_tensor_affine, scale=0.015198335982859135,\n",
      "       zero_point=0), tensor([ 0.2155,  0.0806,  0.1546, -0.4507], requires_grad=True))), ('quant.scale', tensor([1.])), ('quant.zero_point', tensor([0]))])\n",
      "********************************************************************************************\n",
      "ACDNetQuant(\n",
      "  (sfeb): Sequential(\n",
      "    (0): Conv2d(1, 4, kernel_size=(1, 9), stride=(1, 2), bias=False)\n",
      "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(4, 32, kernel_size=(1, 5), stride=(1, 2), bias=False)\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=(1, 50), stride=(1, 50), padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (tfeb): Sequential(\n",
      "    (0): Conv2d(1, 7, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(7, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (5): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Conv2d(17, 22, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (8): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): Conv2d(22, 26, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (12): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU()\n",
      "    (14): Conv2d(26, 17, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (15): BatchNorm2d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU()\n",
      "    (17): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (18): Conv2d(17, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (19): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU()\n",
      "    (21): Conv2d(27, 27, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (22): BatchNorm2d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): ReLU()\n",
      "    (24): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (25): Conv2d(27, 35, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (26): BatchNorm2d(35, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (27): ReLU()\n",
      "    (28): Conv2d(35, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (29): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (30): ReLU()\n",
      "    (31): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "    (32): Dropout(p=0.2, inplace=False)\n",
      "    (33): Conv2d(88, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (34): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU()\n",
      "    (36): AvgPool2d(kernel_size=(1, 3), stride=(1, 3), padding=0)\n",
      "    (37): Flatten(start_dim=1, end_dim=-1)\n",
      "    (38): Linear(in_features=4, out_features=4, bias=True)\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): Softmax(dim=1)\n",
      "  )\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53259c8d-ce50-4747-ac3b-8cdc18cc73f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a0d7f-70cd-4ccf-a42f-f48c3f23c9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
